{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch import nn\n",
    "import torch.nn.init as init\n",
    "from torch import optim\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part contains all the functions used for the preprocessing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "    \n",
    "    return pd.read_csv(filepath)\n",
    "\n",
    "def clean_data(df):\n",
    "    df.dropna(subset = ['y'], inplace = True)\n",
    "    df.drop(['contact', 'poutcome', 'duration'], axis = 1, inplace=True) # these features have too many non values \n",
    "    df.dropna(inplace=True) # others\n",
    "    for column in df.columns:\n",
    "\n",
    "        if pd.api.types.is_numeric_dtype(df[column]):\n",
    "        \n",
    "            if column not in ['previous', 'pdays']:\n",
    "\n",
    "                Q1 = df[column].quantile(0.25)\n",
    "                Q3 = df[column].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower = Q1 - 1.5*IQR\n",
    "                upper = Q3 + 1.5*IQR\n",
    "                upper_array = np.where(df[column] >= upper)[0]\n",
    "                lower_array = np.where(df[column] <= lower)[0]\n",
    "                upper_array = [idx for idx in upper_array if idx in df.index]\n",
    "                lower_array = [idx for idx in lower_array if idx in df.index]\n",
    "                df = df[(df[column] >= lower) & (df[column] <= upper)]\n",
    "    return df\n",
    "\n",
    "def encode_data(df):\n",
    "\n",
    "    df_encoded = df.replace(['yes', 'no'], [1, 0])\n",
    "    \n",
    "    df_encoded.replace(['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec'], \n",
    "                    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], inplace=True)\n",
    "    \n",
    "    df_encoded = one_hot_encode(df_encoded, 'job')\n",
    "\n",
    "    df_encoded = one_hot_encode(df_encoded, 'education')\n",
    "\n",
    "    df_encoded = one_hot_encode(df_encoded, 'marital')\n",
    "\n",
    "    return df_encoded\n",
    "\n",
    "def one_hot_encode(df, column_name):\n",
    "    if column_name in df.columns:\n",
    "\n",
    "        df_encoded = pd.get_dummies(df[column_name], prefix=column_name, dtype=int)\n",
    "\n",
    "        df.drop(column_name, axis=1, inplace=True)\n",
    "\n",
    "        df = pd.concat([df, df_encoded], axis=1)\n",
    "\n",
    "        print(\"One-hot encoding applied to column \", column_name, \" successfully.\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    else:\n",
    "\n",
    "        print(f\"Column '{column_name}' not found in the DataFrame.\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part contains functions for feature extraction like scaling data and dimension reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features(df):\n",
    "\n",
    "    # apply normalization \n",
    "    std_scaler = StandardScaler()\n",
    "\n",
    "    df_scaled = std_scaler.fit_transform(df.drop('y', axis=1).values)\n",
    "\n",
    "    df_scaled = pd.DataFrame(df_scaled, columns=df.drop('y', axis=1).columns)\n",
    "\n",
    "    df_scaled['y'] = df['y']\n",
    "\n",
    "    df_scaled.index = df.index\n",
    "\n",
    "    return df_scaled\n",
    "\n",
    "def reduce_dimensions(df):\n",
    "\n",
    "    # Select samples, transform and reduce features\n",
    "    df_reduced = pd.concat([df[df['y'] == 1].head(4000).reset_index(drop=True),\n",
    "                                df[df['y'] == 0].tail(4000).reset_index(drop=True)],\n",
    "                                ignore_index=False).sort_index(kind='merge')\n",
    "\n",
    "    dim_red = PCA()\n",
    "\n",
    "    x_pca = dim_red.fit_transform(df_reduced.drop('y', axis=1))\n",
    "\n",
    "    training_features = ['pca0', 'pca1', 'pca2', 'pca3', 'pca4', 'pca5', 'pca6', 'pca7', 'pca8', 'pca9', 'pca10', 'pca11', 'pca12', 'pca13']\n",
    "\n",
    "    df_pca = pd.DataFrame(x_pca, columns=dim_red.get_feature_names_out())\n",
    "\n",
    "    df_pca_working_set = df_pca[training_features]\n",
    "\n",
    "    df_pca_working_set = df_pca_working_set.reset_index(drop=True)\n",
    "\n",
    "    df_pca_working_set['y'] = df_reduced.reset_index(drop=True)['y']\n",
    "\n",
    "    return df_pca_working_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the model used for training the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "\n",
    "            nn.Linear(14, 32),\n",
    "\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(32, 16),\n",
    "\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(16, 1),\n",
    "\n",
    "            nn.Sigmoid()\n",
    "\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "\n",
    "        for m in self.modules():\n",
    "\n",
    "            if isinstance(m, nn.Linear):\n",
    "\n",
    "                init.kaiming_normal_(m.weight)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        return self.layers(inputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are split into training and testing data. 1000 samples are used for testing and 7000 for training. The data are chosen so that the imbalance is minimized. The data contain 4000 samples with target value of 'no' and 4000 with target value of 'yes'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df):\n",
    "\n",
    "    x_tensor = torch.tensor(df.drop(columns=['y']).tail(7000).values, dtype=torch.float32)\n",
    "\n",
    "    y_tensor = torch.tensor(df['y'].tail(7000).values, dtype=torch.float32)\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(x_tensor, y_tensor)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size = 64)\n",
    "\n",
    "    x_test_tensor = torch.tensor(df.drop(columns=['y']).head(1000).values, dtype=torch.float32)\n",
    "\n",
    "    y_test_tensor = torch.tensor(df['y'].head(1000).values, dtype=torch.float32)\n",
    "\n",
    "    test_dataset = torch.utils.data.TensorDataset(x_test_tensor, y_test_tensor)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 8)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the experiments, the number of epichs was determined to be 3000. To minimize the loss, the learning rate is reduced during the training process. The weights of the output are manipulated to prevent the model being biased to the 0 class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 3000\n",
    "LEARNING_RATE = .001\n",
    "\n",
    "def train(model, train_loader):\n",
    "    \n",
    "    learning_rate = LEARNING_RATE\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "    \n",
    "    cost = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        total_training_loss = 0\n",
    "\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "\n",
    "            inputs, targets = data\n",
    "\n",
    "            targets = targets.unsqueeze(1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            preds = model.forward(inputs)\n",
    "\n",
    "            weight = torch.where(targets == 1, torch.tensor([1.0]), torch.tensor([0.02]))\n",
    "            \n",
    "            loss = cost(preds, targets) * weight\n",
    "            \n",
    "            loss = loss.mean()\n",
    "            \n",
    "            total_training_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        if (epoch%500) == 0 and epoch != 0:\n",
    "            \n",
    "            learning_rate = learning_rate / 2\n",
    "            \n",
    "            for param in optimizer.param_groups:\n",
    "            \n",
    "                param['lr'] = learning_rate\n",
    "\n",
    "        print(f'Epoch {epoch}. Learning Rate: {learning_rate} Total Training Loss: ', total_training_loss)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, the model's performance is evaluated using metrics such as confusion matrix, accuracy, precision, recall (sensitivity), specificity, and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metrics(y_true, y_pred):\n",
    "    \n",
    "    cm = metrics.confusion_matrix(y_true, y_pred, labels=[1, 0])\n",
    "\n",
    "    print(cm.T)\n",
    "\n",
    "    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = [1, 0])\n",
    "\n",
    "    cm_display.plot()\n",
    "    \n",
    "    Accuracy = metrics.accuracy_score(y_true, y_pred) \n",
    "    \n",
    "    Precision = metrics.precision_score(y_true, y_pred)\n",
    "\n",
    "    Sensitivity_recall = metrics.recall_score(y_true, y_pred)\n",
    "\n",
    "    Specificity = metrics.recall_score(y_true, y_pred, pos_label=0)\n",
    "\n",
    "    F1_score = metrics.f1_score(y_true, y_pred)\n",
    "\n",
    "    print({\"Accuracy\":Accuracy,\"Precision\":Precision,\"Sensitivity_recall\":Sensitivity_recall,\"Specificity\":Specificity,\"F1_score\":F1_score})\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "\n",
    "    y_true = np.empty((0, ))\n",
    "\n",
    "    y_pred = np.empty((0, ))\n",
    "\n",
    "    for inputs, labels in test_loader:\n",
    "\n",
    "            output = model(inputs)\n",
    "\n",
    "            output = (output > 0.5).float()\n",
    "\n",
    "            output = output.detach().cpu().numpy()\n",
    "\n",
    "            y_pred = np.append(y_pred, output)\n",
    "\n",
    "            labels = labels.detach().cpu().numpy()\n",
    "\n",
    "            y_true =  np.append(y_true, labels) # Save Truth\n",
    "    \n",
    "    extract_metrics(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoding applied to column  job  successfully.\n",
      "One-hot encoding applied to column  education  successfully.\n",
      "One-hot encoding applied to column  marital  successfully.\n",
      "Epoch 0. Learning Rate: 0.001 Total Training Loss:  9.21127376286313\n",
      "Epoch 1. Learning Rate: 0.001 Total Training Loss:  8.21543651074171\n",
      "Epoch 2. Learning Rate: 0.001 Total Training Loss:  7.554110895143822\n",
      "Epoch 3. Learning Rate: 0.001 Total Training Loss:  7.249600376235321\n",
      "Epoch 4. Learning Rate: 0.001 Total Training Loss:  7.028230243362486\n",
      "Epoch 5. Learning Rate: 0.001 Total Training Loss:  6.848356295144185\n",
      "Epoch 6. Learning Rate: 0.001 Total Training Loss:  6.68904207739979\n",
      "Epoch 7. Learning Rate: 0.001 Total Training Loss:  6.564061771146953\n",
      "Epoch 8. Learning Rate: 0.001 Total Training Loss:  6.455306798918173\n",
      "Epoch 9. Learning Rate: 0.001 Total Training Loss:  6.35146923805587\n",
      "Epoch 10. Learning Rate: 0.001 Total Training Loss:  6.2583561352221295\n",
      "Epoch 11. Learning Rate: 0.001 Total Training Loss:  6.166997833875939\n",
      "Epoch 12. Learning Rate: 0.001 Total Training Loss:  6.083516812999733\n",
      "Epoch 13. Learning Rate: 0.001 Total Training Loss:  6.013892593444325\n",
      "Epoch 14. Learning Rate: 0.001 Total Training Loss:  5.944027774035931\n",
      "Epoch 15. Learning Rate: 0.001 Total Training Loss:  5.874844870879315\n",
      "Epoch 16. Learning Rate: 0.001 Total Training Loss:  5.8184887890238315\n",
      "Epoch 17. Learning Rate: 0.001 Total Training Loss:  5.760706730885431\n",
      "Epoch 18. Learning Rate: 0.001 Total Training Loss:  5.70593107608147\n",
      "Epoch 19. Learning Rate: 0.001 Total Training Loss:  5.648068779497407\n",
      "Epoch 20. Learning Rate: 0.001 Total Training Loss:  5.598025311366655\n",
      "Epoch 21. Learning Rate: 0.001 Total Training Loss:  5.546798398718238\n",
      "Epoch 22. Learning Rate: 0.001 Total Training Loss:  5.497599033871666\n",
      "Epoch 23. Learning Rate: 0.001 Total Training Loss:  5.445020248414949\n",
      "Epoch 24. Learning Rate: 0.001 Total Training Loss:  5.405373803107068\n",
      "Epoch 25. Learning Rate: 0.001 Total Training Loss:  5.352478986023925\n",
      "Epoch 26. Learning Rate: 0.001 Total Training Loss:  5.3163341821637005\n",
      "Epoch 27. Learning Rate: 0.001 Total Training Loss:  5.2705564186908305\n",
      "Epoch 28. Learning Rate: 0.001 Total Training Loss:  5.222116365795955\n",
      "Epoch 29. Learning Rate: 0.001 Total Training Loss:  5.182504206662998\n",
      "Epoch 30. Learning Rate: 0.001 Total Training Loss:  5.145057113259099\n",
      "Epoch 31. Learning Rate: 0.001 Total Training Loss:  5.107204002444632\n",
      "Epoch 32. Learning Rate: 0.001 Total Training Loss:  5.073998584412038\n",
      "Epoch 33. Learning Rate: 0.001 Total Training Loss:  5.036709640175104\n",
      "Epoch 34. Learning Rate: 0.001 Total Training Loss:  5.001082717673853\n",
      "Epoch 35. Learning Rate: 0.001 Total Training Loss:  4.97514844185207\n",
      "Epoch 36. Learning Rate: 0.001 Total Training Loss:  4.933945107972249\n",
      "Epoch 37. Learning Rate: 0.001 Total Training Loss:  4.91007120301947\n",
      "Epoch 38. Learning Rate: 0.001 Total Training Loss:  4.87227373663336\n",
      "Epoch 39. Learning Rate: 0.001 Total Training Loss:  4.842233466915786\n",
      "Epoch 40. Learning Rate: 0.001 Total Training Loss:  4.814811323303729\n",
      "Epoch 41. Learning Rate: 0.001 Total Training Loss:  4.786067657289095\n",
      "Epoch 42. Learning Rate: 0.001 Total Training Loss:  4.756789187551476\n",
      "Epoch 43. Learning Rate: 0.001 Total Training Loss:  4.734923868207261\n",
      "Epoch 44. Learning Rate: 0.001 Total Training Loss:  4.703784492448904\n",
      "Epoch 45. Learning Rate: 0.001 Total Training Loss:  4.679761396953836\n",
      "Epoch 46. Learning Rate: 0.001 Total Training Loss:  4.654011513572186\n",
      "Epoch 47. Learning Rate: 0.001 Total Training Loss:  4.634614090318792\n",
      "Epoch 48. Learning Rate: 0.001 Total Training Loss:  4.614074530894868\n",
      "Epoch 49. Learning Rate: 0.001 Total Training Loss:  4.593540169298649\n",
      "Epoch 50. Learning Rate: 0.001 Total Training Loss:  4.57386372203473\n",
      "Epoch 51. Learning Rate: 0.001 Total Training Loss:  4.555565936840139\n",
      "Epoch 52. Learning Rate: 0.001 Total Training Loss:  4.535633109509945\n",
      "Epoch 53. Learning Rate: 0.001 Total Training Loss:  4.51790829456877\n",
      "Epoch 54. Learning Rate: 0.001 Total Training Loss:  4.503553342656232\n",
      "Epoch 55. Learning Rate: 0.001 Total Training Loss:  4.488917121780105\n",
      "Epoch 56. Learning Rate: 0.001 Total Training Loss:  4.46829761355184\n",
      "Epoch 57. Learning Rate: 0.001 Total Training Loss:  4.4548606963362545\n",
      "Epoch 58. Learning Rate: 0.001 Total Training Loss:  4.438859657617286\n",
      "Epoch 59. Learning Rate: 0.001 Total Training Loss:  4.425073001882993\n",
      "Epoch 60. Learning Rate: 0.001 Total Training Loss:  4.409014888457023\n",
      "Epoch 61. Learning Rate: 0.001 Total Training Loss:  4.395504071027972\n",
      "Epoch 62. Learning Rate: 0.001 Total Training Loss:  4.387056976673193\n",
      "Epoch 63. Learning Rate: 0.001 Total Training Loss:  4.375643039820716\n",
      "Epoch 64. Learning Rate: 0.001 Total Training Loss:  4.362084747641347\n",
      "Epoch 65. Learning Rate: 0.001 Total Training Loss:  4.348434403538704\n",
      "Epoch 66. Learning Rate: 0.001 Total Training Loss:  4.332486787228845\n",
      "Epoch 67. Learning Rate: 0.001 Total Training Loss:  4.323982481844723\n",
      "Epoch 68. Learning Rate: 0.001 Total Training Loss:  4.307176164467819\n",
      "Epoch 69. Learning Rate: 0.001 Total Training Loss:  4.294035460217856\n",
      "Epoch 70. Learning Rate: 0.001 Total Training Loss:  4.279503047699109\n",
      "Epoch 71. Learning Rate: 0.001 Total Training Loss:  4.272040516952984\n",
      "Epoch 72. Learning Rate: 0.001 Total Training Loss:  4.260157858720049\n",
      "Epoch 73. Learning Rate: 0.001 Total Training Loss:  4.249376491876319\n",
      "Epoch 74. Learning Rate: 0.001 Total Training Loss:  4.237416399177164\n",
      "Epoch 75. Learning Rate: 0.001 Total Training Loss:  4.22397476201877\n",
      "Epoch 76. Learning Rate: 0.001 Total Training Loss:  4.216839195461944\n",
      "Epoch 77. Learning Rate: 0.001 Total Training Loss:  4.206566138076596\n",
      "Epoch 78. Learning Rate: 0.001 Total Training Loss:  4.19835283444263\n",
      "Epoch 79. Learning Rate: 0.001 Total Training Loss:  4.186883564572781\n",
      "Epoch 80. Learning Rate: 0.001 Total Training Loss:  4.175924303126521\n",
      "Epoch 81. Learning Rate: 0.001 Total Training Loss:  4.168833722593263\n",
      "Epoch 82. Learning Rate: 0.001 Total Training Loss:  4.158265153178945\n",
      "Epoch 83. Learning Rate: 0.001 Total Training Loss:  4.1497560671996325\n",
      "Epoch 84. Learning Rate: 0.001 Total Training Loss:  4.1410350520163774\n",
      "Epoch 85. Learning Rate: 0.001 Total Training Loss:  4.130000081146136\n",
      "Epoch 86. Learning Rate: 0.001 Total Training Loss:  4.121886691544205\n",
      "Epoch 87. Learning Rate: 0.001 Total Training Loss:  4.112621973268688\n",
      "Epoch 88. Learning Rate: 0.001 Total Training Loss:  4.106078757089563\n",
      "Epoch 89. Learning Rate: 0.001 Total Training Loss:  4.096596693736501\n",
      "Epoch 90. Learning Rate: 0.001 Total Training Loss:  4.086640523280948\n",
      "Epoch 91. Learning Rate: 0.001 Total Training Loss:  4.087278134538792\n",
      "Epoch 92. Learning Rate: 0.001 Total Training Loss:  4.077053988468833\n",
      "Epoch 93. Learning Rate: 0.001 Total Training Loss:  4.06842360238079\n",
      "Epoch 94. Learning Rate: 0.001 Total Training Loss:  4.058805147651583\n",
      "Epoch 95. Learning Rate: 0.001 Total Training Loss:  4.051891781156883\n",
      "Epoch 96. Learning Rate: 0.001 Total Training Loss:  4.043427182361484\n",
      "Epoch 97. Learning Rate: 0.001 Total Training Loss:  4.037835081806406\n",
      "Epoch 98. Learning Rate: 0.001 Total Training Loss:  4.031765085179359\n",
      "Epoch 99. Learning Rate: 0.001 Total Training Loss:  4.024903869256377\n",
      "Epoch 100. Learning Rate: 0.001 Total Training Loss:  4.014234093367122\n",
      "Epoch 101. Learning Rate: 0.001 Total Training Loss:  4.010541504714638\n",
      "Epoch 102. Learning Rate: 0.001 Total Training Loss:  4.002873112913221\n",
      "Epoch 103. Learning Rate: 0.001 Total Training Loss:  3.991701105958782\n",
      "Epoch 104. Learning Rate: 0.001 Total Training Loss:  3.990868191118352\n",
      "Epoch 105. Learning Rate: 0.001 Total Training Loss:  3.9788258143235\n",
      "Epoch 106. Learning Rate: 0.001 Total Training Loss:  3.9790103679988533\n",
      "Epoch 107. Learning Rate: 0.001 Total Training Loss:  3.9674755031010136\n",
      "Epoch 108. Learning Rate: 0.001 Total Training Loss:  3.9609167537419125\n",
      "Epoch 109. Learning Rate: 0.001 Total Training Loss:  3.954376180889085\n",
      "Epoch 110. Learning Rate: 0.001 Total Training Loss:  3.9492031135596335\n",
      "Epoch 111. Learning Rate: 0.001 Total Training Loss:  3.9459907391574234\n",
      "Epoch 112. Learning Rate: 0.001 Total Training Loss:  3.9371639114106074\n",
      "Epoch 113. Learning Rate: 0.001 Total Training Loss:  3.930771136539988\n",
      "Epoch 114. Learning Rate: 0.001 Total Training Loss:  3.928202118841\n",
      "Epoch 115. Learning Rate: 0.001 Total Training Loss:  3.9171446399996057\n",
      "Epoch 116. Learning Rate: 0.001 Total Training Loss:  3.9141455305507407\n",
      "Epoch 117. Learning Rate: 0.001 Total Training Loss:  3.906726316548884\n",
      "Epoch 118. Learning Rate: 0.001 Total Training Loss:  3.8925390916410834\n",
      "Epoch 119. Learning Rate: 0.001 Total Training Loss:  3.8900114591233432\n",
      "Epoch 120. Learning Rate: 0.001 Total Training Loss:  3.890424172510393\n",
      "Epoch 121. Learning Rate: 0.001 Total Training Loss:  3.8825743997003883\n",
      "Epoch 122. Learning Rate: 0.001 Total Training Loss:  3.8694301667856053\n",
      "Epoch 123. Learning Rate: 0.001 Total Training Loss:  3.8731584262568504\n",
      "Epoch 124. Learning Rate: 0.001 Total Training Loss:  3.8694750912254676\n",
      "Epoch 125. Learning Rate: 0.001 Total Training Loss:  3.856253038160503\n",
      "Epoch 126. Learning Rate: 0.001 Total Training Loss:  3.8548016055719927\n",
      "Epoch 127. Learning Rate: 0.001 Total Training Loss:  3.848400933551602\n",
      "Epoch 128. Learning Rate: 0.001 Total Training Loss:  3.847497155191377\n",
      "Epoch 129. Learning Rate: 0.001 Total Training Loss:  3.8440898701082915\n",
      "Epoch 130. Learning Rate: 0.001 Total Training Loss:  3.8375952088972554\n",
      "Epoch 131. Learning Rate: 0.001 Total Training Loss:  3.829893505666405\n",
      "Epoch 132. Learning Rate: 0.001 Total Training Loss:  3.824647839879617\n",
      "Epoch 133. Learning Rate: 0.001 Total Training Loss:  3.822432123241015\n",
      "Epoch 134. Learning Rate: 0.001 Total Training Loss:  3.809368815156631\n",
      "Epoch 135. Learning Rate: 0.001 Total Training Loss:  3.8105220559518784\n",
      "Epoch 136. Learning Rate: 0.001 Total Training Loss:  3.800677795195952\n",
      "Epoch 137. Learning Rate: 0.001 Total Training Loss:  3.797053880756721\n",
      "Epoch 138. Learning Rate: 0.001 Total Training Loss:  3.8001154277008027\n",
      "Epoch 139. Learning Rate: 0.001 Total Training Loss:  3.7833474365761504\n",
      "Epoch 140. Learning Rate: 0.001 Total Training Loss:  3.780853928416036\n",
      "Epoch 141. Learning Rate: 0.001 Total Training Loss:  3.779913470498286\n",
      "Epoch 142. Learning Rate: 0.001 Total Training Loss:  3.7686715982854366\n",
      "Epoch 143. Learning Rate: 0.001 Total Training Loss:  3.76127613929566\n",
      "Epoch 144. Learning Rate: 0.001 Total Training Loss:  3.757919728406705\n",
      "Epoch 145. Learning Rate: 0.001 Total Training Loss:  3.7603363485541195\n",
      "Epoch 146. Learning Rate: 0.001 Total Training Loss:  3.7494118206668645\n",
      "Epoch 147. Learning Rate: 0.001 Total Training Loss:  3.7403611356858164\n",
      "Epoch 148. Learning Rate: 0.001 Total Training Loss:  3.7451427701162174\n",
      "Epoch 149. Learning Rate: 0.001 Total Training Loss:  3.7381654389901087\n",
      "Epoch 150. Learning Rate: 0.001 Total Training Loss:  3.7330412624869496\n",
      "Epoch 151. Learning Rate: 0.001 Total Training Loss:  3.726581478607841\n",
      "Epoch 152. Learning Rate: 0.001 Total Training Loss:  3.71904825407546\n",
      "Epoch 153. Learning Rate: 0.001 Total Training Loss:  3.712161551695317\n",
      "Epoch 154. Learning Rate: 0.001 Total Training Loss:  3.7126106205396354\n",
      "Epoch 155. Learning Rate: 0.001 Total Training Loss:  3.7126243656966835\n",
      "Epoch 156. Learning Rate: 0.001 Total Training Loss:  3.7052849795436487\n",
      "Epoch 157. Learning Rate: 0.001 Total Training Loss:  3.7024428899167106\n",
      "Epoch 158. Learning Rate: 0.001 Total Training Loss:  3.6956843881634995\n",
      "Epoch 159. Learning Rate: 0.001 Total Training Loss:  3.6929297677706927\n",
      "Epoch 160. Learning Rate: 0.001 Total Training Loss:  3.68409112165682\n",
      "Epoch 161. Learning Rate: 0.001 Total Training Loss:  3.688033724320121\n",
      "Epoch 162. Learning Rate: 0.001 Total Training Loss:  3.679625641554594\n",
      "Epoch 163. Learning Rate: 0.001 Total Training Loss:  3.677477477816865\n",
      "Epoch 164. Learning Rate: 0.001 Total Training Loss:  3.6762507501989603\n",
      "Epoch 165. Learning Rate: 0.001 Total Training Loss:  3.667283347225748\n",
      "Epoch 166. Learning Rate: 0.001 Total Training Loss:  3.665577049017884\n",
      "Epoch 167. Learning Rate: 0.001 Total Training Loss:  3.669430428533815\n",
      "Epoch 168. Learning Rate: 0.001 Total Training Loss:  3.657218216219917\n",
      "Epoch 169. Learning Rate: 0.001 Total Training Loss:  3.6589711053529754\n",
      "Epoch 170. Learning Rate: 0.001 Total Training Loss:  3.6535649499855936\n",
      "Epoch 171. Learning Rate: 0.001 Total Training Loss:  3.646924916189164\n",
      "Epoch 172. Learning Rate: 0.001 Total Training Loss:  3.6478537800721824\n",
      "Epoch 173. Learning Rate: 0.001 Total Training Loss:  3.640335587435402\n",
      "Epoch 174. Learning Rate: 0.001 Total Training Loss:  3.6429250807268545\n",
      "Epoch 175. Learning Rate: 0.001 Total Training Loss:  3.6340733418473974\n",
      "Epoch 176. Learning Rate: 0.001 Total Training Loss:  3.637780947959982\n",
      "Epoch 177. Learning Rate: 0.001 Total Training Loss:  3.6259857224067673\n",
      "Epoch 178. Learning Rate: 0.001 Total Training Loss:  3.621097720344551\n",
      "Epoch 179. Learning Rate: 0.001 Total Training Loss:  3.6155332272173837\n",
      "Epoch 180. Learning Rate: 0.001 Total Training Loss:  3.6103659152286127\n",
      "Epoch 181. Learning Rate: 0.001 Total Training Loss:  3.613535089418292\n",
      "Epoch 182. Learning Rate: 0.001 Total Training Loss:  3.608132409164682\n",
      "Epoch 183. Learning Rate: 0.001 Total Training Loss:  3.5980243941303343\n",
      "Epoch 184. Learning Rate: 0.001 Total Training Loss:  3.5917769296793267\n",
      "Epoch 185. Learning Rate: 0.001 Total Training Loss:  3.589242221438326\n",
      "Epoch 186. Learning Rate: 0.001 Total Training Loss:  3.591136214789003\n",
      "Epoch 187. Learning Rate: 0.001 Total Training Loss:  3.5885808329330757\n",
      "Epoch 188. Learning Rate: 0.001 Total Training Loss:  3.5779804985504597\n",
      "Epoch 189. Learning Rate: 0.001 Total Training Loss:  3.5808024829020724\n",
      "Epoch 190. Learning Rate: 0.001 Total Training Loss:  3.5717317410744727\n",
      "Epoch 191. Learning Rate: 0.001 Total Training Loss:  3.561745799612254\n",
      "Epoch 192. Learning Rate: 0.001 Total Training Loss:  3.566914326045662\n",
      "Epoch 193. Learning Rate: 0.001 Total Training Loss:  3.55605741974432\n",
      "Epoch 194. Learning Rate: 0.001 Total Training Loss:  3.5621149512007833\n",
      "Epoch 195. Learning Rate: 0.001 Total Training Loss:  3.5532733440632\n",
      "Epoch 196. Learning Rate: 0.001 Total Training Loss:  3.549029003479518\n",
      "Epoch 197. Learning Rate: 0.001 Total Training Loss:  3.546908754040487\n",
      "Epoch 198. Learning Rate: 0.001 Total Training Loss:  3.5341197294183075\n",
      "Epoch 199. Learning Rate: 0.001 Total Training Loss:  3.531677959836088\n",
      "Epoch 200. Learning Rate: 0.001 Total Training Loss:  3.5292528617428616\n",
      "Epoch 201. Learning Rate: 0.001 Total Training Loss:  3.523150634719059\n",
      "Epoch 202. Learning Rate: 0.001 Total Training Loss:  3.522492643329315\n",
      "Epoch 203. Learning Rate: 0.001 Total Training Loss:  3.5134671370033175\n",
      "Epoch 204. Learning Rate: 0.001 Total Training Loss:  3.511905844323337\n",
      "Epoch 205. Learning Rate: 0.001 Total Training Loss:  3.510873560560867\n",
      "Epoch 206. Learning Rate: 0.001 Total Training Loss:  3.505852176458575\n",
      "Epoch 207. Learning Rate: 0.001 Total Training Loss:  3.4977592992363498\n",
      "Epoch 208. Learning Rate: 0.001 Total Training Loss:  3.4946405676892027\n",
      "Epoch 209. Learning Rate: 0.001 Total Training Loss:  3.4904994493117556\n",
      "Epoch 210. Learning Rate: 0.001 Total Training Loss:  3.48920255783014\n",
      "Epoch 211. Learning Rate: 0.001 Total Training Loss:  3.4765582128893584\n",
      "Epoch 212. Learning Rate: 0.001 Total Training Loss:  3.4765898503828794\n",
      "Epoch 213. Learning Rate: 0.001 Total Training Loss:  3.4744691202649847\n",
      "Epoch 214. Learning Rate: 0.001 Total Training Loss:  3.4780169166624546\n",
      "Epoch 215. Learning Rate: 0.001 Total Training Loss:  3.468420341727324\n",
      "Epoch 216. Learning Rate: 0.001 Total Training Loss:  3.469383888761513\n",
      "Epoch 217. Learning Rate: 0.001 Total Training Loss:  3.461905434494838\n",
      "Epoch 218. Learning Rate: 0.001 Total Training Loss:  3.4643998835235834\n",
      "Epoch 219. Learning Rate: 0.001 Total Training Loss:  3.45079094870016\n",
      "Epoch 220. Learning Rate: 0.001 Total Training Loss:  3.4590663260314614\n",
      "Epoch 221. Learning Rate: 0.001 Total Training Loss:  3.4510234212502837\n",
      "Epoch 222. Learning Rate: 0.001 Total Training Loss:  3.4500759582733735\n",
      "Epoch 223. Learning Rate: 0.001 Total Training Loss:  3.4434984140098095\n",
      "Epoch 224. Learning Rate: 0.001 Total Training Loss:  3.443261208361946\n",
      "Epoch 225. Learning Rate: 0.001 Total Training Loss:  3.4334560120478272\n",
      "Epoch 226. Learning Rate: 0.001 Total Training Loss:  3.4325228717643768\n",
      "Epoch 227. Learning Rate: 0.001 Total Training Loss:  3.429517066339031\n",
      "Epoch 228. Learning Rate: 0.001 Total Training Loss:  3.43041354266461\n",
      "Epoch 229. Learning Rate: 0.001 Total Training Loss:  3.421187056344934\n",
      "Epoch 230. Learning Rate: 0.001 Total Training Loss:  3.4205390487331897\n",
      "Epoch 231. Learning Rate: 0.001 Total Training Loss:  3.420018255128525\n",
      "Epoch 232. Learning Rate: 0.001 Total Training Loss:  3.422161026392132\n",
      "Epoch 233. Learning Rate: 0.001 Total Training Loss:  3.418974810047075\n",
      "Epoch 234. Learning Rate: 0.001 Total Training Loss:  3.4189335126429796\n",
      "Epoch 235. Learning Rate: 0.001 Total Training Loss:  3.410412819706835\n",
      "Epoch 236. Learning Rate: 0.001 Total Training Loss:  3.4057220220565796\n",
      "Epoch 237. Learning Rate: 0.001 Total Training Loss:  3.4066641733516008\n",
      "Epoch 238. Learning Rate: 0.001 Total Training Loss:  3.404181088320911\n",
      "Epoch 239. Learning Rate: 0.001 Total Training Loss:  3.402816091431305\n",
      "Epoch 240. Learning Rate: 0.001 Total Training Loss:  3.400040652952157\n",
      "Epoch 241. Learning Rate: 0.001 Total Training Loss:  3.39864353928715\n",
      "Epoch 242. Learning Rate: 0.001 Total Training Loss:  3.4030961309326813\n",
      "Epoch 243. Learning Rate: 0.001 Total Training Loss:  3.4000320500927046\n",
      "Epoch 244. Learning Rate: 0.001 Total Training Loss:  3.3892494334140792\n",
      "Epoch 245. Learning Rate: 0.001 Total Training Loss:  3.3891646600095555\n",
      "Epoch 246. Learning Rate: 0.001 Total Training Loss:  3.388114359928295\n",
      "Epoch 247. Learning Rate: 0.001 Total Training Loss:  3.384397456655279\n",
      "Epoch 248. Learning Rate: 0.001 Total Training Loss:  3.381121023790911\n",
      "Epoch 249. Learning Rate: 0.001 Total Training Loss:  3.3806112720631063\n",
      "Epoch 250. Learning Rate: 0.001 Total Training Loss:  3.3803661230485886\n",
      "Epoch 251. Learning Rate: 0.001 Total Training Loss:  3.3713538968004286\n",
      "Epoch 252. Learning Rate: 0.001 Total Training Loss:  3.373820954700932\n",
      "Epoch 253. Learning Rate: 0.001 Total Training Loss:  3.368407260510139\n",
      "Epoch 254. Learning Rate: 0.001 Total Training Loss:  3.370362772955559\n",
      "Epoch 255. Learning Rate: 0.001 Total Training Loss:  3.35796469100751\n",
      "Epoch 256. Learning Rate: 0.001 Total Training Loss:  3.3696036064065993\n",
      "Epoch 257. Learning Rate: 0.001 Total Training Loss:  3.355225170380436\n",
      "Epoch 258. Learning Rate: 0.001 Total Training Loss:  3.3597379371058196\n",
      "Epoch 259. Learning Rate: 0.001 Total Training Loss:  3.3516354453749955\n",
      "Epoch 260. Learning Rate: 0.001 Total Training Loss:  3.3595555081265047\n",
      "Epoch 261. Learning Rate: 0.001 Total Training Loss:  3.3442921474343166\n",
      "Epoch 262. Learning Rate: 0.001 Total Training Loss:  3.347170996014029\n",
      "Epoch 263. Learning Rate: 0.001 Total Training Loss:  3.3423971332376823\n",
      "Epoch 264. Learning Rate: 0.001 Total Training Loss:  3.3405756787396967\n",
      "Epoch 265. Learning Rate: 0.001 Total Training Loss:  3.333280407357961\n",
      "Epoch 266. Learning Rate: 0.001 Total Training Loss:  3.3326423509279266\n",
      "Epoch 267. Learning Rate: 0.001 Total Training Loss:  3.3349495072616264\n",
      "Epoch 268. Learning Rate: 0.001 Total Training Loss:  3.331203056499362\n",
      "Epoch 269. Learning Rate: 0.001 Total Training Loss:  3.3279680234845728\n",
      "Epoch 270. Learning Rate: 0.001 Total Training Loss:  3.327073583728634\n",
      "Epoch 271. Learning Rate: 0.001 Total Training Loss:  3.3263183415401727\n",
      "Epoch 272. Learning Rate: 0.001 Total Training Loss:  3.3172907531261444\n",
      "Epoch 273. Learning Rate: 0.001 Total Training Loss:  3.3256593148689717\n",
      "Epoch 274. Learning Rate: 0.001 Total Training Loss:  3.310469724587165\n",
      "Epoch 275. Learning Rate: 0.001 Total Training Loss:  3.3200756383594126\n",
      "Epoch 276. Learning Rate: 0.001 Total Training Loss:  3.310466267284937\n",
      "Epoch 277. Learning Rate: 0.001 Total Training Loss:  3.3080133623443544\n",
      "Epoch 278. Learning Rate: 0.001 Total Training Loss:  3.3010075371712446\n",
      "Epoch 279. Learning Rate: 0.001 Total Training Loss:  3.3107828189386055\n",
      "Epoch 280. Learning Rate: 0.001 Total Training Loss:  3.301278119091876\n",
      "Epoch 281. Learning Rate: 0.001 Total Training Loss:  3.2947233519516885\n",
      "Epoch 282. Learning Rate: 0.001 Total Training Loss:  3.3039746278664097\n",
      "Epoch 283. Learning Rate: 0.001 Total Training Loss:  3.2942754848627374\n",
      "Epoch 284. Learning Rate: 0.001 Total Training Loss:  3.2975914156995714\n",
      "Epoch 285. Learning Rate: 0.001 Total Training Loss:  3.289205778040923\n",
      "Epoch 286. Learning Rate: 0.001 Total Training Loss:  3.28696893772576\n",
      "Epoch 287. Learning Rate: 0.001 Total Training Loss:  3.2892011924413964\n",
      "Epoch 288. Learning Rate: 0.001 Total Training Loss:  3.278549174196087\n",
      "Epoch 289. Learning Rate: 0.001 Total Training Loss:  3.2867758438223973\n",
      "Epoch 290. Learning Rate: 0.001 Total Training Loss:  3.2804943948285654\n",
      "Epoch 291. Learning Rate: 0.001 Total Training Loss:  3.286298979539424\n",
      "Epoch 292. Learning Rate: 0.001 Total Training Loss:  3.2731037552002817\n",
      "Epoch 293. Learning Rate: 0.001 Total Training Loss:  3.2748692621244118\n",
      "Epoch 294. Learning Rate: 0.001 Total Training Loss:  3.27750425436534\n",
      "Epoch 295. Learning Rate: 0.001 Total Training Loss:  3.273454263107851\n",
      "Epoch 296. Learning Rate: 0.001 Total Training Loss:  3.275409809197299\n",
      "Epoch 297. Learning Rate: 0.001 Total Training Loss:  3.2687150444835424\n",
      "Epoch 298. Learning Rate: 0.001 Total Training Loss:  3.2626480341423303\n",
      "Epoch 299. Learning Rate: 0.001 Total Training Loss:  3.2637227958766744\n",
      "Epoch 300. Learning Rate: 0.001 Total Training Loss:  3.262709965230897\n",
      "Epoch 301. Learning Rate: 0.001 Total Training Loss:  3.256168214837089\n",
      "Epoch 302. Learning Rate: 0.001 Total Training Loss:  3.254523403244093\n",
      "Epoch 303. Learning Rate: 0.001 Total Training Loss:  3.2603289891267195\n",
      "Epoch 304. Learning Rate: 0.001 Total Training Loss:  3.2474809555569664\n",
      "Epoch 305. Learning Rate: 0.001 Total Training Loss:  3.2459656033897772\n",
      "Epoch 306. Learning Rate: 0.001 Total Training Loss:  3.243817232782021\n",
      "Epoch 307. Learning Rate: 0.001 Total Training Loss:  3.2398766508558765\n",
      "Epoch 308. Learning Rate: 0.001 Total Training Loss:  3.243811045656912\n",
      "Epoch 309. Learning Rate: 0.001 Total Training Loss:  3.2436340099666268\n",
      "Epoch 310. Learning Rate: 0.001 Total Training Loss:  3.239234425476752\n",
      "Epoch 311. Learning Rate: 0.001 Total Training Loss:  3.2365175950108096\n",
      "Epoch 312. Learning Rate: 0.001 Total Training Loss:  3.2362924367189407\n",
      "Epoch 313. Learning Rate: 0.001 Total Training Loss:  3.2346002272097394\n",
      "Epoch 314. Learning Rate: 0.001 Total Training Loss:  3.223916219896637\n",
      "Epoch 315. Learning Rate: 0.001 Total Training Loss:  3.2346785576082766\n",
      "Epoch 316. Learning Rate: 0.001 Total Training Loss:  3.221317595685832\n",
      "Epoch 317. Learning Rate: 0.001 Total Training Loss:  3.225064810481854\n",
      "Epoch 318. Learning Rate: 0.001 Total Training Loss:  3.215857765986584\n",
      "Epoch 319. Learning Rate: 0.001 Total Training Loss:  3.214938539545983\n",
      "Epoch 320. Learning Rate: 0.001 Total Training Loss:  3.218063541688025\n",
      "Epoch 321. Learning Rate: 0.001 Total Training Loss:  3.2106706417398527\n",
      "Epoch 322. Learning Rate: 0.001 Total Training Loss:  3.2107028737664223\n",
      "Epoch 323. Learning Rate: 0.001 Total Training Loss:  3.208576685981825\n",
      "Epoch 324. Learning Rate: 0.001 Total Training Loss:  3.2048502090619877\n",
      "Epoch 325. Learning Rate: 0.001 Total Training Loss:  3.212009798386134\n",
      "Epoch 326. Learning Rate: 0.001 Total Training Loss:  3.203420059522614\n",
      "Epoch 327. Learning Rate: 0.001 Total Training Loss:  3.2001166966510937\n",
      "Epoch 328. Learning Rate: 0.001 Total Training Loss:  3.199941118946299\n",
      "Epoch 329. Learning Rate: 0.001 Total Training Loss:  3.196809480432421\n",
      "Epoch 330. Learning Rate: 0.001 Total Training Loss:  3.2009793674806133\n",
      "Epoch 331. Learning Rate: 0.001 Total Training Loss:  3.1932134975213557\n",
      "Epoch 332. Learning Rate: 0.001 Total Training Loss:  3.1951443441212177\n",
      "Epoch 333. Learning Rate: 0.001 Total Training Loss:  3.1901522744446993\n",
      "Epoch 334. Learning Rate: 0.001 Total Training Loss:  3.1899691454600543\n",
      "Epoch 335. Learning Rate: 0.001 Total Training Loss:  3.1874533789232373\n",
      "Epoch 336. Learning Rate: 0.001 Total Training Loss:  3.181625986006111\n",
      "Epoch 337. Learning Rate: 0.001 Total Training Loss:  3.1778466067044064\n",
      "Epoch 338. Learning Rate: 0.001 Total Training Loss:  3.177214619820006\n",
      "Epoch 339. Learning Rate: 0.001 Total Training Loss:  3.175501605728641\n",
      "Epoch 340. Learning Rate: 0.001 Total Training Loss:  3.181756641017273\n",
      "Epoch 341. Learning Rate: 0.001 Total Training Loss:  3.1698812189279124\n",
      "Epoch 342. Learning Rate: 0.001 Total Training Loss:  3.16849359346088\n",
      "Epoch 343. Learning Rate: 0.001 Total Training Loss:  3.158860737225041\n",
      "Epoch 344. Learning Rate: 0.001 Total Training Loss:  3.1671962491236627\n",
      "Epoch 345. Learning Rate: 0.001 Total Training Loss:  3.1609598769573495\n",
      "Epoch 346. Learning Rate: 0.001 Total Training Loss:  3.1534597044810653\n",
      "Epoch 347. Learning Rate: 0.001 Total Training Loss:  3.1554555140901357\n",
      "Epoch 348. Learning Rate: 0.001 Total Training Loss:  3.1574429247993976\n",
      "Epoch 349. Learning Rate: 0.001 Total Training Loss:  3.159156854264438\n",
      "Epoch 350. Learning Rate: 0.001 Total Training Loss:  3.141534012975171\n",
      "Epoch 351. Learning Rate: 0.001 Total Training Loss:  3.1473630368709564\n",
      "Epoch 352. Learning Rate: 0.001 Total Training Loss:  3.1413561647059396\n",
      "Epoch 353. Learning Rate: 0.001 Total Training Loss:  3.138043757295236\n",
      "Epoch 354. Learning Rate: 0.001 Total Training Loss:  3.143325496581383\n",
      "Epoch 355. Learning Rate: 0.001 Total Training Loss:  3.1405946837039664\n",
      "Epoch 356. Learning Rate: 0.001 Total Training Loss:  3.12930479622446\n",
      "Epoch 357. Learning Rate: 0.001 Total Training Loss:  3.1276508377632126\n",
      "Epoch 358. Learning Rate: 0.001 Total Training Loss:  3.1177429971285164\n",
      "Epoch 359. Learning Rate: 0.001 Total Training Loss:  3.1206475018989295\n",
      "Epoch 360. Learning Rate: 0.001 Total Training Loss:  3.1250635720789433\n",
      "Epoch 361. Learning Rate: 0.001 Total Training Loss:  3.112467876402661\n",
      "Epoch 362. Learning Rate: 0.001 Total Training Loss:  3.114687826251611\n",
      "Epoch 363. Learning Rate: 0.001 Total Training Loss:  3.11849504976999\n",
      "Epoch 364. Learning Rate: 0.001 Total Training Loss:  3.1174358897842467\n",
      "Epoch 365. Learning Rate: 0.001 Total Training Loss:  3.1144661692669615\n",
      "Epoch 366. Learning Rate: 0.001 Total Training Loss:  3.10131374548655\n",
      "Epoch 367. Learning Rate: 0.001 Total Training Loss:  3.1047934491652995\n",
      "Epoch 368. Learning Rate: 0.001 Total Training Loss:  3.1101998949889094\n",
      "Epoch 369. Learning Rate: 0.001 Total Training Loss:  3.0926419051829726\n",
      "Epoch 370. Learning Rate: 0.001 Total Training Loss:  3.0980576291913167\n",
      "Epoch 371. Learning Rate: 0.001 Total Training Loss:  3.09391302021686\n",
      "Epoch 372. Learning Rate: 0.001 Total Training Loss:  3.1014312090119347\n",
      "Epoch 373. Learning Rate: 0.001 Total Training Loss:  3.0810768061783165\n",
      "Epoch 374. Learning Rate: 0.001 Total Training Loss:  3.0886635795468464\n",
      "Epoch 375. Learning Rate: 0.001 Total Training Loss:  3.0757374039385468\n",
      "Epoch 376. Learning Rate: 0.001 Total Training Loss:  3.0762925206217915\n",
      "Epoch 377. Learning Rate: 0.001 Total Training Loss:  3.0726457758573815\n",
      "Epoch 378. Learning Rate: 0.001 Total Training Loss:  3.0710025461157784\n",
      "Epoch 379. Learning Rate: 0.001 Total Training Loss:  3.0618815616471693\n",
      "Epoch 380. Learning Rate: 0.001 Total Training Loss:  3.067044273717329\n",
      "Epoch 381. Learning Rate: 0.001 Total Training Loss:  3.0608468933496624\n",
      "Epoch 382. Learning Rate: 0.001 Total Training Loss:  3.0524024289334193\n",
      "Epoch 383. Learning Rate: 0.001 Total Training Loss:  3.0610623826505616\n",
      "Epoch 384. Learning Rate: 0.001 Total Training Loss:  3.0529703710926697\n",
      "Epoch 385. Learning Rate: 0.001 Total Training Loss:  3.062165429466404\n",
      "Epoch 386. Learning Rate: 0.001 Total Training Loss:  3.0434002995025367\n",
      "Epoch 387. Learning Rate: 0.001 Total Training Loss:  3.0576203603995964\n",
      "Epoch 388. Learning Rate: 0.001 Total Training Loss:  3.037789173074998\n",
      "Epoch 389. Learning Rate: 0.001 Total Training Loss:  3.04302417836152\n",
      "Epoch 390. Learning Rate: 0.001 Total Training Loss:  3.028281378792599\n",
      "Epoch 391. Learning Rate: 0.001 Total Training Loss:  3.041513789445162\n",
      "Epoch 392. Learning Rate: 0.001 Total Training Loss:  3.0284391946624964\n",
      "Epoch 393. Learning Rate: 0.001 Total Training Loss:  3.0383958019083366\n",
      "Epoch 394. Learning Rate: 0.001 Total Training Loss:  3.024190798983909\n",
      "Epoch 395. Learning Rate: 0.001 Total Training Loss:  3.0279971092240885\n",
      "Epoch 396. Learning Rate: 0.001 Total Training Loss:  3.0172594259493053\n",
      "Epoch 397. Learning Rate: 0.001 Total Training Loss:  3.029459262616001\n",
      "Epoch 398. Learning Rate: 0.001 Total Training Loss:  3.0094240725738928\n",
      "Epoch 399. Learning Rate: 0.001 Total Training Loss:  3.0088591176318005\n",
      "Epoch 400. Learning Rate: 0.001 Total Training Loss:  2.999050834798254\n",
      "Epoch 401. Learning Rate: 0.001 Total Training Loss:  3.001214454183355\n",
      "Epoch 402. Learning Rate: 0.001 Total Training Loss:  3.0125118733849376\n",
      "Epoch 403. Learning Rate: 0.001 Total Training Loss:  2.994723659590818\n",
      "Epoch 404. Learning Rate: 0.001 Total Training Loss:  3.0000098244054243\n",
      "Epoch 405. Learning Rate: 0.001 Total Training Loss:  3.002767345053144\n",
      "Epoch 406. Learning Rate: 0.001 Total Training Loss:  2.98648362304084\n",
      "Epoch 407. Learning Rate: 0.001 Total Training Loss:  2.9976255914662033\n",
      "Epoch 408. Learning Rate: 0.001 Total Training Loss:  2.9858746168902144\n",
      "Epoch 409. Learning Rate: 0.001 Total Training Loss:  2.985252598300576\n",
      "Epoch 410. Learning Rate: 0.001 Total Training Loss:  2.9793170606717467\n",
      "Epoch 411. Learning Rate: 0.001 Total Training Loss:  2.9781200302531943\n",
      "Epoch 412. Learning Rate: 0.001 Total Training Loss:  2.9703651985619217\n",
      "Epoch 413. Learning Rate: 0.001 Total Training Loss:  2.985287099494599\n",
      "Epoch 414. Learning Rate: 0.001 Total Training Loss:  2.9731219056993723\n",
      "Epoch 415. Learning Rate: 0.001 Total Training Loss:  2.972750414395705\n",
      "Epoch 416. Learning Rate: 0.001 Total Training Loss:  2.9696438948158175\n",
      "Epoch 417. Learning Rate: 0.001 Total Training Loss:  2.9623029339127243\n",
      "Epoch 418. Learning Rate: 0.001 Total Training Loss:  2.97097289795056\n",
      "Epoch 419. Learning Rate: 0.001 Total Training Loss:  2.9674136987887323\n",
      "Epoch 420. Learning Rate: 0.001 Total Training Loss:  2.9676584298722446\n",
      "Epoch 421. Learning Rate: 0.001 Total Training Loss:  2.9715034724213183\n",
      "Epoch 422. Learning Rate: 0.001 Total Training Loss:  2.9658164186403155\n",
      "Epoch 423. Learning Rate: 0.001 Total Training Loss:  2.9543664706870914\n",
      "Epoch 424. Learning Rate: 0.001 Total Training Loss:  2.951039157807827\n",
      "Epoch 425. Learning Rate: 0.001 Total Training Loss:  2.961594021646306\n",
      "Epoch 426. Learning Rate: 0.001 Total Training Loss:  2.9583750928286463\n",
      "Epoch 427. Learning Rate: 0.001 Total Training Loss:  2.9614523260388523\n",
      "Epoch 428. Learning Rate: 0.001 Total Training Loss:  2.9460516520775855\n",
      "Epoch 429. Learning Rate: 0.001 Total Training Loss:  2.940176860662177\n",
      "Epoch 430. Learning Rate: 0.001 Total Training Loss:  2.9396850822959095\n",
      "Epoch 431. Learning Rate: 0.001 Total Training Loss:  2.9424142921343446\n",
      "Epoch 432. Learning Rate: 0.001 Total Training Loss:  2.931516725104302\n",
      "Epoch 433. Learning Rate: 0.001 Total Training Loss:  2.927193970885128\n",
      "Epoch 434. Learning Rate: 0.001 Total Training Loss:  2.9400982172228396\n",
      "Epoch 435. Learning Rate: 0.001 Total Training Loss:  2.932509628124535\n",
      "Epoch 436. Learning Rate: 0.001 Total Training Loss:  2.930443243123591\n",
      "Epoch 437. Learning Rate: 0.001 Total Training Loss:  2.948200188810006\n",
      "Epoch 438. Learning Rate: 0.001 Total Training Loss:  2.9277138102333993\n",
      "Epoch 439. Learning Rate: 0.001 Total Training Loss:  2.925487619359046\n",
      "Epoch 440. Learning Rate: 0.001 Total Training Loss:  2.914493673015386\n",
      "Epoch 441. Learning Rate: 0.001 Total Training Loss:  2.917064092354849\n",
      "Epoch 442. Learning Rate: 0.001 Total Training Loss:  2.9095935004297644\n",
      "Epoch 443. Learning Rate: 0.001 Total Training Loss:  2.911464552162215\n",
      "Epoch 444. Learning Rate: 0.001 Total Training Loss:  2.9012771653942764\n",
      "Epoch 445. Learning Rate: 0.001 Total Training Loss:  2.9139089591335505\n",
      "Epoch 446. Learning Rate: 0.001 Total Training Loss:  2.90426600840874\n",
      "Epoch 447. Learning Rate: 0.001 Total Training Loss:  2.8961326614953578\n",
      "Epoch 448. Learning Rate: 0.001 Total Training Loss:  2.9040410483721644\n",
      "Epoch 449. Learning Rate: 0.001 Total Training Loss:  2.897883638739586\n",
      "Epoch 450. Learning Rate: 0.001 Total Training Loss:  2.8948306352831423\n",
      "Epoch 451. Learning Rate: 0.001 Total Training Loss:  2.8919428430963308\n",
      "Epoch 452. Learning Rate: 0.001 Total Training Loss:  2.88182053854689\n",
      "Epoch 453. Learning Rate: 0.001 Total Training Loss:  2.8888916654977947\n",
      "Epoch 454. Learning Rate: 0.001 Total Training Loss:  2.879947848850861\n",
      "Epoch 455. Learning Rate: 0.001 Total Training Loss:  2.8844366357661784\n",
      "Epoch 456. Learning Rate: 0.001 Total Training Loss:  2.8742954602930695\n",
      "Epoch 457. Learning Rate: 0.001 Total Training Loss:  2.884793276898563\n",
      "Epoch 458. Learning Rate: 0.001 Total Training Loss:  2.868897589389235\n",
      "Epoch 459. Learning Rate: 0.001 Total Training Loss:  2.864450277062133\n",
      "Epoch 460. Learning Rate: 0.001 Total Training Loss:  2.867232088930905\n",
      "Epoch 461. Learning Rate: 0.001 Total Training Loss:  2.8693546417634934\n",
      "Epoch 462. Learning Rate: 0.001 Total Training Loss:  2.8660807558335364\n",
      "Epoch 463. Learning Rate: 0.001 Total Training Loss:  2.8693169872276485\n",
      "Epoch 464. Learning Rate: 0.001 Total Training Loss:  2.858754671411589\n",
      "Epoch 465. Learning Rate: 0.001 Total Training Loss:  2.857288003899157\n",
      "Epoch 466. Learning Rate: 0.001 Total Training Loss:  2.8604437459725887\n",
      "Epoch 467. Learning Rate: 0.001 Total Training Loss:  2.8615014490205795\n",
      "Epoch 468. Learning Rate: 0.001 Total Training Loss:  2.85866538179107\n",
      "Epoch 469. Learning Rate: 0.001 Total Training Loss:  2.848131609847769\n",
      "Epoch 470. Learning Rate: 0.001 Total Training Loss:  2.8543065302073956\n",
      "Epoch 471. Learning Rate: 0.001 Total Training Loss:  2.847502879332751\n",
      "Epoch 472. Learning Rate: 0.001 Total Training Loss:  2.837971737375483\n",
      "Epoch 473. Learning Rate: 0.001 Total Training Loss:  2.8395290919579566\n",
      "Epoch 474. Learning Rate: 0.001 Total Training Loss:  2.838248775806278\n",
      "Epoch 475. Learning Rate: 0.001 Total Training Loss:  2.843259879620746\n",
      "Epoch 476. Learning Rate: 0.001 Total Training Loss:  2.8358816525433213\n",
      "Epoch 477. Learning Rate: 0.001 Total Training Loss:  2.8337475436273962\n",
      "Epoch 478. Learning Rate: 0.001 Total Training Loss:  2.8232520387973636\n",
      "Epoch 479. Learning Rate: 0.001 Total Training Loss:  2.8451805454678833\n",
      "Epoch 480. Learning Rate: 0.001 Total Training Loss:  2.8196629236917943\n",
      "Epoch 481. Learning Rate: 0.001 Total Training Loss:  2.823548979125917\n",
      "Epoch 482. Learning Rate: 0.001 Total Training Loss:  2.8296583760529757\n",
      "Epoch 483. Learning Rate: 0.001 Total Training Loss:  2.826280748238787\n",
      "Epoch 484. Learning Rate: 0.001 Total Training Loss:  2.8204042587894946\n",
      "Epoch 485. Learning Rate: 0.001 Total Training Loss:  2.837320617167279\n",
      "Epoch 486. Learning Rate: 0.001 Total Training Loss:  2.813836687942967\n",
      "Epoch 487. Learning Rate: 0.001 Total Training Loss:  2.810763857793063\n",
      "Epoch 488. Learning Rate: 0.001 Total Training Loss:  2.812774908496067\n",
      "Epoch 489. Learning Rate: 0.001 Total Training Loss:  2.8088889697100967\n",
      "Epoch 490. Learning Rate: 0.001 Total Training Loss:  2.8088045364711434\n",
      "Epoch 491. Learning Rate: 0.001 Total Training Loss:  2.8127446493599564\n",
      "Epoch 492. Learning Rate: 0.001 Total Training Loss:  2.8076871705707163\n",
      "Epoch 493. Learning Rate: 0.001 Total Training Loss:  2.8049588014837354\n",
      "Epoch 494. Learning Rate: 0.001 Total Training Loss:  2.7937098839320242\n",
      "Epoch 495. Learning Rate: 0.001 Total Training Loss:  2.8003247620072216\n",
      "Epoch 496. Learning Rate: 0.001 Total Training Loss:  2.801668249303475\n",
      "Epoch 497. Learning Rate: 0.001 Total Training Loss:  2.7937857951037586\n",
      "Epoch 498. Learning Rate: 0.001 Total Training Loss:  2.7927708758506924\n",
      "Epoch 499. Learning Rate: 0.001 Total Training Loss:  2.784388408297673\n",
      "Epoch 500. Learning Rate: 0.0005 Total Training Loss:  2.795672244625166\n",
      "Epoch 501. Learning Rate: 0.0005 Total Training Loss:  2.846683270530775\n",
      "Epoch 502. Learning Rate: 0.0005 Total Training Loss:  2.678837709594518\n",
      "Epoch 503. Learning Rate: 0.0005 Total Training Loss:  2.628700688597746\n",
      "Epoch 504. Learning Rate: 0.0005 Total Training Loss:  2.6107796444557607\n",
      "Epoch 505. Learning Rate: 0.0005 Total Training Loss:  2.6090963918250054\n",
      "Epoch 506. Learning Rate: 0.0005 Total Training Loss:  2.609245542087592\n",
      "Epoch 507. Learning Rate: 0.0005 Total Training Loss:  2.6024404966738075\n",
      "Epoch 508. Learning Rate: 0.0005 Total Training Loss:  2.601151789771393\n",
      "Epoch 509. Learning Rate: 0.0005 Total Training Loss:  2.5988801795756444\n",
      "Epoch 510. Learning Rate: 0.0005 Total Training Loss:  2.5938046359224245\n",
      "Epoch 511. Learning Rate: 0.0005 Total Training Loss:  2.5978779984870926\n",
      "Epoch 512. Learning Rate: 0.0005 Total Training Loss:  2.5927665270864964\n",
      "Epoch 513. Learning Rate: 0.0005 Total Training Loss:  2.593408182496205\n",
      "Epoch 514. Learning Rate: 0.0005 Total Training Loss:  2.5870731248287484\n",
      "Epoch 515. Learning Rate: 0.0005 Total Training Loss:  2.59030146303121\n",
      "Epoch 516. Learning Rate: 0.0005 Total Training Loss:  2.5887536484515294\n",
      "Epoch 517. Learning Rate: 0.0005 Total Training Loss:  2.5836259385105222\n",
      "Epoch 518. Learning Rate: 0.0005 Total Training Loss:  2.5856450070859864\n",
      "Epoch 519. Learning Rate: 0.0005 Total Training Loss:  2.587826421717182\n",
      "Epoch 520. Learning Rate: 0.0005 Total Training Loss:  2.5845899037085474\n",
      "Epoch 521. Learning Rate: 0.0005 Total Training Loss:  2.5840138434432447\n",
      "Epoch 522. Learning Rate: 0.0005 Total Training Loss:  2.579782545915805\n",
      "Epoch 523. Learning Rate: 0.0005 Total Training Loss:  2.57913613691926\n",
      "Epoch 524. Learning Rate: 0.0005 Total Training Loss:  2.5780318119795993\n",
      "Epoch 525. Learning Rate: 0.0005 Total Training Loss:  2.575084738084115\n",
      "Epoch 526. Learning Rate: 0.0005 Total Training Loss:  2.5729926036437973\n",
      "Epoch 527. Learning Rate: 0.0005 Total Training Loss:  2.5750050814822316\n",
      "Epoch 528. Learning Rate: 0.0005 Total Training Loss:  2.5741425341693684\n",
      "Epoch 529. Learning Rate: 0.0005 Total Training Loss:  2.5749350442783907\n",
      "Epoch 530. Learning Rate: 0.0005 Total Training Loss:  2.5714208749122918\n",
      "Epoch 531. Learning Rate: 0.0005 Total Training Loss:  2.5691093472996727\n",
      "Epoch 532. Learning Rate: 0.0005 Total Training Loss:  2.570137236150913\n",
      "Epoch 533. Learning Rate: 0.0005 Total Training Loss:  2.565564133110456\n",
      "Epoch 534. Learning Rate: 0.0005 Total Training Loss:  2.566633531358093\n",
      "Epoch 535. Learning Rate: 0.0005 Total Training Loss:  2.569921607035212\n",
      "Epoch 536. Learning Rate: 0.0005 Total Training Loss:  2.566254273056984\n",
      "Epoch 537. Learning Rate: 0.0005 Total Training Loss:  2.561878753709607\n",
      "Epoch 538. Learning Rate: 0.0005 Total Training Loss:  2.567122428212315\n",
      "Epoch 539. Learning Rate: 0.0005 Total Training Loss:  2.5610925584333017\n",
      "Epoch 540. Learning Rate: 0.0005 Total Training Loss:  2.559727811603807\n",
      "Epoch 541. Learning Rate: 0.0005 Total Training Loss:  2.5617728129727766\n",
      "Epoch 542. Learning Rate: 0.0005 Total Training Loss:  2.560362118878402\n",
      "Epoch 543. Learning Rate: 0.0005 Total Training Loss:  2.555379654862918\n",
      "Epoch 544. Learning Rate: 0.0005 Total Training Loss:  2.5580225004814565\n",
      "Epoch 545. Learning Rate: 0.0005 Total Training Loss:  2.55715412448626\n",
      "Epoch 546. Learning Rate: 0.0005 Total Training Loss:  2.551325100241229\n",
      "Epoch 547. Learning Rate: 0.0005 Total Training Loss:  2.5538456671638414\n",
      "Epoch 548. Learning Rate: 0.0005 Total Training Loss:  2.553198951529339\n",
      "Epoch 549. Learning Rate: 0.0005 Total Training Loss:  2.5544467832660303\n",
      "Epoch 550. Learning Rate: 0.0005 Total Training Loss:  2.550315002561547\n",
      "Epoch 551. Learning Rate: 0.0005 Total Training Loss:  2.550757000106387\n",
      "Epoch 552. Learning Rate: 0.0005 Total Training Loss:  2.5501612795051187\n",
      "Epoch 553. Learning Rate: 0.0005 Total Training Loss:  2.5457880422472954\n",
      "Epoch 554. Learning Rate: 0.0005 Total Training Loss:  2.5479388083331287\n",
      "Epoch 555. Learning Rate: 0.0005 Total Training Loss:  2.5454803213942796\n",
      "Epoch 556. Learning Rate: 0.0005 Total Training Loss:  2.5441673021996394\n",
      "Epoch 557. Learning Rate: 0.0005 Total Training Loss:  2.5460436694556847\n",
      "Epoch 558. Learning Rate: 0.0005 Total Training Loss:  2.539773891447112\n",
      "Epoch 559. Learning Rate: 0.0005 Total Training Loss:  2.542195724789053\n",
      "Epoch 560. Learning Rate: 0.0005 Total Training Loss:  2.541815573698841\n",
      "Epoch 561. Learning Rate: 0.0005 Total Training Loss:  2.54479808639735\n",
      "Epoch 562. Learning Rate: 0.0005 Total Training Loss:  2.538233614177443\n",
      "Epoch 563. Learning Rate: 0.0005 Total Training Loss:  2.5386092187836766\n",
      "Epoch 564. Learning Rate: 0.0005 Total Training Loss:  2.5361277513438836\n",
      "Epoch 565. Learning Rate: 0.0005 Total Training Loss:  2.537702545057982\n",
      "Epoch 566. Learning Rate: 0.0005 Total Training Loss:  2.535003291675821\n",
      "Epoch 567. Learning Rate: 0.0005 Total Training Loss:  2.535626155557111\n",
      "Epoch 568. Learning Rate: 0.0005 Total Training Loss:  2.5359989574644715\n",
      "Epoch 569. Learning Rate: 0.0005 Total Training Loss:  2.5306346628349274\n",
      "Epoch 570. Learning Rate: 0.0005 Total Training Loss:  2.5306734959594905\n",
      "Epoch 571. Learning Rate: 0.0005 Total Training Loss:  2.528842621599324\n",
      "Epoch 572. Learning Rate: 0.0005 Total Training Loss:  2.533974925871007\n",
      "Epoch 573. Learning Rate: 0.0005 Total Training Loss:  2.5258780979784206\n",
      "Epoch 574. Learning Rate: 0.0005 Total Training Loss:  2.528685620985925\n",
      "Epoch 575. Learning Rate: 0.0005 Total Training Loss:  2.528169411700219\n",
      "Epoch 576. Learning Rate: 0.0005 Total Training Loss:  2.52454422484152\n",
      "Epoch 577. Learning Rate: 0.0005 Total Training Loss:  2.524907769402489\n",
      "Epoch 578. Learning Rate: 0.0005 Total Training Loss:  2.5198937855893746\n",
      "Epoch 579. Learning Rate: 0.0005 Total Training Loss:  2.522366491262801\n",
      "Epoch 580. Learning Rate: 0.0005 Total Training Loss:  2.5194146065041423\n",
      "Epoch 581. Learning Rate: 0.0005 Total Training Loss:  2.5185096408240497\n",
      "Epoch 582. Learning Rate: 0.0005 Total Training Loss:  2.5189869491150603\n",
      "Epoch 583. Learning Rate: 0.0005 Total Training Loss:  2.5181580404751003\n",
      "Epoch 584. Learning Rate: 0.0005 Total Training Loss:  2.512789117055945\n",
      "Epoch 585. Learning Rate: 0.0005 Total Training Loss:  2.5142051946604624\n",
      "Epoch 586. Learning Rate: 0.0005 Total Training Loss:  2.514420474297367\n",
      "Epoch 587. Learning Rate: 0.0005 Total Training Loss:  2.510714833275415\n",
      "Epoch 588. Learning Rate: 0.0005 Total Training Loss:  2.515591154107824\n",
      "Epoch 589. Learning Rate: 0.0005 Total Training Loss:  2.509596685995348\n",
      "Epoch 590. Learning Rate: 0.0005 Total Training Loss:  2.5143179419683293\n",
      "Epoch 591. Learning Rate: 0.0005 Total Training Loss:  2.507520764367655\n",
      "Epoch 592. Learning Rate: 0.0005 Total Training Loss:  2.5109887125436217\n",
      "Epoch 593. Learning Rate: 0.0005 Total Training Loss:  2.510030213627033\n",
      "Epoch 594. Learning Rate: 0.0005 Total Training Loss:  2.505569611908868\n",
      "Epoch 595. Learning Rate: 0.0005 Total Training Loss:  2.5055976222502068\n",
      "Epoch 596. Learning Rate: 0.0005 Total Training Loss:  2.502530849073082\n",
      "Epoch 597. Learning Rate: 0.0005 Total Training Loss:  2.502494820859283\n",
      "Epoch 598. Learning Rate: 0.0005 Total Training Loss:  2.5023211361840367\n",
      "Epoch 599. Learning Rate: 0.0005 Total Training Loss:  2.5066459940280765\n",
      "Epoch 600. Learning Rate: 0.0005 Total Training Loss:  2.4992492174496874\n",
      "Epoch 601. Learning Rate: 0.0005 Total Training Loss:  2.5046510306419805\n",
      "Epoch 602. Learning Rate: 0.0005 Total Training Loss:  2.498872907133773\n",
      "Epoch 603. Learning Rate: 0.0005 Total Training Loss:  2.49695479858201\n",
      "Epoch 604. Learning Rate: 0.0005 Total Training Loss:  2.494588561821729\n",
      "Epoch 605. Learning Rate: 0.0005 Total Training Loss:  2.4966283125104383\n",
      "Epoch 606. Learning Rate: 0.0005 Total Training Loss:  2.498409688589163\n",
      "Epoch 607. Learning Rate: 0.0005 Total Training Loss:  2.490490307682194\n",
      "Epoch 608. Learning Rate: 0.0005 Total Training Loss:  2.497964514652267\n",
      "Epoch 609. Learning Rate: 0.0005 Total Training Loss:  2.4896264355629683\n",
      "Epoch 610. Learning Rate: 0.0005 Total Training Loss:  2.4907010968308896\n",
      "Epoch 611. Learning Rate: 0.0005 Total Training Loss:  2.491059887688607\n",
      "Epoch 612. Learning Rate: 0.0005 Total Training Loss:  2.493755302624777\n",
      "Epoch 613. Learning Rate: 0.0005 Total Training Loss:  2.4895043466240168\n",
      "Epoch 614. Learning Rate: 0.0005 Total Training Loss:  2.488012191723101\n",
      "Epoch 615. Learning Rate: 0.0005 Total Training Loss:  2.4921232868218794\n",
      "Epoch 616. Learning Rate: 0.0005 Total Training Loss:  2.4855910593178123\n",
      "Epoch 617. Learning Rate: 0.0005 Total Training Loss:  2.483159804251045\n",
      "Epoch 618. Learning Rate: 0.0005 Total Training Loss:  2.484757488593459\n",
      "Epoch 619. Learning Rate: 0.0005 Total Training Loss:  2.482454388635233\n",
      "Epoch 620. Learning Rate: 0.0005 Total Training Loss:  2.4826191758038476\n",
      "Epoch 621. Learning Rate: 0.0005 Total Training Loss:  2.484020968200639\n",
      "Epoch 622. Learning Rate: 0.0005 Total Training Loss:  2.4809575009858236\n",
      "Epoch 623. Learning Rate: 0.0005 Total Training Loss:  2.482031462015584\n",
      "Epoch 624. Learning Rate: 0.0005 Total Training Loss:  2.4780716608511284\n",
      "Epoch 625. Learning Rate: 0.0005 Total Training Loss:  2.477763882954605\n",
      "Epoch 626. Learning Rate: 0.0005 Total Training Loss:  2.4777611662866548\n",
      "Epoch 627. Learning Rate: 0.0005 Total Training Loss:  2.477760327165015\n",
      "Epoch 628. Learning Rate: 0.0005 Total Training Loss:  2.474014408653602\n",
      "Epoch 629. Learning Rate: 0.0005 Total Training Loss:  2.473580995807424\n",
      "Epoch 630. Learning Rate: 0.0005 Total Training Loss:  2.4731371686793864\n",
      "Epoch 631. Learning Rate: 0.0005 Total Training Loss:  2.472409699577838\n",
      "Epoch 632. Learning Rate: 0.0005 Total Training Loss:  2.472445936873555\n",
      "Epoch 633. Learning Rate: 0.0005 Total Training Loss:  2.4729727475205436\n",
      "Epoch 634. Learning Rate: 0.0005 Total Training Loss:  2.468636153033003\n",
      "Epoch 635. Learning Rate: 0.0005 Total Training Loss:  2.470256609027274\n",
      "Epoch 636. Learning Rate: 0.0005 Total Training Loss:  2.469981179572642\n",
      "Epoch 637. Learning Rate: 0.0005 Total Training Loss:  2.466257376363501\n",
      "Epoch 638. Learning Rate: 0.0005 Total Training Loss:  2.464895640150644\n",
      "Epoch 639. Learning Rate: 0.0005 Total Training Loss:  2.466802584240213\n",
      "Epoch 640. Learning Rate: 0.0005 Total Training Loss:  2.469781474210322\n",
      "Epoch 641. Learning Rate: 0.0005 Total Training Loss:  2.464203134761192\n",
      "Epoch 642. Learning Rate: 0.0005 Total Training Loss:  2.4631722818594426\n",
      "Epoch 643. Learning Rate: 0.0005 Total Training Loss:  2.466892084456049\n",
      "Epoch 644. Learning Rate: 0.0005 Total Training Loss:  2.4584622324910015\n",
      "Epoch 645. Learning Rate: 0.0005 Total Training Loss:  2.4594744206406176\n",
      "Epoch 646. Learning Rate: 0.0005 Total Training Loss:  2.458924040081911\n",
      "Epoch 647. Learning Rate: 0.0005 Total Training Loss:  2.460906111751683\n",
      "Epoch 648. Learning Rate: 0.0005 Total Training Loss:  2.455359774408862\n",
      "Epoch 649. Learning Rate: 0.0005 Total Training Loss:  2.460937645053491\n",
      "Epoch 650. Learning Rate: 0.0005 Total Training Loss:  2.4539938520174474\n",
      "Epoch 651. Learning Rate: 0.0005 Total Training Loss:  2.4558932868530974\n",
      "Epoch 652. Learning Rate: 0.0005 Total Training Loss:  2.4554529008455575\n",
      "Epoch 653. Learning Rate: 0.0005 Total Training Loss:  2.455962872947566\n",
      "Epoch 654. Learning Rate: 0.0005 Total Training Loss:  2.4549558063736185\n",
      "Epoch 655. Learning Rate: 0.0005 Total Training Loss:  2.4555886026937515\n",
      "Epoch 656. Learning Rate: 0.0005 Total Training Loss:  2.4524424087721854\n",
      "Epoch 657. Learning Rate: 0.0005 Total Training Loss:  2.453939210041426\n",
      "Epoch 658. Learning Rate: 0.0005 Total Training Loss:  2.450201809289865\n",
      "Epoch 659. Learning Rate: 0.0005 Total Training Loss:  2.450560151715763\n",
      "Epoch 660. Learning Rate: 0.0005 Total Training Loss:  2.4517540133092552\n",
      "Epoch 661. Learning Rate: 0.0005 Total Training Loss:  2.447306481190026\n",
      "Epoch 662. Learning Rate: 0.0005 Total Training Loss:  2.4494796054204926\n",
      "Epoch 663. Learning Rate: 0.0005 Total Training Loss:  2.4480040558846667\n",
      "Epoch 664. Learning Rate: 0.0005 Total Training Loss:  2.4476121488260105\n",
      "Epoch 665. Learning Rate: 0.0005 Total Training Loss:  2.4415669491281733\n",
      "Epoch 666. Learning Rate: 0.0005 Total Training Loss:  2.4456561690894887\n",
      "Epoch 667. Learning Rate: 0.0005 Total Training Loss:  2.4408295459579676\n",
      "Epoch 668. Learning Rate: 0.0005 Total Training Loss:  2.4429921875707805\n",
      "Epoch 669. Learning Rate: 0.0005 Total Training Loss:  2.445377662894316\n",
      "Epoch 670. Learning Rate: 0.0005 Total Training Loss:  2.442237959126942\n",
      "Epoch 671. Learning Rate: 0.0005 Total Training Loss:  2.4376532739261165\n",
      "Epoch 672. Learning Rate: 0.0005 Total Training Loss:  2.4454041965072975\n",
      "Epoch 673. Learning Rate: 0.0005 Total Training Loss:  2.4419795505236834\n",
      "Epoch 674. Learning Rate: 0.0005 Total Training Loss:  2.442280394374393\n",
      "Epoch 675. Learning Rate: 0.0005 Total Training Loss:  2.4332092202967033\n",
      "Epoch 676. Learning Rate: 0.0005 Total Training Loss:  2.4403697081143036\n",
      "Epoch 677. Learning Rate: 0.0005 Total Training Loss:  2.437775957165286\n",
      "Epoch 678. Learning Rate: 0.0005 Total Training Loss:  2.4345125334803015\n",
      "Epoch 679. Learning Rate: 0.0005 Total Training Loss:  2.4377506910823286\n",
      "Epoch 680. Learning Rate: 0.0005 Total Training Loss:  2.4342280067503452\n",
      "Epoch 681. Learning Rate: 0.0005 Total Training Loss:  2.4318106840364635\n",
      "Epoch 682. Learning Rate: 0.0005 Total Training Loss:  2.4341263663955033\n",
      "Epoch 683. Learning Rate: 0.0005 Total Training Loss:  2.4323063254123554\n",
      "Epoch 684. Learning Rate: 0.0005 Total Training Loss:  2.431308872415684\n",
      "Epoch 685. Learning Rate: 0.0005 Total Training Loss:  2.432952764094807\n",
      "Epoch 686. Learning Rate: 0.0005 Total Training Loss:  2.433662897790782\n",
      "Epoch 687. Learning Rate: 0.0005 Total Training Loss:  2.4267139392904937\n",
      "Epoch 688. Learning Rate: 0.0005 Total Training Loss:  2.429072562721558\n",
      "Epoch 689. Learning Rate: 0.0005 Total Training Loss:  2.4279600603040308\n",
      "Epoch 690. Learning Rate: 0.0005 Total Training Loss:  2.429782716324553\n",
      "Epoch 691. Learning Rate: 0.0005 Total Training Loss:  2.4287959907669574\n",
      "Epoch 692. Learning Rate: 0.0005 Total Training Loss:  2.422091183369048\n",
      "Epoch 693. Learning Rate: 0.0005 Total Training Loss:  2.4239270449616015\n",
      "Epoch 694. Learning Rate: 0.0005 Total Training Loss:  2.4248283891938627\n",
      "Epoch 695. Learning Rate: 0.0005 Total Training Loss:  2.423396403552033\n",
      "Epoch 696. Learning Rate: 0.0005 Total Training Loss:  2.423455433337949\n",
      "Epoch 697. Learning Rate: 0.0005 Total Training Loss:  2.421818071627058\n",
      "Epoch 698. Learning Rate: 0.0005 Total Training Loss:  2.421703293104656\n",
      "Epoch 699. Learning Rate: 0.0005 Total Training Loss:  2.424260479048826\n",
      "Epoch 700. Learning Rate: 0.0005 Total Training Loss:  2.4218645934015512\n",
      "Epoch 701. Learning Rate: 0.0005 Total Training Loss:  2.4238889379194006\n",
      "Epoch 702. Learning Rate: 0.0005 Total Training Loss:  2.415860397857614\n",
      "Epoch 703. Learning Rate: 0.0005 Total Training Loss:  2.4177621927810833\n",
      "Epoch 704. Learning Rate: 0.0005 Total Training Loss:  2.421143632614985\n",
      "Epoch 705. Learning Rate: 0.0005 Total Training Loss:  2.4131510903825983\n",
      "Epoch 706. Learning Rate: 0.0005 Total Training Loss:  2.4187633896945044\n",
      "Epoch 707. Learning Rate: 0.0005 Total Training Loss:  2.4158978917403147\n",
      "Epoch 708. Learning Rate: 0.0005 Total Training Loss:  2.412667394266464\n",
      "Epoch 709. Learning Rate: 0.0005 Total Training Loss:  2.4156399118946865\n",
      "Epoch 710. Learning Rate: 0.0005 Total Training Loss:  2.413135019131005\n",
      "Epoch 711. Learning Rate: 0.0005 Total Training Loss:  2.4115047935629264\n",
      "Epoch 712. Learning Rate: 0.0005 Total Training Loss:  2.4114753891481087\n",
      "Epoch 713. Learning Rate: 0.0005 Total Training Loss:  2.413926194771193\n",
      "Epoch 714. Learning Rate: 0.0005 Total Training Loss:  2.4117412446066737\n",
      "Epoch 715. Learning Rate: 0.0005 Total Training Loss:  2.4074575871927664\n",
      "Epoch 716. Learning Rate: 0.0005 Total Training Loss:  2.410422355751507\n",
      "Epoch 717. Learning Rate: 0.0005 Total Training Loss:  2.4090347826713696\n",
      "Epoch 718. Learning Rate: 0.0005 Total Training Loss:  2.408870240324177\n",
      "Epoch 719. Learning Rate: 0.0005 Total Training Loss:  2.4064531106268987\n",
      "Epoch 720. Learning Rate: 0.0005 Total Training Loss:  2.4067673784447834\n",
      "Epoch 721. Learning Rate: 0.0005 Total Training Loss:  2.4075486530782655\n",
      "Epoch 722. Learning Rate: 0.0005 Total Training Loss:  2.4032263617264107\n",
      "Epoch 723. Learning Rate: 0.0005 Total Training Loss:  2.408255252172239\n",
      "Epoch 724. Learning Rate: 0.0005 Total Training Loss:  2.4035699766827747\n",
      "Epoch 725. Learning Rate: 0.0005 Total Training Loss:  2.404063747613691\n",
      "Epoch 726. Learning Rate: 0.0005 Total Training Loss:  2.4026309499749914\n",
      "Epoch 727. Learning Rate: 0.0005 Total Training Loss:  2.4007629074621946\n",
      "Epoch 728. Learning Rate: 0.0005 Total Training Loss:  2.3984407756943256\n",
      "Epoch 729. Learning Rate: 0.0005 Total Training Loss:  2.402427159366198\n",
      "Epoch 730. Learning Rate: 0.0005 Total Training Loss:  2.4029737553792074\n",
      "Epoch 731. Learning Rate: 0.0005 Total Training Loss:  2.3985488729085773\n",
      "Epoch 732. Learning Rate: 0.0005 Total Training Loss:  2.3980985494563356\n",
      "Epoch 733. Learning Rate: 0.0005 Total Training Loss:  2.401776784216054\n",
      "Epoch 734. Learning Rate: 0.0005 Total Training Loss:  2.398042328248266\n",
      "Epoch 735. Learning Rate: 0.0005 Total Training Loss:  2.393291498417966\n",
      "Epoch 736. Learning Rate: 0.0005 Total Training Loss:  2.3948768752161413\n",
      "Epoch 737. Learning Rate: 0.0005 Total Training Loss:  2.3913708119653165\n",
      "Epoch 738. Learning Rate: 0.0005 Total Training Loss:  2.3928628970170394\n",
      "Epoch 739. Learning Rate: 0.0005 Total Training Loss:  2.3946328746387735\n",
      "Epoch 740. Learning Rate: 0.0005 Total Training Loss:  2.3957045535207726\n",
      "Epoch 741. Learning Rate: 0.0005 Total Training Loss:  2.3924330421723425\n",
      "Epoch 742. Learning Rate: 0.0005 Total Training Loss:  2.3873776581604034\n",
      "Epoch 743. Learning Rate: 0.0005 Total Training Loss:  2.3932561692781746\n",
      "Epoch 744. Learning Rate: 0.0005 Total Training Loss:  2.3901070164283738\n",
      "Epoch 745. Learning Rate: 0.0005 Total Training Loss:  2.3862237764988095\n",
      "Epoch 746. Learning Rate: 0.0005 Total Training Loss:  2.3900932201649994\n",
      "Epoch 747. Learning Rate: 0.0005 Total Training Loss:  2.3867361831944436\n",
      "Epoch 748. Learning Rate: 0.0005 Total Training Loss:  2.3893483752035536\n",
      "Epoch 749. Learning Rate: 0.0005 Total Training Loss:  2.3943935655406676\n",
      "Epoch 750. Learning Rate: 0.0005 Total Training Loss:  2.3881874658400193\n",
      "Epoch 751. Learning Rate: 0.0005 Total Training Loss:  2.3857744713895954\n",
      "Epoch 752. Learning Rate: 0.0005 Total Training Loss:  2.3855158158112317\n",
      "Epoch 753. Learning Rate: 0.0005 Total Training Loss:  2.3860247510601766\n",
      "Epoch 754. Learning Rate: 0.0005 Total Training Loss:  2.38069562218152\n",
      "Epoch 755. Learning Rate: 0.0005 Total Training Loss:  2.3893316285684705\n",
      "Epoch 756. Learning Rate: 0.0005 Total Training Loss:  2.3856590648647398\n",
      "Epoch 757. Learning Rate: 0.0005 Total Training Loss:  2.3827661109389737\n",
      "Epoch 758. Learning Rate: 0.0005 Total Training Loss:  2.3851031663361937\n",
      "Epoch 759. Learning Rate: 0.0005 Total Training Loss:  2.379696191637777\n",
      "Epoch 760. Learning Rate: 0.0005 Total Training Loss:  2.3803218078101054\n",
      "Epoch 761. Learning Rate: 0.0005 Total Training Loss:  2.379471019783523\n",
      "Epoch 762. Learning Rate: 0.0005 Total Training Loss:  2.3832641914486885\n",
      "Epoch 763. Learning Rate: 0.0005 Total Training Loss:  2.3757998858345672\n",
      "Epoch 764. Learning Rate: 0.0005 Total Training Loss:  2.381775742978789\n",
      "Epoch 765. Learning Rate: 0.0005 Total Training Loss:  2.378885851241648\n",
      "Epoch 766. Learning Rate: 0.0005 Total Training Loss:  2.379958906210959\n",
      "Epoch 767. Learning Rate: 0.0005 Total Training Loss:  2.3755881865508854\n",
      "Epoch 768. Learning Rate: 0.0005 Total Training Loss:  2.374721630010754\n",
      "Epoch 769. Learning Rate: 0.0005 Total Training Loss:  2.377744743716903\n",
      "Epoch 770. Learning Rate: 0.0005 Total Training Loss:  2.3757703028968535\n",
      "Epoch 771. Learning Rate: 0.0005 Total Training Loss:  2.374096597370226\n",
      "Epoch 772. Learning Rate: 0.0005 Total Training Loss:  2.3748817679006606\n",
      "Epoch 773. Learning Rate: 0.0005 Total Training Loss:  2.375589461240452\n",
      "Epoch 774. Learning Rate: 0.0005 Total Training Loss:  2.371859990817029\n",
      "Epoch 775. Learning Rate: 0.0005 Total Training Loss:  2.373787115328014\n",
      "Epoch 776. Learning Rate: 0.0005 Total Training Loss:  2.3711821935139596\n",
      "Epoch 777. Learning Rate: 0.0005 Total Training Loss:  2.3699691303190775\n",
      "Epoch 778. Learning Rate: 0.0005 Total Training Loss:  2.3724506540456787\n",
      "Epoch 779. Learning Rate: 0.0005 Total Training Loss:  2.368544560391456\n",
      "Epoch 780. Learning Rate: 0.0005 Total Training Loss:  2.369794565369375\n",
      "Epoch 781. Learning Rate: 0.0005 Total Training Loss:  2.3713567309314385\n",
      "Epoch 782. Learning Rate: 0.0005 Total Training Loss:  2.3655693320324644\n",
      "Epoch 783. Learning Rate: 0.0005 Total Training Loss:  2.3720540673239157\n",
      "Epoch 784. Learning Rate: 0.0005 Total Training Loss:  2.3637076522572897\n",
      "Epoch 785. Learning Rate: 0.0005 Total Training Loss:  2.3706001630052924\n",
      "Epoch 786. Learning Rate: 0.0005 Total Training Loss:  2.3658401340944692\n",
      "Epoch 787. Learning Rate: 0.0005 Total Training Loss:  2.3659931770525873\n",
      "Epoch 788. Learning Rate: 0.0005 Total Training Loss:  2.3681728211813606\n",
      "Epoch 789. Learning Rate: 0.0005 Total Training Loss:  2.3652485673082992\n",
      "Epoch 790. Learning Rate: 0.0005 Total Training Loss:  2.3628479174803942\n",
      "Epoch 791. Learning Rate: 0.0005 Total Training Loss:  2.362669755355455\n",
      "Epoch 792. Learning Rate: 0.0005 Total Training Loss:  2.3630057976115495\n",
      "Epoch 793. Learning Rate: 0.0005 Total Training Loss:  2.363257635908667\n",
      "Epoch 794. Learning Rate: 0.0005 Total Training Loss:  2.363928413426038\n",
      "Epoch 795. Learning Rate: 0.0005 Total Training Loss:  2.3562402395764366\n",
      "Epoch 796. Learning Rate: 0.0005 Total Training Loss:  2.360992811212782\n",
      "Epoch 797. Learning Rate: 0.0005 Total Training Loss:  2.357997311919462\n",
      "Epoch 798. Learning Rate: 0.0005 Total Training Loss:  2.3600462175672874\n",
      "Epoch 799. Learning Rate: 0.0005 Total Training Loss:  2.3609114644350484\n",
      "Epoch 800. Learning Rate: 0.0005 Total Training Loss:  2.3590543870814145\n",
      "Epoch 801. Learning Rate: 0.0005 Total Training Loss:  2.3579031105036847\n",
      "Epoch 802. Learning Rate: 0.0005 Total Training Loss:  2.354151249397546\n",
      "Epoch 803. Learning Rate: 0.0005 Total Training Loss:  2.353235735965427\n",
      "Epoch 804. Learning Rate: 0.0005 Total Training Loss:  2.3572694030590355\n",
      "Epoch 805. Learning Rate: 0.0005 Total Training Loss:  2.349372889730148\n",
      "Epoch 806. Learning Rate: 0.0005 Total Training Loss:  2.3567357714055106\n",
      "Epoch 807. Learning Rate: 0.0005 Total Training Loss:  2.352131398394704\n",
      "Epoch 808. Learning Rate: 0.0005 Total Training Loss:  2.3523988517699763\n",
      "Epoch 809. Learning Rate: 0.0005 Total Training Loss:  2.3519309599651024\n",
      "Epoch 810. Learning Rate: 0.0005 Total Training Loss:  2.348641832708381\n",
      "Epoch 811. Learning Rate: 0.0005 Total Training Loss:  2.352565767534543\n",
      "Epoch 812. Learning Rate: 0.0005 Total Training Loss:  2.347181163204368\n",
      "Epoch 813. Learning Rate: 0.0005 Total Training Loss:  2.3540450520813465\n",
      "Epoch 814. Learning Rate: 0.0005 Total Training Loss:  2.3477158029563725\n",
      "Epoch 815. Learning Rate: 0.0005 Total Training Loss:  2.3495063653681427\n",
      "Epoch 816. Learning Rate: 0.0005 Total Training Loss:  2.3473083580029197\n",
      "Epoch 817. Learning Rate: 0.0005 Total Training Loss:  2.3434853206854314\n",
      "Epoch 818. Learning Rate: 0.0005 Total Training Loss:  2.3535738656064495\n",
      "Epoch 819. Learning Rate: 0.0005 Total Training Loss:  2.345479200826958\n",
      "Epoch 820. Learning Rate: 0.0005 Total Training Loss:  2.3407022632309236\n",
      "Epoch 821. Learning Rate: 0.0005 Total Training Loss:  2.344496332050767\n",
      "Epoch 822. Learning Rate: 0.0005 Total Training Loss:  2.3400127850472927\n",
      "Epoch 823. Learning Rate: 0.0005 Total Training Loss:  2.3416589472908527\n",
      "Epoch 824. Learning Rate: 0.0005 Total Training Loss:  2.339135400019586\n",
      "Epoch 825. Learning Rate: 0.0005 Total Training Loss:  2.338557329669129\n",
      "Epoch 826. Learning Rate: 0.0005 Total Training Loss:  2.3396292534307577\n",
      "Epoch 827. Learning Rate: 0.0005 Total Training Loss:  2.338346279109828\n",
      "Epoch 828. Learning Rate: 0.0005 Total Training Loss:  2.3384462848189287\n",
      "Epoch 829. Learning Rate: 0.0005 Total Training Loss:  2.3354389477171935\n",
      "Epoch 830. Learning Rate: 0.0005 Total Training Loss:  2.336610399768688\n",
      "Epoch 831. Learning Rate: 0.0005 Total Training Loss:  2.337971592321992\n",
      "Epoch 832. Learning Rate: 0.0005 Total Training Loss:  2.337593108182773\n",
      "Epoch 833. Learning Rate: 0.0005 Total Training Loss:  2.33008650445845\n",
      "Epoch 834. Learning Rate: 0.0005 Total Training Loss:  2.333127912017517\n",
      "Epoch 835. Learning Rate: 0.0005 Total Training Loss:  2.3345751314773224\n",
      "Epoch 836. Learning Rate: 0.0005 Total Training Loss:  2.3282277735415846\n",
      "Epoch 837. Learning Rate: 0.0005 Total Training Loss:  2.330352178425528\n",
      "Epoch 838. Learning Rate: 0.0005 Total Training Loss:  2.329641194723081\n",
      "Epoch 839. Learning Rate: 0.0005 Total Training Loss:  2.3342230066191405\n",
      "Epoch 840. Learning Rate: 0.0005 Total Training Loss:  2.329928518505767\n",
      "Epoch 841. Learning Rate: 0.0005 Total Training Loss:  2.3322701854631305\n",
      "Epoch 842. Learning Rate: 0.0005 Total Training Loss:  2.32559083547676\n",
      "Epoch 843. Learning Rate: 0.0005 Total Training Loss:  2.3306052708649077\n",
      "Epoch 844. Learning Rate: 0.0005 Total Training Loss:  2.3284546472132206\n",
      "Epoch 845. Learning Rate: 0.0005 Total Training Loss:  2.3278319714590907\n",
      "Epoch 846. Learning Rate: 0.0005 Total Training Loss:  2.320065769541543\n",
      "Epoch 847. Learning Rate: 0.0005 Total Training Loss:  2.3287012699292973\n",
      "Epoch 848. Learning Rate: 0.0005 Total Training Loss:  2.3223862356389873\n",
      "Epoch 849. Learning Rate: 0.0005 Total Training Loss:  2.3250986958737485\n",
      "Epoch 850. Learning Rate: 0.0005 Total Training Loss:  2.3237255107378587\n",
      "Epoch 851. Learning Rate: 0.0005 Total Training Loss:  2.3233021456399\n",
      "Epoch 852. Learning Rate: 0.0005 Total Training Loss:  2.3176529260817915\n",
      "Epoch 853. Learning Rate: 0.0005 Total Training Loss:  2.322257518826518\n",
      "Epoch 854. Learning Rate: 0.0005 Total Training Loss:  2.324625398381613\n",
      "Epoch 855. Learning Rate: 0.0005 Total Training Loss:  2.318517918465659\n",
      "Epoch 856. Learning Rate: 0.0005 Total Training Loss:  2.320400257012807\n",
      "Epoch 857. Learning Rate: 0.0005 Total Training Loss:  2.3182154817623086\n",
      "Epoch 858. Learning Rate: 0.0005 Total Training Loss:  2.3177709079463966\n",
      "Epoch 859. Learning Rate: 0.0005 Total Training Loss:  2.316869930480607\n",
      "Epoch 860. Learning Rate: 0.0005 Total Training Loss:  2.3147337900591083\n",
      "Epoch 861. Learning Rate: 0.0005 Total Training Loss:  2.321386875351891\n",
      "Epoch 862. Learning Rate: 0.0005 Total Training Loss:  2.3160069739678875\n",
      "Epoch 863. Learning Rate: 0.0005 Total Training Loss:  2.318784475501161\n",
      "Epoch 864. Learning Rate: 0.0005 Total Training Loss:  2.3090639506699517\n",
      "Epoch 865. Learning Rate: 0.0005 Total Training Loss:  2.3171556065790355\n",
      "Epoch 866. Learning Rate: 0.0005 Total Training Loss:  2.314024343562778\n",
      "Epoch 867. Learning Rate: 0.0005 Total Training Loss:  2.3115078395348974\n",
      "Epoch 868. Learning Rate: 0.0005 Total Training Loss:  2.3110294860671274\n",
      "Epoch 869. Learning Rate: 0.0005 Total Training Loss:  2.3101149378926493\n",
      "Epoch 870. Learning Rate: 0.0005 Total Training Loss:  2.3181120273657143\n",
      "Epoch 871. Learning Rate: 0.0005 Total Training Loss:  2.3158011429477483\n",
      "Epoch 872. Learning Rate: 0.0005 Total Training Loss:  2.306318774004467\n",
      "Epoch 873. Learning Rate: 0.0005 Total Training Loss:  2.3091368483146653\n",
      "Epoch 874. Learning Rate: 0.0005 Total Training Loss:  2.307475512730889\n",
      "Epoch 875. Learning Rate: 0.0005 Total Training Loss:  2.310976509936154\n",
      "Epoch 876. Learning Rate: 0.0005 Total Training Loss:  2.304037415771745\n",
      "Epoch 877. Learning Rate: 0.0005 Total Training Loss:  2.3088064856128767\n",
      "Epoch 878. Learning Rate: 0.0005 Total Training Loss:  2.305533279082738\n",
      "Epoch 879. Learning Rate: 0.0005 Total Training Loss:  2.3077830394613557\n",
      "Epoch 880. Learning Rate: 0.0005 Total Training Loss:  2.298093547578901\n",
      "Epoch 881. Learning Rate: 0.0005 Total Training Loss:  2.3084940251428634\n",
      "Epoch 882. Learning Rate: 0.0005 Total Training Loss:  2.3018118774634786\n",
      "Epoch 883. Learning Rate: 0.0005 Total Training Loss:  2.306835028284695\n",
      "Epoch 884. Learning Rate: 0.0005 Total Training Loss:  2.2999629504047334\n",
      "Epoch 885. Learning Rate: 0.0005 Total Training Loss:  2.301617408869788\n",
      "Epoch 886. Learning Rate: 0.0005 Total Training Loss:  2.3014246115344577\n",
      "Epoch 887. Learning Rate: 0.0005 Total Training Loss:  2.3040348397335038\n",
      "Epoch 888. Learning Rate: 0.0005 Total Training Loss:  2.2943223615875468\n",
      "Epoch 889. Learning Rate: 0.0005 Total Training Loss:  2.2983953546499833\n",
      "Epoch 890. Learning Rate: 0.0005 Total Training Loss:  2.298627619689796\n",
      "Epoch 891. Learning Rate: 0.0005 Total Training Loss:  2.3046120837680064\n",
      "Epoch 892. Learning Rate: 0.0005 Total Training Loss:  2.294020760455169\n",
      "Epoch 893. Learning Rate: 0.0005 Total Training Loss:  2.2937934034853242\n",
      "Epoch 894. Learning Rate: 0.0005 Total Training Loss:  2.300835249829106\n",
      "Epoch 895. Learning Rate: 0.0005 Total Training Loss:  2.293118597473949\n",
      "Epoch 896. Learning Rate: 0.0005 Total Training Loss:  2.2951653831405565\n",
      "Epoch 897. Learning Rate: 0.0005 Total Training Loss:  2.3007816951721907\n",
      "Epoch 898. Learning Rate: 0.0005 Total Training Loss:  2.293756789295003\n",
      "Epoch 899. Learning Rate: 0.0005 Total Training Loss:  2.28928015002748\n",
      "Epoch 900. Learning Rate: 0.0005 Total Training Loss:  2.2951627304428257\n",
      "Epoch 901. Learning Rate: 0.0005 Total Training Loss:  2.291204713168554\n",
      "Epoch 902. Learning Rate: 0.0005 Total Training Loss:  2.2872336239670403\n",
      "Epoch 903. Learning Rate: 0.0005 Total Training Loss:  2.291889422573149\n",
      "Epoch 904. Learning Rate: 0.0005 Total Training Loss:  2.290322109765839\n",
      "Epoch 905. Learning Rate: 0.0005 Total Training Loss:  2.2834864765172824\n",
      "Epoch 906. Learning Rate: 0.0005 Total Training Loss:  2.2942097528721206\n",
      "Epoch 907. Learning Rate: 0.0005 Total Training Loss:  2.286789224948734\n",
      "Epoch 908. Learning Rate: 0.0005 Total Training Loss:  2.2908249835600145\n",
      "Epoch 909. Learning Rate: 0.0005 Total Training Loss:  2.287540781253483\n",
      "Epoch 910. Learning Rate: 0.0005 Total Training Loss:  2.2844622452394105\n",
      "Epoch 911. Learning Rate: 0.0005 Total Training Loss:  2.2834584150696173\n",
      "Epoch 912. Learning Rate: 0.0005 Total Training Loss:  2.2921512659522705\n",
      "Epoch 913. Learning Rate: 0.0005 Total Training Loss:  2.284542872919701\n",
      "Epoch 914. Learning Rate: 0.0005 Total Training Loss:  2.283940389752388\n",
      "Epoch 915. Learning Rate: 0.0005 Total Training Loss:  2.281519281619694\n",
      "Epoch 916. Learning Rate: 0.0005 Total Training Loss:  2.2919251350103877\n",
      "Epoch 917. Learning Rate: 0.0005 Total Training Loss:  2.283596462046262\n",
      "Epoch 918. Learning Rate: 0.0005 Total Training Loss:  2.2739580456982367\n",
      "Epoch 919. Learning Rate: 0.0005 Total Training Loss:  2.281296751636546\n",
      "Epoch 920. Learning Rate: 0.0005 Total Training Loss:  2.279905668052379\n",
      "Epoch 921. Learning Rate: 0.0005 Total Training Loss:  2.2777982996194623\n",
      "Epoch 922. Learning Rate: 0.0005 Total Training Loss:  2.275100844213739\n",
      "Epoch 923. Learning Rate: 0.0005 Total Training Loss:  2.279815830348525\n",
      "Epoch 924. Learning Rate: 0.0005 Total Training Loss:  2.280170007084962\n",
      "Epoch 925. Learning Rate: 0.0005 Total Training Loss:  2.2770237190416083\n",
      "Epoch 926. Learning Rate: 0.0005 Total Training Loss:  2.280467351898551\n",
      "Epoch 927. Learning Rate: 0.0005 Total Training Loss:  2.273343489796389\n",
      "Epoch 928. Learning Rate: 0.0005 Total Training Loss:  2.278980554547161\n",
      "Epoch 929. Learning Rate: 0.0005 Total Training Loss:  2.2763941145385616\n",
      "Epoch 930. Learning Rate: 0.0005 Total Training Loss:  2.2766391574405134\n",
      "Epoch 931. Learning Rate: 0.0005 Total Training Loss:  2.274103877018206\n",
      "Epoch 932. Learning Rate: 0.0005 Total Training Loss:  2.276736364059616\n",
      "Epoch 933. Learning Rate: 0.0005 Total Training Loss:  2.2724594436003827\n",
      "Epoch 934. Learning Rate: 0.0005 Total Training Loss:  2.268084528390318\n",
      "Epoch 935. Learning Rate: 0.0005 Total Training Loss:  2.27906223887112\n",
      "Epoch 936. Learning Rate: 0.0005 Total Training Loss:  2.268337476416491\n",
      "Epoch 937. Learning Rate: 0.0005 Total Training Loss:  2.2713276289287023\n",
      "Epoch 938. Learning Rate: 0.0005 Total Training Loss:  2.2684530003461987\n",
      "Epoch 939. Learning Rate: 0.0005 Total Training Loss:  2.2767530455021188\n",
      "Epoch 940. Learning Rate: 0.0005 Total Training Loss:  2.264163081184961\n",
      "Epoch 941. Learning Rate: 0.0005 Total Training Loss:  2.2696814412483945\n",
      "Epoch 942. Learning Rate: 0.0005 Total Training Loss:  2.265469705336727\n",
      "Epoch 943. Learning Rate: 0.0005 Total Training Loss:  2.2716331982519478\n",
      "Epoch 944. Learning Rate: 0.0005 Total Training Loss:  2.263701600022614\n",
      "Epoch 945. Learning Rate: 0.0005 Total Training Loss:  2.2679241342120804\n",
      "Epoch 946. Learning Rate: 0.0005 Total Training Loss:  2.262582884693984\n",
      "Epoch 947. Learning Rate: 0.0005 Total Training Loss:  2.2639556947979145\n",
      "Epoch 948. Learning Rate: 0.0005 Total Training Loss:  2.2688364377245307\n",
      "Epoch 949. Learning Rate: 0.0005 Total Training Loss:  2.2640142053132877\n",
      "Epoch 950. Learning Rate: 0.0005 Total Training Loss:  2.264142725733109\n",
      "Epoch 951. Learning Rate: 0.0005 Total Training Loss:  2.262164135579951\n",
      "Epoch 952. Learning Rate: 0.0005 Total Training Loss:  2.2638287295703776\n",
      "Epoch 953. Learning Rate: 0.0005 Total Training Loss:  2.265059124212712\n",
      "Epoch 954. Learning Rate: 0.0005 Total Training Loss:  2.263164345175028\n",
      "Epoch 955. Learning Rate: 0.0005 Total Training Loss:  2.2570948988432065\n",
      "Epoch 956. Learning Rate: 0.0005 Total Training Loss:  2.2618679205188528\n",
      "Epoch 957. Learning Rate: 0.0005 Total Training Loss:  2.2649910205509514\n",
      "Epoch 958. Learning Rate: 0.0005 Total Training Loss:  2.2532498692162335\n",
      "Epoch 959. Learning Rate: 0.0005 Total Training Loss:  2.2614984659012407\n",
      "Epoch 960. Learning Rate: 0.0005 Total Training Loss:  2.257706892036367\n",
      "Epoch 961. Learning Rate: 0.0005 Total Training Loss:  2.2592141888744663\n",
      "Epoch 962. Learning Rate: 0.0005 Total Training Loss:  2.2614188097650185\n",
      "Epoch 963. Learning Rate: 0.0005 Total Training Loss:  2.251989464915823\n",
      "Epoch 964. Learning Rate: 0.0005 Total Training Loss:  2.2539257069583982\n",
      "Epoch 965. Learning Rate: 0.0005 Total Training Loss:  2.257729876146186\n",
      "Epoch 966. Learning Rate: 0.0005 Total Training Loss:  2.2531364756287076\n",
      "Epoch 967. Learning Rate: 0.0005 Total Training Loss:  2.255295802373439\n",
      "Epoch 968. Learning Rate: 0.0005 Total Training Loss:  2.2522153983591124\n",
      "Epoch 969. Learning Rate: 0.0005 Total Training Loss:  2.2516276797978207\n",
      "Epoch 970. Learning Rate: 0.0005 Total Training Loss:  2.2521403694408946\n",
      "Epoch 971. Learning Rate: 0.0005 Total Training Loss:  2.2544797415612265\n",
      "Epoch 972. Learning Rate: 0.0005 Total Training Loss:  2.249712299089879\n",
      "Epoch 973. Learning Rate: 0.0005 Total Training Loss:  2.2481420394033194\n",
      "Epoch 974. Learning Rate: 0.0005 Total Training Loss:  2.2524088814971037\n",
      "Epoch 975. Learning Rate: 0.0005 Total Training Loss:  2.2523740866745356\n",
      "Epoch 976. Learning Rate: 0.0005 Total Training Loss:  2.2444764978718013\n",
      "Epoch 977. Learning Rate: 0.0005 Total Training Loss:  2.2497235576738603\n",
      "Epoch 978. Learning Rate: 0.0005 Total Training Loss:  2.249563245975878\n",
      "Epoch 979. Learning Rate: 0.0005 Total Training Loss:  2.2434287946962286\n",
      "Epoch 980. Learning Rate: 0.0005 Total Training Loss:  2.2451459239819087\n",
      "Epoch 981. Learning Rate: 0.0005 Total Training Loss:  2.2472509739454836\n",
      "Epoch 982. Learning Rate: 0.0005 Total Training Loss:  2.243034254235681\n",
      "Epoch 983. Learning Rate: 0.0005 Total Training Loss:  2.2405759666580707\n",
      "Epoch 984. Learning Rate: 0.0005 Total Training Loss:  2.2469275321927853\n",
      "Epoch 985. Learning Rate: 0.0005 Total Training Loss:  2.246100317686796\n",
      "Epoch 986. Learning Rate: 0.0005 Total Training Loss:  2.2433860478922725\n",
      "Epoch 987. Learning Rate: 0.0005 Total Training Loss:  2.23806825093925\n",
      "Epoch 988. Learning Rate: 0.0005 Total Training Loss:  2.2461886723758653\n",
      "Epoch 989. Learning Rate: 0.0005 Total Training Loss:  2.236027760547586\n",
      "Epoch 990. Learning Rate: 0.0005 Total Training Loss:  2.2437640358693898\n",
      "Epoch 991. Learning Rate: 0.0005 Total Training Loss:  2.2366783601464704\n",
      "Epoch 992. Learning Rate: 0.0005 Total Training Loss:  2.241066791350022\n",
      "Epoch 993. Learning Rate: 0.0005 Total Training Loss:  2.2360043893568218\n",
      "Epoch 994. Learning Rate: 0.0005 Total Training Loss:  2.241094156401232\n",
      "Epoch 995. Learning Rate: 0.0005 Total Training Loss:  2.2349718644691166\n",
      "Epoch 996. Learning Rate: 0.0005 Total Training Loss:  2.238106199365575\n",
      "Epoch 997. Learning Rate: 0.0005 Total Training Loss:  2.2367455189232714\n",
      "Epoch 998. Learning Rate: 0.0005 Total Training Loss:  2.238266253611073\n",
      "Epoch 999. Learning Rate: 0.0005 Total Training Loss:  2.233759691414889\n",
      "Epoch 1000. Learning Rate: 0.00025 Total Training Loss:  2.236319008981809\n",
      "Epoch 1001. Learning Rate: 0.00025 Total Training Loss:  2.2281668963260017\n",
      "Epoch 1002. Learning Rate: 0.00025 Total Training Loss:  2.1534053559880704\n",
      "Epoch 1003. Learning Rate: 0.00025 Total Training Loss:  2.1459294001106173\n",
      "Epoch 1004. Learning Rate: 0.00025 Total Training Loss:  2.146280786662828\n",
      "Epoch 1005. Learning Rate: 0.00025 Total Training Loss:  2.146241760405246\n",
      "Epoch 1006. Learning Rate: 0.00025 Total Training Loss:  2.1429042712261435\n",
      "Epoch 1007. Learning Rate: 0.00025 Total Training Loss:  2.1408978719846345\n",
      "Epoch 1008. Learning Rate: 0.00025 Total Training Loss:  2.1416105733369477\n",
      "Epoch 1009. Learning Rate: 0.00025 Total Training Loss:  2.1416661715193186\n",
      "Epoch 1010. Learning Rate: 0.00025 Total Training Loss:  2.1424635668226983\n",
      "Epoch 1011. Learning Rate: 0.00025 Total Training Loss:  2.1380932334577665\n",
      "Epoch 1012. Learning Rate: 0.00025 Total Training Loss:  2.142877378180856\n",
      "Epoch 1013. Learning Rate: 0.00025 Total Training Loss:  2.14080236243899\n",
      "Epoch 1014. Learning Rate: 0.00025 Total Training Loss:  2.140672150271712\n",
      "Epoch 1015. Learning Rate: 0.00025 Total Training Loss:  2.1391097845626064\n",
      "Epoch 1016. Learning Rate: 0.00025 Total Training Loss:  2.138619844627101\n",
      "Epoch 1017. Learning Rate: 0.00025 Total Training Loss:  2.139098852116149\n",
      "Epoch 1018. Learning Rate: 0.00025 Total Training Loss:  2.1379358009726275\n",
      "Epoch 1019. Learning Rate: 0.00025 Total Training Loss:  2.1395347441430204\n",
      "Epoch 1020. Learning Rate: 0.00025 Total Training Loss:  2.1382515156292357\n",
      "Epoch 1021. Learning Rate: 0.00025 Total Training Loss:  2.1387996316188946\n",
      "Epoch 1022. Learning Rate: 0.00025 Total Training Loss:  2.136496664461447\n",
      "Epoch 1023. Learning Rate: 0.00025 Total Training Loss:  2.140303180756746\n",
      "Epoch 1024. Learning Rate: 0.00025 Total Training Loss:  2.135304882773198\n",
      "Epoch 1025. Learning Rate: 0.00025 Total Training Loss:  2.137176529329736\n",
      "Epoch 1026. Learning Rate: 0.00025 Total Training Loss:  2.13711626897566\n",
      "Epoch 1027. Learning Rate: 0.00025 Total Training Loss:  2.135172670154134\n",
      "Epoch 1028. Learning Rate: 0.00025 Total Training Loss:  2.1320802562404424\n",
      "Epoch 1029. Learning Rate: 0.00025 Total Training Loss:  2.1379776778630912\n",
      "Epoch 1030. Learning Rate: 0.00025 Total Training Loss:  2.1391204737883527\n",
      "Epoch 1031. Learning Rate: 0.00025 Total Training Loss:  2.131234111060621\n",
      "Epoch 1032. Learning Rate: 0.00025 Total Training Loss:  2.134916182840243\n",
      "Epoch 1033. Learning Rate: 0.00025 Total Training Loss:  2.132389564620098\n",
      "Epoch 1034. Learning Rate: 0.00025 Total Training Loss:  2.134679706854513\n",
      "Epoch 1035. Learning Rate: 0.00025 Total Training Loss:  2.1340705357142724\n",
      "Epoch 1036. Learning Rate: 0.00025 Total Training Loss:  2.132143856055336\n",
      "Epoch 1037. Learning Rate: 0.00025 Total Training Loss:  2.1313783139630686\n",
      "Epoch 1038. Learning Rate: 0.00025 Total Training Loss:  2.1345331322809216\n",
      "Epoch 1039. Learning Rate: 0.00025 Total Training Loss:  2.1319897238281555\n",
      "Epoch 1040. Learning Rate: 0.00025 Total Training Loss:  2.1294318296713755\n",
      "Epoch 1041. Learning Rate: 0.00025 Total Training Loss:  2.1325631928630173\n",
      "Epoch 1042. Learning Rate: 0.00025 Total Training Loss:  2.1308848544140346\n",
      "Epoch 1043. Learning Rate: 0.00025 Total Training Loss:  2.131866340059787\n",
      "Epoch 1044. Learning Rate: 0.00025 Total Training Loss:  2.130830928334035\n",
      "Epoch 1045. Learning Rate: 0.00025 Total Training Loss:  2.1297314141702373\n",
      "Epoch 1046. Learning Rate: 0.00025 Total Training Loss:  2.1309431565459818\n",
      "Epoch 1047. Learning Rate: 0.00025 Total Training Loss:  2.1307183250319213\n",
      "Epoch 1048. Learning Rate: 0.00025 Total Training Loss:  2.1293551832495723\n",
      "Epoch 1049. Learning Rate: 0.00025 Total Training Loss:  2.1291845362866297\n",
      "Epoch 1050. Learning Rate: 0.00025 Total Training Loss:  2.1266746305918787\n",
      "Epoch 1051. Learning Rate: 0.00025 Total Training Loss:  2.1307290804979857\n",
      "Epoch 1052. Learning Rate: 0.00025 Total Training Loss:  2.127018188155489\n",
      "Epoch 1053. Learning Rate: 0.00025 Total Training Loss:  2.127031448238995\n",
      "Epoch 1054. Learning Rate: 0.00025 Total Training Loss:  2.1257474376179744\n",
      "Epoch 1055. Learning Rate: 0.00025 Total Training Loss:  2.1265161072078627\n",
      "Epoch 1056. Learning Rate: 0.00025 Total Training Loss:  2.1276496174687054\n",
      "Epoch 1057. Learning Rate: 0.00025 Total Training Loss:  2.1215596298570745\n",
      "Epoch 1058. Learning Rate: 0.00025 Total Training Loss:  2.125947036227444\n",
      "Epoch 1059. Learning Rate: 0.00025 Total Training Loss:  2.125096895149909\n",
      "Epoch 1060. Learning Rate: 0.00025 Total Training Loss:  2.1268735306803137\n",
      "Epoch 1061. Learning Rate: 0.00025 Total Training Loss:  2.121800384076778\n",
      "Epoch 1062. Learning Rate: 0.00025 Total Training Loss:  2.123118741408689\n",
      "Epoch 1063. Learning Rate: 0.00025 Total Training Loss:  2.1248883017979097\n",
      "Epoch 1064. Learning Rate: 0.00025 Total Training Loss:  2.123812369944062\n",
      "Epoch 1065. Learning Rate: 0.00025 Total Training Loss:  2.1206500107073225\n",
      "Epoch 1066. Learning Rate: 0.00025 Total Training Loss:  2.1211512839072384\n",
      "Epoch 1067. Learning Rate: 0.00025 Total Training Loss:  2.124682859663153\n",
      "Epoch 1068. Learning Rate: 0.00025 Total Training Loss:  2.120541472482728\n",
      "Epoch 1069. Learning Rate: 0.00025 Total Training Loss:  2.12131251412211\n",
      "Epoch 1070. Learning Rate: 0.00025 Total Training Loss:  2.1239358573802747\n",
      "Epoch 1071. Learning Rate: 0.00025 Total Training Loss:  2.1208804198540747\n",
      "Epoch 1072. Learning Rate: 0.00025 Total Training Loss:  2.119320459110895\n",
      "Epoch 1073. Learning Rate: 0.00025 Total Training Loss:  2.12191168544814\n",
      "Epoch 1074. Learning Rate: 0.00025 Total Training Loss:  2.1198469003429636\n",
      "Epoch 1075. Learning Rate: 0.00025 Total Training Loss:  2.1170326188439503\n",
      "Epoch 1076. Learning Rate: 0.00025 Total Training Loss:  2.118588121898938\n",
      "Epoch 1077. Learning Rate: 0.00025 Total Training Loss:  2.117582293401938\n",
      "Epoch 1078. Learning Rate: 0.00025 Total Training Loss:  2.121964351332281\n",
      "Epoch 1079. Learning Rate: 0.00025 Total Training Loss:  2.1151669407845475\n",
      "Epoch 1080. Learning Rate: 0.00025 Total Training Loss:  2.120437652338296\n",
      "Epoch 1081. Learning Rate: 0.00025 Total Training Loss:  2.1161148967512418\n",
      "Epoch 1082. Learning Rate: 0.00025 Total Training Loss:  2.1182557327556424\n",
      "Epoch 1083. Learning Rate: 0.00025 Total Training Loss:  2.116269595833728\n",
      "Epoch 1084. Learning Rate: 0.00025 Total Training Loss:  2.115211505413754\n",
      "Epoch 1085. Learning Rate: 0.00025 Total Training Loss:  2.118353531550383\n",
      "Epoch 1086. Learning Rate: 0.00025 Total Training Loss:  2.115340060146991\n",
      "Epoch 1087. Learning Rate: 0.00025 Total Training Loss:  2.1159068243869115\n",
      "Epoch 1088. Learning Rate: 0.00025 Total Training Loss:  2.1122651604237035\n",
      "Epoch 1089. Learning Rate: 0.00025 Total Training Loss:  2.1143426092457958\n",
      "Epoch 1090. Learning Rate: 0.00025 Total Training Loss:  2.11579516006168\n",
      "Epoch 1091. Learning Rate: 0.00025 Total Training Loss:  2.111897945345845\n",
      "Epoch 1092. Learning Rate: 0.00025 Total Training Loss:  2.1132287325162906\n",
      "Epoch 1093. Learning Rate: 0.00025 Total Training Loss:  2.112749099585926\n",
      "Epoch 1094. Learning Rate: 0.00025 Total Training Loss:  2.1129452134482563\n",
      "Epoch 1095. Learning Rate: 0.00025 Total Training Loss:  2.1125179259397555\n",
      "Epoch 1096. Learning Rate: 0.00025 Total Training Loss:  2.112093153700698\n",
      "Epoch 1097. Learning Rate: 0.00025 Total Training Loss:  2.1121435252716765\n",
      "Epoch 1098. Learning Rate: 0.00025 Total Training Loss:  2.112970644462621\n",
      "Epoch 1099. Learning Rate: 0.00025 Total Training Loss:  2.1096402533585206\n",
      "Epoch 1100. Learning Rate: 0.00025 Total Training Loss:  2.110027645685477\n",
      "Epoch 1101. Learning Rate: 0.00025 Total Training Loss:  2.1092691879021004\n",
      "Epoch 1102. Learning Rate: 0.00025 Total Training Loss:  2.108305762783857\n",
      "Epoch 1103. Learning Rate: 0.00025 Total Training Loss:  2.1113722939626314\n",
      "Epoch 1104. Learning Rate: 0.00025 Total Training Loss:  2.108371739392169\n",
      "Epoch 1105. Learning Rate: 0.00025 Total Training Loss:  2.1072266629489604\n",
      "Epoch 1106. Learning Rate: 0.00025 Total Training Loss:  2.1079169741715305\n",
      "Epoch 1107. Learning Rate: 0.00025 Total Training Loss:  2.110319811035879\n",
      "Epoch 1108. Learning Rate: 0.00025 Total Training Loss:  2.1063599428453017\n",
      "Epoch 1109. Learning Rate: 0.00025 Total Training Loss:  2.1083988406462595\n",
      "Epoch 1110. Learning Rate: 0.00025 Total Training Loss:  2.1065995630051475\n",
      "Epoch 1111. Learning Rate: 0.00025 Total Training Loss:  2.105384644586593\n",
      "Epoch 1112. Learning Rate: 0.00025 Total Training Loss:  2.1056288446416147\n",
      "Epoch 1113. Learning Rate: 0.00025 Total Training Loss:  2.1069314370688517\n",
      "Epoch 1114. Learning Rate: 0.00025 Total Training Loss:  2.1062849403824657\n",
      "Epoch 1115. Learning Rate: 0.00025 Total Training Loss:  2.104687879589619\n",
      "Epoch 1116. Learning Rate: 0.00025 Total Training Loss:  2.105622813338414\n",
      "Epoch 1117. Learning Rate: 0.00025 Total Training Loss:  2.100480730761774\n",
      "Epoch 1118. Learning Rate: 0.00025 Total Training Loss:  2.105216257390566\n",
      "Epoch 1119. Learning Rate: 0.00025 Total Training Loss:  2.106503620860167\n",
      "Epoch 1120. Learning Rate: 0.00025 Total Training Loss:  2.1040372969873715\n",
      "Epoch 1121. Learning Rate: 0.00025 Total Training Loss:  2.0994022781960666\n",
      "Epoch 1122. Learning Rate: 0.00025 Total Training Loss:  2.1040651244693436\n",
      "Epoch 1123. Learning Rate: 0.00025 Total Training Loss:  2.1016112069482915\n",
      "Epoch 1124. Learning Rate: 0.00025 Total Training Loss:  2.102327166503528\n",
      "Epoch 1125. Learning Rate: 0.00025 Total Training Loss:  2.104821109038312\n",
      "Epoch 1126. Learning Rate: 0.00025 Total Training Loss:  2.099501751625212\n",
      "Epoch 1127. Learning Rate: 0.00025 Total Training Loss:  2.100424925913103\n",
      "Epoch 1128. Learning Rate: 0.00025 Total Training Loss:  2.102307366556488\n",
      "Epoch 1129. Learning Rate: 0.00025 Total Training Loss:  2.0996274102071766\n",
      "Epoch 1130. Learning Rate: 0.00025 Total Training Loss:  2.0966280379216187\n",
      "Epoch 1131. Learning Rate: 0.00025 Total Training Loss:  2.1023272550082766\n",
      "Epoch 1132. Learning Rate: 0.00025 Total Training Loss:  2.09770260102232\n",
      "Epoch 1133. Learning Rate: 0.00025 Total Training Loss:  2.100899770972319\n",
      "Epoch 1134. Learning Rate: 0.00025 Total Training Loss:  2.09860819845926\n",
      "Epoch 1135. Learning Rate: 0.00025 Total Training Loss:  2.0977345815626904\n",
      "Epoch 1136. Learning Rate: 0.00025 Total Training Loss:  2.095193430170184\n",
      "Epoch 1137. Learning Rate: 0.00025 Total Training Loss:  2.0987457766896114\n",
      "Epoch 1138. Learning Rate: 0.00025 Total Training Loss:  2.0977234345918987\n",
      "Epoch 1139. Learning Rate: 0.00025 Total Training Loss:  2.094747391558485\n",
      "Epoch 1140. Learning Rate: 0.00025 Total Training Loss:  2.0970846484124195\n",
      "Epoch 1141. Learning Rate: 0.00025 Total Training Loss:  2.097332801669836\n",
      "Epoch 1142. Learning Rate: 0.00025 Total Training Loss:  2.0959141244529746\n",
      "Epoch 1143. Learning Rate: 0.00025 Total Training Loss:  2.0935001323814504\n",
      "Epoch 1144. Learning Rate: 0.00025 Total Training Loss:  2.096329063846497\n",
      "Epoch 1145. Learning Rate: 0.00025 Total Training Loss:  2.0914619246614166\n",
      "Epoch 1146. Learning Rate: 0.00025 Total Training Loss:  2.0958414304768667\n",
      "Epoch 1147. Learning Rate: 0.00025 Total Training Loss:  2.093045215937309\n",
      "Epoch 1148. Learning Rate: 0.00025 Total Training Loss:  2.0961640124733094\n",
      "Epoch 1149. Learning Rate: 0.00025 Total Training Loss:  2.0940078741987236\n",
      "Epoch 1150. Learning Rate: 0.00025 Total Training Loss:  2.092644406511681\n",
      "Epoch 1151. Learning Rate: 0.00025 Total Training Loss:  2.0915399818331935\n",
      "Epoch 1152. Learning Rate: 0.00025 Total Training Loss:  2.0917732065718155\n",
      "Epoch 1153. Learning Rate: 0.00025 Total Training Loss:  2.0895626560377423\n",
      "Epoch 1154. Learning Rate: 0.00025 Total Training Loss:  2.0928422377619427\n",
      "Epoch 1155. Learning Rate: 0.00025 Total Training Loss:  2.086387051123893\n",
      "Epoch 1156. Learning Rate: 0.00025 Total Training Loss:  2.091258098836988\n",
      "Epoch 1157. Learning Rate: 0.00025 Total Training Loss:  2.0904912755650003\n",
      "Epoch 1158. Learning Rate: 0.00025 Total Training Loss:  2.0921750097186305\n",
      "Epoch 1159. Learning Rate: 0.00025 Total Training Loss:  2.0893001633230597\n",
      "Epoch 1160. Learning Rate: 0.00025 Total Training Loss:  2.085401652904693\n",
      "Epoch 1161. Learning Rate: 0.00025 Total Training Loss:  2.0889898226596415\n",
      "Epoch 1162. Learning Rate: 0.00025 Total Training Loss:  2.0881808041012846\n",
      "Epoch 1163. Learning Rate: 0.00025 Total Training Loss:  2.08790999581106\n",
      "Epoch 1164. Learning Rate: 0.00025 Total Training Loss:  2.0884539077233057\n",
      "Epoch 1165. Learning Rate: 0.00025 Total Training Loss:  2.087091574212536\n",
      "Epoch 1166. Learning Rate: 0.00025 Total Training Loss:  2.0870335555518977\n",
      "Epoch 1167. Learning Rate: 0.00025 Total Training Loss:  2.086097373801749\n",
      "Epoch 1168. Learning Rate: 0.00025 Total Training Loss:  2.0850622054131236\n",
      "Epoch 1169. Learning Rate: 0.00025 Total Training Loss:  2.0849125644308515\n",
      "Epoch 1170. Learning Rate: 0.00025 Total Training Loss:  2.0845795448112767\n",
      "Epoch 1171. Learning Rate: 0.00025 Total Training Loss:  2.0867801787680946\n",
      "Epoch 1172. Learning Rate: 0.00025 Total Training Loss:  2.0858954610303044\n",
      "Epoch 1173. Learning Rate: 0.00025 Total Training Loss:  2.0831796363636386\n",
      "Epoch 1174. Learning Rate: 0.00025 Total Training Loss:  2.0868415805744007\n",
      "Epoch 1175. Learning Rate: 0.00025 Total Training Loss:  2.0804252252564766\n",
      "Epoch 1176. Learning Rate: 0.00025 Total Training Loss:  2.0835633401293308\n",
      "Epoch 1177. Learning Rate: 0.00025 Total Training Loss:  2.0825898160401266\n",
      "Epoch 1178. Learning Rate: 0.00025 Total Training Loss:  2.0854964336031117\n",
      "Epoch 1179. Learning Rate: 0.00025 Total Training Loss:  2.0828697753022425\n",
      "Epoch 1180. Learning Rate: 0.00025 Total Training Loss:  2.0825712652876973\n",
      "Epoch 1181. Learning Rate: 0.00025 Total Training Loss:  2.0823117972468026\n",
      "Epoch 1182. Learning Rate: 0.00025 Total Training Loss:  2.082763024227461\n",
      "Epoch 1183. Learning Rate: 0.00025 Total Training Loss:  2.0857883335847873\n",
      "Epoch 1184. Learning Rate: 0.00025 Total Training Loss:  2.081153268023627\n",
      "Epoch 1185. Learning Rate: 0.00025 Total Training Loss:  2.07836764579406\n",
      "Epoch 1186. Learning Rate: 0.00025 Total Training Loss:  2.0827125467767473\n",
      "Epoch 1187. Learning Rate: 0.00025 Total Training Loss:  2.0797786082257517\n",
      "Epoch 1188. Learning Rate: 0.00025 Total Training Loss:  2.081250167102553\n",
      "Epoch 1189. Learning Rate: 0.00025 Total Training Loss:  2.0778941884927917\n",
      "Epoch 1190. Learning Rate: 0.00025 Total Training Loss:  2.082079799467465\n",
      "Epoch 1191. Learning Rate: 0.00025 Total Training Loss:  2.076991933543468\n",
      "Epoch 1192. Learning Rate: 0.00025 Total Training Loss:  2.0785226211883128\n",
      "Epoch 1193. Learning Rate: 0.00025 Total Training Loss:  2.0770882478100248\n",
      "Epoch 1194. Learning Rate: 0.00025 Total Training Loss:  2.0760130634880625\n",
      "Epoch 1195. Learning Rate: 0.00025 Total Training Loss:  2.079568219574867\n",
      "Epoch 1196. Learning Rate: 0.00025 Total Training Loss:  2.075405933399452\n",
      "Epoch 1197. Learning Rate: 0.00025 Total Training Loss:  2.078686971042771\n",
      "Epoch 1198. Learning Rate: 0.00025 Total Training Loss:  2.0776245251472574\n",
      "Epoch 1199. Learning Rate: 0.00025 Total Training Loss:  2.0749210994690657\n",
      "Epoch 1200. Learning Rate: 0.00025 Total Training Loss:  2.075911861495115\n",
      "Epoch 1201. Learning Rate: 0.00025 Total Training Loss:  2.077925308345584\n",
      "Epoch 1202. Learning Rate: 0.00025 Total Training Loss:  2.074138107098406\n",
      "Epoch 1203. Learning Rate: 0.00025 Total Training Loss:  2.0759106143086683\n",
      "Epoch 1204. Learning Rate: 0.00025 Total Training Loss:  2.073495952266967\n",
      "Epoch 1205. Learning Rate: 0.00025 Total Training Loss:  2.07509866298642\n",
      "Epoch 1206. Learning Rate: 0.00025 Total Training Loss:  2.0727968991268426\n",
      "Epoch 1207. Learning Rate: 0.00025 Total Training Loss:  2.071925599098904\n",
      "Epoch 1208. Learning Rate: 0.00025 Total Training Loss:  2.073640150076244\n",
      "Epoch 1209. Learning Rate: 0.00025 Total Training Loss:  2.074480096809566\n",
      "Epoch 1210. Learning Rate: 0.00025 Total Training Loss:  2.073430203978205\n",
      "Epoch 1211. Learning Rate: 0.00025 Total Training Loss:  2.0742642262775917\n",
      "Epoch 1212. Learning Rate: 0.00025 Total Training Loss:  2.070999757939717\n",
      "Epoch 1213. Learning Rate: 0.00025 Total Training Loss:  2.073647034703754\n",
      "Epoch 1214. Learning Rate: 0.00025 Total Training Loss:  2.0702385549084283\n",
      "Epoch 1215. Learning Rate: 0.00025 Total Training Loss:  2.0751701848057564\n",
      "Epoch 1216. Learning Rate: 0.00025 Total Training Loss:  2.0689497133134864\n",
      "Epoch 1217. Learning Rate: 0.00025 Total Training Loss:  2.0727832052798476\n",
      "Epoch 1218. Learning Rate: 0.00025 Total Training Loss:  2.0692200642370153\n",
      "Epoch 1219. Learning Rate: 0.00025 Total Training Loss:  2.070025018765591\n",
      "Epoch 1220. Learning Rate: 0.00025 Total Training Loss:  2.0698417029052507\n",
      "Epoch 1221. Learning Rate: 0.00025 Total Training Loss:  2.071029668295523\n",
      "Epoch 1222. Learning Rate: 0.00025 Total Training Loss:  2.0688057456864044\n",
      "Epoch 1223. Learning Rate: 0.00025 Total Training Loss:  2.0703386171371676\n",
      "Epoch 1224. Learning Rate: 0.00025 Total Training Loss:  2.0679873630870134\n",
      "Epoch 1225. Learning Rate: 0.00025 Total Training Loss:  2.069692575838417\n",
      "Epoch 1226. Learning Rate: 0.00025 Total Training Loss:  2.068069395812927\n",
      "Epoch 1227. Learning Rate: 0.00025 Total Training Loss:  2.0666273116949014\n",
      "Epoch 1228. Learning Rate: 0.00025 Total Training Loss:  2.0672443389776163\n",
      "Epoch 1229. Learning Rate: 0.00025 Total Training Loss:  2.0682074711367022\n",
      "Epoch 1230. Learning Rate: 0.00025 Total Training Loss:  2.0691623453167267\n",
      "Epoch 1231. Learning Rate: 0.00025 Total Training Loss:  2.0677538500749506\n",
      "Epoch 1232. Learning Rate: 0.00025 Total Training Loss:  2.0661996850103606\n",
      "Epoch 1233. Learning Rate: 0.00025 Total Training Loss:  2.0674941922188736\n",
      "Epoch 1234. Learning Rate: 0.00025 Total Training Loss:  2.0671380891581066\n",
      "Epoch 1235. Learning Rate: 0.00025 Total Training Loss:  2.0623444637749344\n",
      "Epoch 1236. Learning Rate: 0.00025 Total Training Loss:  2.0667406939610373\n",
      "Epoch 1237. Learning Rate: 0.00025 Total Training Loss:  2.064246166322846\n",
      "Epoch 1238. Learning Rate: 0.00025 Total Training Loss:  2.0659584717068356\n",
      "Epoch 1239. Learning Rate: 0.00025 Total Training Loss:  2.065995272510918\n",
      "Epoch 1240. Learning Rate: 0.00025 Total Training Loss:  2.062729402590776\n",
      "Epoch 1241. Learning Rate: 0.00025 Total Training Loss:  2.0640654365706723\n",
      "Epoch 1242. Learning Rate: 0.00025 Total Training Loss:  2.065185575163923\n",
      "Epoch 1243. Learning Rate: 0.00025 Total Training Loss:  2.0641948343545664\n",
      "Epoch 1244. Learning Rate: 0.00025 Total Training Loss:  2.062268434703583\n",
      "Epoch 1245. Learning Rate: 0.00025 Total Training Loss:  2.065560641756747\n",
      "Epoch 1246. Learning Rate: 0.00025 Total Training Loss:  2.061487728264183\n",
      "Epoch 1247. Learning Rate: 0.00025 Total Training Loss:  2.0623823946516495\n",
      "Epoch 1248. Learning Rate: 0.00025 Total Training Loss:  2.0626886412501335\n",
      "Epoch 1249. Learning Rate: 0.00025 Total Training Loss:  2.0624784783576615\n",
      "Epoch 1250. Learning Rate: 0.00025 Total Training Loss:  2.06167099592858\n",
      "Epoch 1251. Learning Rate: 0.00025 Total Training Loss:  2.061204813828226\n",
      "Epoch 1252. Learning Rate: 0.00025 Total Training Loss:  2.064033867616672\n",
      "Epoch 1253. Learning Rate: 0.00025 Total Training Loss:  2.061787821148755\n",
      "Epoch 1254. Learning Rate: 0.00025 Total Training Loss:  2.0590518220851664\n",
      "Epoch 1255. Learning Rate: 0.00025 Total Training Loss:  2.060656188026769\n",
      "Epoch 1256. Learning Rate: 0.00025 Total Training Loss:  2.062637489376357\n",
      "Epoch 1257. Learning Rate: 0.00025 Total Training Loss:  2.0599963376880623\n",
      "Epoch 1258. Learning Rate: 0.00025 Total Training Loss:  2.0565140936232638\n",
      "Epoch 1259. Learning Rate: 0.00025 Total Training Loss:  2.0601411066018045\n",
      "Epoch 1260. Learning Rate: 0.00025 Total Training Loss:  2.058639074792154\n",
      "Epoch 1261. Learning Rate: 0.00025 Total Training Loss:  2.0598049590480514\n",
      "Epoch 1262. Learning Rate: 0.00025 Total Training Loss:  2.0601370346266776\n",
      "Epoch 1263. Learning Rate: 0.00025 Total Training Loss:  2.0584211554960348\n",
      "Epoch 1264. Learning Rate: 0.00025 Total Training Loss:  2.058311224449426\n",
      "Epoch 1265. Learning Rate: 0.00025 Total Training Loss:  2.057666600769153\n",
      "Epoch 1266. Learning Rate: 0.00025 Total Training Loss:  2.057746926409891\n",
      "Epoch 1267. Learning Rate: 0.00025 Total Training Loss:  2.058939740702044\n",
      "Epoch 1268. Learning Rate: 0.00025 Total Training Loss:  2.056346532946918\n",
      "Epoch 1269. Learning Rate: 0.00025 Total Training Loss:  2.055323720560409\n",
      "Epoch 1270. Learning Rate: 0.00025 Total Training Loss:  2.05855803747545\n",
      "Epoch 1271. Learning Rate: 0.00025 Total Training Loss:  2.0567804044112563\n",
      "Epoch 1272. Learning Rate: 0.00025 Total Training Loss:  2.0575352421728894\n",
      "Epoch 1273. Learning Rate: 0.00025 Total Training Loss:  2.054289159743348\n",
      "Epoch 1274. Learning Rate: 0.00025 Total Training Loss:  2.0585496749554295\n",
      "Epoch 1275. Learning Rate: 0.00025 Total Training Loss:  2.054464436485432\n",
      "Epoch 1276. Learning Rate: 0.00025 Total Training Loss:  2.0565139333775733\n",
      "Epoch 1277. Learning Rate: 0.00025 Total Training Loss:  2.0519489668949973\n",
      "Epoch 1278. Learning Rate: 0.00025 Total Training Loss:  2.054946532298345\n",
      "Epoch 1279. Learning Rate: 0.00025 Total Training Loss:  2.0567426387860905\n",
      "Epoch 1280. Learning Rate: 0.00025 Total Training Loss:  2.0533712455653585\n",
      "Epoch 1281. Learning Rate: 0.00025 Total Training Loss:  2.053599270468112\n",
      "Epoch 1282. Learning Rate: 0.00025 Total Training Loss:  2.053953879629262\n",
      "Epoch 1283. Learning Rate: 0.00025 Total Training Loss:  2.0528533316974062\n",
      "Epoch 1284. Learning Rate: 0.00025 Total Training Loss:  2.055524790281197\n",
      "Epoch 1285. Learning Rate: 0.00025 Total Training Loss:  2.0501036921632476\n",
      "Epoch 1286. Learning Rate: 0.00025 Total Training Loss:  2.0536245250550564\n",
      "Epoch 1287. Learning Rate: 0.00025 Total Training Loss:  2.053751207276946\n",
      "Epoch 1288. Learning Rate: 0.00025 Total Training Loss:  2.0550443354877643\n",
      "Epoch 1289. Learning Rate: 0.00025 Total Training Loss:  2.049931386281969\n",
      "Epoch 1290. Learning Rate: 0.00025 Total Training Loss:  2.0509470300748944\n",
      "Epoch 1291. Learning Rate: 0.00025 Total Training Loss:  2.0501231886155438\n",
      "Epoch 1292. Learning Rate: 0.00025 Total Training Loss:  2.052371798781678\n",
      "Epoch 1293. Learning Rate: 0.00025 Total Training Loss:  2.0484351181949023\n",
      "Epoch 1294. Learning Rate: 0.00025 Total Training Loss:  2.0527912217366975\n",
      "Epoch 1295. Learning Rate: 0.00025 Total Training Loss:  2.0498039521626197\n",
      "Epoch 1296. Learning Rate: 0.00025 Total Training Loss:  2.050197466131067\n",
      "Epoch 1297. Learning Rate: 0.00025 Total Training Loss:  2.0515686343132984\n",
      "Epoch 1298. Learning Rate: 0.00025 Total Training Loss:  2.048774263530504\n",
      "Epoch 1299. Learning Rate: 0.00025 Total Training Loss:  2.0499930439400487\n",
      "Epoch 1300. Learning Rate: 0.00025 Total Training Loss:  2.0485273098165635\n",
      "Epoch 1301. Learning Rate: 0.00025 Total Training Loss:  2.0482825795188546\n",
      "Epoch 1302. Learning Rate: 0.00025 Total Training Loss:  2.0485243364528287\n",
      "Epoch 1303. Learning Rate: 0.00025 Total Training Loss:  2.0499366738367826\n",
      "Epoch 1304. Learning Rate: 0.00025 Total Training Loss:  2.0471402215771377\n",
      "Epoch 1305. Learning Rate: 0.00025 Total Training Loss:  2.04826011339901\n",
      "Epoch 1306. Learning Rate: 0.00025 Total Training Loss:  2.0485223125724588\n",
      "Epoch 1307. Learning Rate: 0.00025 Total Training Loss:  2.045713417668594\n",
      "Epoch 1308. Learning Rate: 0.00025 Total Training Loss:  2.0491012869169936\n",
      "Epoch 1309. Learning Rate: 0.00025 Total Training Loss:  2.0462350462621544\n",
      "Epoch 1310. Learning Rate: 0.00025 Total Training Loss:  2.0471831759496126\n",
      "Epoch 1311. Learning Rate: 0.00025 Total Training Loss:  2.048418974125525\n",
      "Epoch 1312. Learning Rate: 0.00025 Total Training Loss:  2.0440010688034818\n",
      "Epoch 1313. Learning Rate: 0.00025 Total Training Loss:  2.0462950440414716\n",
      "Epoch 1314. Learning Rate: 0.00025 Total Training Loss:  2.0473728665092494\n",
      "Epoch 1315. Learning Rate: 0.00025 Total Training Loss:  2.0451601127861068\n",
      "Epoch 1316. Learning Rate: 0.00025 Total Training Loss:  2.046256140281912\n",
      "Epoch 1317. Learning Rate: 0.00025 Total Training Loss:  2.0470963685947936\n",
      "Epoch 1318. Learning Rate: 0.00025 Total Training Loss:  2.043318463722244\n",
      "Epoch 1319. Learning Rate: 0.00025 Total Training Loss:  2.0447671611618716\n",
      "Epoch 1320. Learning Rate: 0.00025 Total Training Loss:  2.0448130447184667\n",
      "Epoch 1321. Learning Rate: 0.00025 Total Training Loss:  2.045178817876149\n",
      "Epoch 1322. Learning Rate: 0.00025 Total Training Loss:  2.0422645879152697\n",
      "Epoch 1323. Learning Rate: 0.00025 Total Training Loss:  2.043550245492952\n",
      "Epoch 1324. Learning Rate: 0.00025 Total Training Loss:  2.0444849947525654\n",
      "Epoch 1325. Learning Rate: 0.00025 Total Training Loss:  2.0427116382925306\n",
      "Epoch 1326. Learning Rate: 0.00025 Total Training Loss:  2.043574062292464\n",
      "Epoch 1327. Learning Rate: 0.00025 Total Training Loss:  2.043999662157148\n",
      "Epoch 1328. Learning Rate: 0.00025 Total Training Loss:  2.0431510828784667\n",
      "Epoch 1329. Learning Rate: 0.00025 Total Training Loss:  2.0402474400762003\n",
      "Epoch 1330. Learning Rate: 0.00025 Total Training Loss:  2.0430481868679635\n",
      "Epoch 1331. Learning Rate: 0.00025 Total Training Loss:  2.042865452473052\n",
      "Epoch 1332. Learning Rate: 0.00025 Total Training Loss:  2.0397486058063805\n",
      "Epoch 1333. Learning Rate: 0.00025 Total Training Loss:  2.0422354014590383\n",
      "Epoch 1334. Learning Rate: 0.00025 Total Training Loss:  2.041378015273949\n",
      "Epoch 1335. Learning Rate: 0.00025 Total Training Loss:  2.040451600885717\n",
      "Epoch 1336. Learning Rate: 0.00025 Total Training Loss:  2.0391200956655666\n",
      "Epoch 1337. Learning Rate: 0.00025 Total Training Loss:  2.0435823255684227\n",
      "Epoch 1338. Learning Rate: 0.00025 Total Training Loss:  2.0365189604635816\n",
      "Epoch 1339. Learning Rate: 0.00025 Total Training Loss:  2.040507631318178\n",
      "Epoch 1340. Learning Rate: 0.00025 Total Training Loss:  2.038597816019319\n",
      "Epoch 1341. Learning Rate: 0.00025 Total Training Loss:  2.03874696380808\n",
      "Epoch 1342. Learning Rate: 0.00025 Total Training Loss:  2.0384030559216626\n",
      "Epoch 1343. Learning Rate: 0.00025 Total Training Loss:  2.0378151020850055\n",
      "Epoch 1344. Learning Rate: 0.00025 Total Training Loss:  2.0408288335893303\n",
      "Epoch 1345. Learning Rate: 0.00025 Total Training Loss:  2.035712894779863\n",
      "Epoch 1346. Learning Rate: 0.00025 Total Training Loss:  2.038195618544705\n",
      "Epoch 1347. Learning Rate: 0.00025 Total Training Loss:  2.036331238603452\n",
      "Epoch 1348. Learning Rate: 0.00025 Total Training Loss:  2.0348472750920337\n",
      "Epoch 1349. Learning Rate: 0.00025 Total Training Loss:  2.037958229833748\n",
      "Epoch 1350. Learning Rate: 0.00025 Total Training Loss:  2.0339886547008064\n",
      "Epoch 1351. Learning Rate: 0.00025 Total Training Loss:  2.0390928122214973\n",
      "Epoch 1352. Learning Rate: 0.00025 Total Training Loss:  2.0367109963844996\n",
      "Epoch 1353. Learning Rate: 0.00025 Total Training Loss:  2.0333561105071567\n",
      "Epoch 1354. Learning Rate: 0.00025 Total Training Loss:  2.036343003681395\n",
      "Epoch 1355. Learning Rate: 0.00025 Total Training Loss:  2.038896721234778\n",
      "Epoch 1356. Learning Rate: 0.00025 Total Training Loss:  2.036497268563835\n",
      "Epoch 1357. Learning Rate: 0.00025 Total Training Loss:  2.031991773401387\n",
      "Epoch 1358. Learning Rate: 0.00025 Total Training Loss:  2.0343951084068976\n",
      "Epoch 1359. Learning Rate: 0.00025 Total Training Loss:  2.035100739740301\n",
      "Epoch 1360. Learning Rate: 0.00025 Total Training Loss:  2.0365642482356634\n",
      "Epoch 1361. Learning Rate: 0.00025 Total Training Loss:  2.031436457851669\n",
      "Epoch 1362. Learning Rate: 0.00025 Total Training Loss:  2.0339783262461424\n",
      "Epoch 1363. Learning Rate: 0.00025 Total Training Loss:  2.0338635111402255\n",
      "Epoch 1364. Learning Rate: 0.00025 Total Training Loss:  2.0335602728882805\n",
      "Epoch 1365. Learning Rate: 0.00025 Total Training Loss:  2.035796010895865\n",
      "Epoch 1366. Learning Rate: 0.00025 Total Training Loss:  2.028841782332165\n",
      "Epoch 1367. Learning Rate: 0.00025 Total Training Loss:  2.0364390958857257\n",
      "Epoch 1368. Learning Rate: 0.00025 Total Training Loss:  2.033000091643771\n",
      "Epoch 1369. Learning Rate: 0.00025 Total Training Loss:  2.0283116464852355\n",
      "Epoch 1370. Learning Rate: 0.00025 Total Training Loss:  2.0319747387547977\n",
      "Epoch 1371. Learning Rate: 0.00025 Total Training Loss:  2.0324868792667985\n",
      "Epoch 1372. Learning Rate: 0.00025 Total Training Loss:  2.03121076640673\n",
      "Epoch 1373. Learning Rate: 0.00025 Total Training Loss:  2.0308943347481545\n",
      "Epoch 1374. Learning Rate: 0.00025 Total Training Loss:  2.0304039822658524\n",
      "Epoch 1375. Learning Rate: 0.00025 Total Training Loss:  2.0306707962590735\n",
      "Epoch 1376. Learning Rate: 0.00025 Total Training Loss:  2.03255568043096\n",
      "Epoch 1377. Learning Rate: 0.00025 Total Training Loss:  2.0248392192297615\n",
      "Epoch 1378. Learning Rate: 0.00025 Total Training Loss:  2.0335745179036167\n",
      "Epoch 1379. Learning Rate: 0.00025 Total Training Loss:  2.0282379366399255\n",
      "Epoch 1380. Learning Rate: 0.00025 Total Training Loss:  2.028671893349383\n",
      "Epoch 1381. Learning Rate: 0.00025 Total Training Loss:  2.0315382173866965\n",
      "Epoch 1382. Learning Rate: 0.00025 Total Training Loss:  2.028148748941021\n",
      "Epoch 1383. Learning Rate: 0.00025 Total Training Loss:  2.0288777914247476\n",
      "Epoch 1384. Learning Rate: 0.00025 Total Training Loss:  2.030529671756085\n",
      "Epoch 1385. Learning Rate: 0.00025 Total Training Loss:  2.028531060088426\n",
      "Epoch 1386. Learning Rate: 0.00025 Total Training Loss:  2.0255514812888578\n",
      "Epoch 1387. Learning Rate: 0.00025 Total Training Loss:  2.0263168969831895\n",
      "Epoch 1388. Learning Rate: 0.00025 Total Training Loss:  2.0278847404406406\n",
      "Epoch 1389. Learning Rate: 0.00025 Total Training Loss:  2.0247226312349085\n",
      "Epoch 1390. Learning Rate: 0.00025 Total Training Loss:  2.0285643783281557\n",
      "Epoch 1391. Learning Rate: 0.00025 Total Training Loss:  2.0247129458293784\n",
      "Epoch 1392. Learning Rate: 0.00025 Total Training Loss:  2.0288062980689574\n",
      "Epoch 1393. Learning Rate: 0.00025 Total Training Loss:  2.028316884272499\n",
      "Epoch 1394. Learning Rate: 0.00025 Total Training Loss:  2.022070796811022\n",
      "Epoch 1395. Learning Rate: 0.00025 Total Training Loss:  2.0266582139011007\n",
      "Epoch 1396. Learning Rate: 0.00025 Total Training Loss:  2.0260826949379407\n",
      "Epoch 1397. Learning Rate: 0.00025 Total Training Loss:  2.0271167246974073\n",
      "Epoch 1398. Learning Rate: 0.00025 Total Training Loss:  2.0208266126574017\n",
      "Epoch 1399. Learning Rate: 0.00025 Total Training Loss:  2.0273762326396536\n",
      "Epoch 1400. Learning Rate: 0.00025 Total Training Loss:  2.026745207986096\n",
      "Epoch 1401. Learning Rate: 0.00025 Total Training Loss:  2.023353172640782\n",
      "Epoch 1402. Learning Rate: 0.00025 Total Training Loss:  2.0236600163625553\n",
      "Epoch 1403. Learning Rate: 0.00025 Total Training Loss:  2.021852568403119\n",
      "Epoch 1404. Learning Rate: 0.00025 Total Training Loss:  2.0241397998761386\n",
      "Epoch 1405. Learning Rate: 0.00025 Total Training Loss:  2.022132929851068\n",
      "Epoch 1406. Learning Rate: 0.00025 Total Training Loss:  2.027032301266445\n",
      "Epoch 1407. Learning Rate: 0.00025 Total Training Loss:  2.0208771089091897\n",
      "Epoch 1408. Learning Rate: 0.00025 Total Training Loss:  2.0229944705206435\n",
      "Epoch 1409. Learning Rate: 0.00025 Total Training Loss:  2.022097644250607\n",
      "Epoch 1410. Learning Rate: 0.00025 Total Training Loss:  2.022139650187455\n",
      "Epoch 1411. Learning Rate: 0.00025 Total Training Loss:  2.0208989583770745\n",
      "Epoch 1412. Learning Rate: 0.00025 Total Training Loss:  2.023375107906759\n",
      "Epoch 1413. Learning Rate: 0.00025 Total Training Loss:  2.0202443425077945\n",
      "Epoch 1414. Learning Rate: 0.00025 Total Training Loss:  2.0205764045531396\n",
      "Epoch 1415. Learning Rate: 0.00025 Total Training Loss:  2.0198678133892827\n",
      "Epoch 1416. Learning Rate: 0.00025 Total Training Loss:  2.021584758767858\n",
      "Epoch 1417. Learning Rate: 0.00025 Total Training Loss:  2.0188375258876476\n",
      "Epoch 1418. Learning Rate: 0.00025 Total Training Loss:  2.020953191269655\n",
      "Epoch 1419. Learning Rate: 0.00025 Total Training Loss:  2.020772807591129\n",
      "Epoch 1420. Learning Rate: 0.00025 Total Training Loss:  2.016845191566972\n",
      "Epoch 1421. Learning Rate: 0.00025 Total Training Loss:  2.0205396912351716\n",
      "Epoch 1422. Learning Rate: 0.00025 Total Training Loss:  2.0193122140772175\n",
      "Epoch 1423. Learning Rate: 0.00025 Total Training Loss:  2.0193813402729575\n",
      "Epoch 1424. Learning Rate: 0.00025 Total Training Loss:  2.017207888129633\n",
      "Epoch 1425. Learning Rate: 0.00025 Total Training Loss:  2.0191851163108367\n",
      "Epoch 1426. Learning Rate: 0.00025 Total Training Loss:  2.0146652338444255\n",
      "Epoch 1427. Learning Rate: 0.00025 Total Training Loss:  2.0186151574598625\n",
      "Epoch 1428. Learning Rate: 0.00025 Total Training Loss:  2.0164419527864084\n",
      "Epoch 1429. Learning Rate: 0.00025 Total Training Loss:  2.019189123151591\n",
      "Epoch 1430. Learning Rate: 0.00025 Total Training Loss:  2.016436931997305\n",
      "Epoch 1431. Learning Rate: 0.00025 Total Training Loss:  2.0165657170291524\n",
      "Epoch 1432. Learning Rate: 0.00025 Total Training Loss:  2.015467296441784\n",
      "Epoch 1433. Learning Rate: 0.00025 Total Training Loss:  2.0180739027564414\n",
      "Epoch 1434. Learning Rate: 0.00025 Total Training Loss:  2.017922676692251\n",
      "Epoch 1435. Learning Rate: 0.00025 Total Training Loss:  2.013525536021916\n",
      "Epoch 1436. Learning Rate: 0.00025 Total Training Loss:  2.0146631749230437\n",
      "Epoch 1437. Learning Rate: 0.00025 Total Training Loss:  2.014258348644944\n",
      "Epoch 1438. Learning Rate: 0.00025 Total Training Loss:  2.0160883916832972\n",
      "Epoch 1439. Learning Rate: 0.00025 Total Training Loss:  2.0136173906794284\n",
      "Epoch 1440. Learning Rate: 0.00025 Total Training Loss:  2.0156026385666337\n",
      "Epoch 1441. Learning Rate: 0.00025 Total Training Loss:  2.013607200671686\n",
      "Epoch 1442. Learning Rate: 0.00025 Total Training Loss:  2.0126498961471952\n",
      "Epoch 1443. Learning Rate: 0.00025 Total Training Loss:  2.0130103193805553\n",
      "Epoch 1444. Learning Rate: 0.00025 Total Training Loss:  2.0156276524649\n",
      "Epoch 1445. Learning Rate: 0.00025 Total Training Loss:  2.0144502697512507\n",
      "Epoch 1446. Learning Rate: 0.00025 Total Training Loss:  2.0110047548951115\n",
      "Epoch 1447. Learning Rate: 0.00025 Total Training Loss:  2.012469573470298\n",
      "Epoch 1448. Learning Rate: 0.00025 Total Training Loss:  2.013440489099594\n",
      "Epoch 1449. Learning Rate: 0.00025 Total Training Loss:  2.01122319206479\n",
      "Epoch 1450. Learning Rate: 0.00025 Total Training Loss:  2.0132768504263368\n",
      "Epoch 1451. Learning Rate: 0.00025 Total Training Loss:  2.010263959906297\n",
      "Epoch 1452. Learning Rate: 0.00025 Total Training Loss:  2.0127285345224664\n",
      "Epoch 1453. Learning Rate: 0.00025 Total Training Loss:  2.008871067664586\n",
      "Epoch 1454. Learning Rate: 0.00025 Total Training Loss:  2.0179518571239896\n",
      "Epoch 1455. Learning Rate: 0.00025 Total Training Loss:  2.005436462815851\n",
      "Epoch 1456. Learning Rate: 0.00025 Total Training Loss:  2.0143657044682186\n",
      "Epoch 1457. Learning Rate: 0.00025 Total Training Loss:  2.008334610145539\n",
      "Epoch 1458. Learning Rate: 0.00025 Total Training Loss:  2.0112631958327256\n",
      "Epoch 1459. Learning Rate: 0.00025 Total Training Loss:  2.008180707169231\n",
      "Epoch 1460. Learning Rate: 0.00025 Total Training Loss:  2.010429063171614\n",
      "Epoch 1461. Learning Rate: 0.00025 Total Training Loss:  2.008927060785936\n",
      "Epoch 1462. Learning Rate: 0.00025 Total Training Loss:  2.0134707643883303\n",
      "Epoch 1463. Learning Rate: 0.00025 Total Training Loss:  2.0051258602761663\n",
      "Epoch 1464. Learning Rate: 0.00025 Total Training Loss:  2.010263237025356\n",
      "Epoch 1465. Learning Rate: 0.00025 Total Training Loss:  2.008386693050852\n",
      "Epoch 1466. Learning Rate: 0.00025 Total Training Loss:  2.0065750902867876\n",
      "Epoch 1467. Learning Rate: 0.00025 Total Training Loss:  2.0083517405728344\n",
      "Epoch 1468. Learning Rate: 0.00025 Total Training Loss:  2.004370024864329\n",
      "Epoch 1469. Learning Rate: 0.00025 Total Training Loss:  2.0087406801758334\n",
      "Epoch 1470. Learning Rate: 0.00025 Total Training Loss:  2.008410976122832\n",
      "Epoch 1471. Learning Rate: 0.00025 Total Training Loss:  2.0079314321046695\n",
      "Epoch 1472. Learning Rate: 0.00025 Total Training Loss:  2.007386926881736\n",
      "Epoch 1473. Learning Rate: 0.00025 Total Training Loss:  2.005243999621598\n",
      "Epoch 1474. Learning Rate: 0.00025 Total Training Loss:  2.0061077477876097\n",
      "Epoch 1475. Learning Rate: 0.00025 Total Training Loss:  2.0079835482756607\n",
      "Epoch 1476. Learning Rate: 0.00025 Total Training Loss:  2.0074555212340783\n",
      "Epoch 1477. Learning Rate: 0.00025 Total Training Loss:  2.003855885268422\n",
      "Epoch 1478. Learning Rate: 0.00025 Total Training Loss:  2.0060734279686585\n",
      "Epoch 1479. Learning Rate: 0.00025 Total Training Loss:  2.006044411624316\n",
      "Epoch 1480. Learning Rate: 0.00025 Total Training Loss:  2.006810039951233\n",
      "Epoch 1481. Learning Rate: 0.00025 Total Training Loss:  2.0034329678164795\n",
      "Epoch 1482. Learning Rate: 0.00025 Total Training Loss:  2.00817462746636\n",
      "Epoch 1483. Learning Rate: 0.00025 Total Training Loss:  2.0010615795617923\n",
      "Epoch 1484. Learning Rate: 0.00025 Total Training Loss:  2.0075384404335637\n",
      "Epoch 1485. Learning Rate: 0.00025 Total Training Loss:  2.002668067725608\n",
      "Epoch 1486. Learning Rate: 0.00025 Total Training Loss:  2.0052208326524124\n",
      "Epoch 1487. Learning Rate: 0.00025 Total Training Loss:  2.00435556677985\n",
      "Epoch 1488. Learning Rate: 0.00025 Total Training Loss:  2.0031635595369153\n",
      "Epoch 1489. Learning Rate: 0.00025 Total Training Loss:  2.0018178928876296\n",
      "Epoch 1490. Learning Rate: 0.00025 Total Training Loss:  2.0034082666097675\n",
      "Epoch 1491. Learning Rate: 0.00025 Total Training Loss:  2.0041653476364445\n",
      "Epoch 1492. Learning Rate: 0.00025 Total Training Loss:  1.9991785102756694\n",
      "Epoch 1493. Learning Rate: 0.00025 Total Training Loss:  2.0009565315558575\n",
      "Epoch 1494. Learning Rate: 0.00025 Total Training Loss:  2.0050551322638057\n",
      "Epoch 1495. Learning Rate: 0.00025 Total Training Loss:  2.000338542391546\n",
      "Epoch 1496. Learning Rate: 0.00025 Total Training Loss:  2.0040380751306657\n",
      "Epoch 1497. Learning Rate: 0.00025 Total Training Loss:  1.9989642740692943\n",
      "Epoch 1498. Learning Rate: 0.00025 Total Training Loss:  2.0021411635098048\n",
      "Epoch 1499. Learning Rate: 0.00025 Total Training Loss:  1.9980338896566536\n",
      "Epoch 1500. Learning Rate: 0.000125 Total Training Loss:  2.007024370977888\n",
      "Epoch 1501. Learning Rate: 0.000125 Total Training Loss:  1.980571318010334\n",
      "Epoch 1502. Learning Rate: 0.000125 Total Training Loss:  1.954672473948449\n",
      "Epoch 1503. Learning Rate: 0.000125 Total Training Loss:  1.9544725655287039\n",
      "Epoch 1504. Learning Rate: 0.000125 Total Training Loss:  1.953115134354448\n",
      "Epoch 1505. Learning Rate: 0.000125 Total Training Loss:  1.9537825058359886\n",
      "Epoch 1506. Learning Rate: 0.000125 Total Training Loss:  1.9530008395959157\n",
      "Epoch 1507. Learning Rate: 0.000125 Total Training Loss:  1.9527507569582667\n",
      "Epoch 1508. Learning Rate: 0.000125 Total Training Loss:  1.9529480876226444\n",
      "Epoch 1509. Learning Rate: 0.000125 Total Training Loss:  1.9531141005863901\n",
      "Epoch 1510. Learning Rate: 0.000125 Total Training Loss:  1.9522236456396058\n",
      "Epoch 1511. Learning Rate: 0.000125 Total Training Loss:  1.952494365759776\n",
      "Epoch 1512. Learning Rate: 0.000125 Total Training Loss:  1.9520849432592513\n",
      "Epoch 1513. Learning Rate: 0.000125 Total Training Loss:  1.9524701901245862\n",
      "Epoch 1514. Learning Rate: 0.000125 Total Training Loss:  1.9527132607618114\n",
      "Epoch 1515. Learning Rate: 0.000125 Total Training Loss:  1.952316327195149\n",
      "Epoch 1516. Learning Rate: 0.000125 Total Training Loss:  1.952310369873885\n",
      "Epoch 1517. Learning Rate: 0.000125 Total Training Loss:  1.9511961941316258\n",
      "Epoch 1518. Learning Rate: 0.000125 Total Training Loss:  1.9525822346331552\n",
      "Epoch 1519. Learning Rate: 0.000125 Total Training Loss:  1.9513551724958234\n",
      "Epoch 1520. Learning Rate: 0.000125 Total Training Loss:  1.9514609755715355\n",
      "Epoch 1521. Learning Rate: 0.000125 Total Training Loss:  1.951597486884566\n",
      "Epoch 1522. Learning Rate: 0.000125 Total Training Loss:  1.9511511774617247\n",
      "Epoch 1523. Learning Rate: 0.000125 Total Training Loss:  1.9523025333764963\n",
      "Epoch 1524. Learning Rate: 0.000125 Total Training Loss:  1.9512143252941314\n",
      "Epoch 1525. Learning Rate: 0.000125 Total Training Loss:  1.9502058112120721\n",
      "Epoch 1526. Learning Rate: 0.000125 Total Training Loss:  1.9525555360887665\n",
      "Epoch 1527. Learning Rate: 0.000125 Total Training Loss:  1.9503078540437855\n",
      "Epoch 1528. Learning Rate: 0.000125 Total Training Loss:  1.9499991170596331\n",
      "Epoch 1529. Learning Rate: 0.000125 Total Training Loss:  1.950585772312479\n",
      "Epoch 1530. Learning Rate: 0.000125 Total Training Loss:  1.9503258980403189\n",
      "Epoch 1531. Learning Rate: 0.000125 Total Training Loss:  1.9500513174280059\n",
      "Epoch 1532. Learning Rate: 0.000125 Total Training Loss:  1.950016252201749\n",
      "Epoch 1533. Learning Rate: 0.000125 Total Training Loss:  1.9503728534036782\n",
      "Epoch 1534. Learning Rate: 0.000125 Total Training Loss:  1.9504789480997715\n",
      "Epoch 1535. Learning Rate: 0.000125 Total Training Loss:  1.9502875418693293\n",
      "Epoch 1536. Learning Rate: 0.000125 Total Training Loss:  1.9490259924496058\n",
      "Epoch 1537. Learning Rate: 0.000125 Total Training Loss:  1.9493102080014069\n",
      "Epoch 1538. Learning Rate: 0.000125 Total Training Loss:  1.948690390738193\n",
      "Epoch 1539. Learning Rate: 0.000125 Total Training Loss:  1.9496912068279926\n",
      "Epoch 1540. Learning Rate: 0.000125 Total Training Loss:  1.9491457534604706\n",
      "Epoch 1541. Learning Rate: 0.000125 Total Training Loss:  1.9483279702835716\n",
      "Epoch 1542. Learning Rate: 0.000125 Total Training Loss:  1.948580184369348\n",
      "Epoch 1543. Learning Rate: 0.000125 Total Training Loss:  1.9484806560503785\n",
      "Epoch 1544. Learning Rate: 0.000125 Total Training Loss:  1.948537235322874\n",
      "Epoch 1545. Learning Rate: 0.000125 Total Training Loss:  1.947599670587806\n",
      "Epoch 1546. Learning Rate: 0.000125 Total Training Loss:  1.9481961948040407\n",
      "Epoch 1547. Learning Rate: 0.000125 Total Training Loss:  1.9477776117273606\n",
      "Epoch 1548. Learning Rate: 0.000125 Total Training Loss:  1.947457049303921\n",
      "Epoch 1549. Learning Rate: 0.000125 Total Training Loss:  1.9470174663583748\n",
      "Epoch 1550. Learning Rate: 0.000125 Total Training Loss:  1.9482102394104004\n",
      "Epoch 1551. Learning Rate: 0.000125 Total Training Loss:  1.9487384133099113\n",
      "Epoch 1552. Learning Rate: 0.000125 Total Training Loss:  1.946193675918039\n",
      "Epoch 1553. Learning Rate: 0.000125 Total Training Loss:  1.9456810418632813\n",
      "Epoch 1554. Learning Rate: 0.000125 Total Training Loss:  1.9482542808109429\n",
      "Epoch 1555. Learning Rate: 0.000125 Total Training Loss:  1.9472827168356162\n",
      "Epoch 1556. Learning Rate: 0.000125 Total Training Loss:  1.9474254671949893\n",
      "Epoch 1557. Learning Rate: 0.000125 Total Training Loss:  1.9458876852877438\n",
      "Epoch 1558. Learning Rate: 0.000125 Total Training Loss:  1.9463302267540712\n",
      "Epoch 1559. Learning Rate: 0.000125 Total Training Loss:  1.9459291290841065\n",
      "Epoch 1560. Learning Rate: 0.000125 Total Training Loss:  1.9463099429267459\n",
      "Epoch 1561. Learning Rate: 0.000125 Total Training Loss:  1.9461651511664968\n",
      "Epoch 1562. Learning Rate: 0.000125 Total Training Loss:  1.944985981186619\n",
      "Epoch 1563. Learning Rate: 0.000125 Total Training Loss:  1.9457666380912997\n",
      "Epoch 1564. Learning Rate: 0.000125 Total Training Loss:  1.9458200740045868\n",
      "Epoch 1565. Learning Rate: 0.000125 Total Training Loss:  1.944669297983637\n",
      "Epoch 1566. Learning Rate: 0.000125 Total Training Loss:  1.946692796453135\n",
      "Epoch 1567. Learning Rate: 0.000125 Total Training Loss:  1.9441430520091671\n",
      "Epoch 1568. Learning Rate: 0.000125 Total Training Loss:  1.9455861916067079\n",
      "Epoch 1569. Learning Rate: 0.000125 Total Training Loss:  1.945001481421059\n",
      "Epoch 1570. Learning Rate: 0.000125 Total Training Loss:  1.9437363729521167\n",
      "Epoch 1571. Learning Rate: 0.000125 Total Training Loss:  1.9455366095644422\n",
      "Epoch 1572. Learning Rate: 0.000125 Total Training Loss:  1.9434640152030624\n",
      "Epoch 1573. Learning Rate: 0.000125 Total Training Loss:  1.9451545900956262\n",
      "Epoch 1574. Learning Rate: 0.000125 Total Training Loss:  1.9431396274012513\n",
      "Epoch 1575. Learning Rate: 0.000125 Total Training Loss:  1.9440789167711046\n",
      "Epoch 1576. Learning Rate: 0.000125 Total Training Loss:  1.944302648975281\n",
      "Epoch 1577. Learning Rate: 0.000125 Total Training Loss:  1.9438972126226872\n",
      "Epoch 1578. Learning Rate: 0.000125 Total Training Loss:  1.9432394567120355\n",
      "Epoch 1579. Learning Rate: 0.000125 Total Training Loss:  1.943981402699137\n",
      "Epoch 1580. Learning Rate: 0.000125 Total Training Loss:  1.94241298007546\n",
      "Epoch 1581. Learning Rate: 0.000125 Total Training Loss:  1.9451170031388756\n",
      "Epoch 1582. Learning Rate: 0.000125 Total Training Loss:  1.9420731416030321\n",
      "Epoch 1583. Learning Rate: 0.000125 Total Training Loss:  1.9440828420920298\n",
      "Epoch 1584. Learning Rate: 0.000125 Total Training Loss:  1.9412960411282256\n",
      "Epoch 1585. Learning Rate: 0.000125 Total Training Loss:  1.9433428303163964\n",
      "Epoch 1586. Learning Rate: 0.000125 Total Training Loss:  1.9419902834051754\n",
      "Epoch 1587. Learning Rate: 0.000125 Total Training Loss:  1.9435053432825953\n",
      "Epoch 1588. Learning Rate: 0.000125 Total Training Loss:  1.9410598900867626\n",
      "Epoch 1589. Learning Rate: 0.000125 Total Training Loss:  1.9422840343904682\n",
      "Epoch 1590. Learning Rate: 0.000125 Total Training Loss:  1.941067475505406\n",
      "Epoch 1591. Learning Rate: 0.000125 Total Training Loss:  1.9425270181091037\n",
      "Epoch 1592. Learning Rate: 0.000125 Total Training Loss:  1.9406990188290365\n",
      "Epoch 1593. Learning Rate: 0.000125 Total Training Loss:  1.9419863668153994\n",
      "Epoch 1594. Learning Rate: 0.000125 Total Training Loss:  1.9411189284001011\n",
      "Epoch 1595. Learning Rate: 0.000125 Total Training Loss:  1.942635134968441\n",
      "Epoch 1596. Learning Rate: 0.000125 Total Training Loss:  1.941407419682946\n",
      "Epoch 1597. Learning Rate: 0.000125 Total Training Loss:  1.941553402284626\n",
      "Epoch 1598. Learning Rate: 0.000125 Total Training Loss:  1.940094208373921\n",
      "Epoch 1599. Learning Rate: 0.000125 Total Training Loss:  1.940871096070623\n",
      "Epoch 1600. Learning Rate: 0.000125 Total Training Loss:  1.9408059286652133\n",
      "Epoch 1601. Learning Rate: 0.000125 Total Training Loss:  1.9403562064399011\n",
      "Epoch 1602. Learning Rate: 0.000125 Total Training Loss:  1.9393723052344285\n",
      "Epoch 1603. Learning Rate: 0.000125 Total Training Loss:  1.941536893020384\n",
      "Epoch 1604. Learning Rate: 0.000125 Total Training Loss:  1.9385483124351595\n",
      "Epoch 1605. Learning Rate: 0.000125 Total Training Loss:  1.9417938136612065\n",
      "Epoch 1606. Learning Rate: 0.000125 Total Training Loss:  1.93952548148809\n",
      "Epoch 1607. Learning Rate: 0.000125 Total Training Loss:  1.9416131737525575\n",
      "Epoch 1608. Learning Rate: 0.000125 Total Training Loss:  1.9386313908325974\n",
      "Epoch 1609. Learning Rate: 0.000125 Total Training Loss:  1.9406160129001364\n",
      "Epoch 1610. Learning Rate: 0.000125 Total Training Loss:  1.938521933829179\n",
      "Epoch 1611. Learning Rate: 0.000125 Total Training Loss:  1.9403714105137624\n",
      "Epoch 1612. Learning Rate: 0.000125 Total Training Loss:  1.9386443449184299\n",
      "Epoch 1613. Learning Rate: 0.000125 Total Training Loss:  1.9386127548932564\n",
      "Epoch 1614. Learning Rate: 0.000125 Total Training Loss:  1.9380582044250332\n",
      "Epoch 1615. Learning Rate: 0.000125 Total Training Loss:  1.9399987772922032\n",
      "Epoch 1616. Learning Rate: 0.000125 Total Training Loss:  1.938025795709109\n",
      "Epoch 1617. Learning Rate: 0.000125 Total Training Loss:  1.9384564752690494\n",
      "Epoch 1618. Learning Rate: 0.000125 Total Training Loss:  1.939014136500191\n",
      "Epoch 1619. Learning Rate: 0.000125 Total Training Loss:  1.9367892543668859\n",
      "Epoch 1620. Learning Rate: 0.000125 Total Training Loss:  1.938235363428248\n",
      "Epoch 1621. Learning Rate: 0.000125 Total Training Loss:  1.938486751372693\n",
      "Epoch 1622. Learning Rate: 0.000125 Total Training Loss:  1.9376264245074708\n",
      "Epoch 1623. Learning Rate: 0.000125 Total Training Loss:  1.9371033716015518\n",
      "Epoch 1624. Learning Rate: 0.000125 Total Training Loss:  1.9388595716736745\n",
      "Epoch 1625. Learning Rate: 0.000125 Total Training Loss:  1.9373013181611896\n",
      "Epoch 1626. Learning Rate: 0.000125 Total Training Loss:  1.936564917967189\n",
      "Epoch 1627. Learning Rate: 0.000125 Total Training Loss:  1.9374062527203932\n",
      "Epoch 1628. Learning Rate: 0.000125 Total Training Loss:  1.9371896357042715\n",
      "Epoch 1629. Learning Rate: 0.000125 Total Training Loss:  1.9364503262622748\n",
      "Epoch 1630. Learning Rate: 0.000125 Total Training Loss:  1.9363528969115578\n",
      "Epoch 1631. Learning Rate: 0.000125 Total Training Loss:  1.936589126038598\n",
      "Epoch 1632. Learning Rate: 0.000125 Total Training Loss:  1.9364075817284174\n",
      "Epoch 1633. Learning Rate: 0.000125 Total Training Loss:  1.9369165493990295\n",
      "Epoch 1634. Learning Rate: 0.000125 Total Training Loss:  1.9364848845289089\n",
      "Epoch 1635. Learning Rate: 0.000125 Total Training Loss:  1.9355929054145236\n",
      "Epoch 1636. Learning Rate: 0.000125 Total Training Loss:  1.9360928595415317\n",
      "Epoch 1637. Learning Rate: 0.000125 Total Training Loss:  1.9355287833022885\n",
      "Epoch 1638. Learning Rate: 0.000125 Total Training Loss:  1.935739739361452\n",
      "Epoch 1639. Learning Rate: 0.000125 Total Training Loss:  1.9357320336275734\n",
      "Epoch 1640. Learning Rate: 0.000125 Total Training Loss:  1.9348358547722455\n",
      "Epoch 1641. Learning Rate: 0.000125 Total Training Loss:  1.9355249225045554\n",
      "Epoch 1642. Learning Rate: 0.000125 Total Training Loss:  1.9345552466693334\n",
      "Epoch 1643. Learning Rate: 0.000125 Total Training Loss:  1.9368923925503623\n",
      "Epoch 1644. Learning Rate: 0.000125 Total Training Loss:  1.9338757829682436\n",
      "Epoch 1645. Learning Rate: 0.000125 Total Training Loss:  1.9364738072326872\n",
      "Epoch 1646. Learning Rate: 0.000125 Total Training Loss:  1.9344666739052627\n",
      "Epoch 1647. Learning Rate: 0.000125 Total Training Loss:  1.9328990938083734\n",
      "Epoch 1648. Learning Rate: 0.000125 Total Training Loss:  1.935850716981804\n",
      "Epoch 1649. Learning Rate: 0.000125 Total Training Loss:  1.9346303219499532\n",
      "Epoch 1650. Learning Rate: 0.000125 Total Training Loss:  1.9327977515058592\n",
      "Epoch 1651. Learning Rate: 0.000125 Total Training Loss:  1.9364874006423634\n",
      "Epoch 1652. Learning Rate: 0.000125 Total Training Loss:  1.9335308838053606\n",
      "Epoch 1653. Learning Rate: 0.000125 Total Training Loss:  1.9345282480935566\n",
      "Epoch 1654. Learning Rate: 0.000125 Total Training Loss:  1.932018076768145\n",
      "Epoch 1655. Learning Rate: 0.000125 Total Training Loss:  1.934969061636366\n",
      "Epoch 1656. Learning Rate: 0.000125 Total Training Loss:  1.9330008475808427\n",
      "Epoch 1657. Learning Rate: 0.000125 Total Training Loss:  1.9336531073204242\n",
      "Epoch 1658. Learning Rate: 0.000125 Total Training Loss:  1.9341980517783668\n",
      "Epoch 1659. Learning Rate: 0.000125 Total Training Loss:  1.9337580967403483\n",
      "Epoch 1660. Learning Rate: 0.000125 Total Training Loss:  1.933657316287281\n",
      "Epoch 1661. Learning Rate: 0.000125 Total Training Loss:  1.9324528199213091\n",
      "Epoch 1662. Learning Rate: 0.000125 Total Training Loss:  1.935065954865422\n",
      "Epoch 1663. Learning Rate: 0.000125 Total Training Loss:  1.932924505090341\n",
      "Epoch 1664. Learning Rate: 0.000125 Total Training Loss:  1.9334543179138564\n",
      "Epoch 1665. Learning Rate: 0.000125 Total Training Loss:  1.9320464316697326\n",
      "Epoch 1666. Learning Rate: 0.000125 Total Training Loss:  1.9327915390895214\n",
      "Epoch 1667. Learning Rate: 0.000125 Total Training Loss:  1.9330223816505168\n",
      "Epoch 1668. Learning Rate: 0.000125 Total Training Loss:  1.9317377936386038\n",
      "Epoch 1669. Learning Rate: 0.000125 Total Training Loss:  1.9328420492820442\n",
      "Epoch 1670. Learning Rate: 0.000125 Total Training Loss:  1.932741902419366\n",
      "Epoch 1671. Learning Rate: 0.000125 Total Training Loss:  1.9314287445740774\n",
      "Epoch 1672. Learning Rate: 0.000125 Total Training Loss:  1.9314790885255206\n",
      "Epoch 1673. Learning Rate: 0.000125 Total Training Loss:  1.9327585360442754\n",
      "Epoch 1674. Learning Rate: 0.000125 Total Training Loss:  1.9323953516432084\n",
      "Epoch 1675. Learning Rate: 0.000125 Total Training Loss:  1.9307490467908792\n",
      "Epoch 1676. Learning Rate: 0.000125 Total Training Loss:  1.9310588586959057\n",
      "Epoch 1677. Learning Rate: 0.000125 Total Training Loss:  1.9316867269517388\n",
      "Epoch 1678. Learning Rate: 0.000125 Total Training Loss:  1.9320472438412253\n",
      "Epoch 1679. Learning Rate: 0.000125 Total Training Loss:  1.930491353443358\n",
      "Epoch 1680. Learning Rate: 0.000125 Total Training Loss:  1.9320337651006412\n",
      "Epoch 1681. Learning Rate: 0.000125 Total Training Loss:  1.9290602545952424\n",
      "Epoch 1682. Learning Rate: 0.000125 Total Training Loss:  1.9322607746871654\n",
      "Epoch 1683. Learning Rate: 0.000125 Total Training Loss:  1.9299690245534293\n",
      "Epoch 1684. Learning Rate: 0.000125 Total Training Loss:  1.931086791533744\n",
      "Epoch 1685. Learning Rate: 0.000125 Total Training Loss:  1.92935084894998\n",
      "Epoch 1686. Learning Rate: 0.000125 Total Training Loss:  1.9302425495407078\n",
      "Epoch 1687. Learning Rate: 0.000125 Total Training Loss:  1.930218187480932\n",
      "Epoch 1688. Learning Rate: 0.000125 Total Training Loss:  1.9301473389787134\n",
      "Epoch 1689. Learning Rate: 0.000125 Total Training Loss:  1.929131622047862\n",
      "Epoch 1690. Learning Rate: 0.000125 Total Training Loss:  1.9300343680661172\n",
      "Epoch 1691. Learning Rate: 0.000125 Total Training Loss:  1.929616442444967\n",
      "Epoch 1692. Learning Rate: 0.000125 Total Training Loss:  1.9303737714071758\n",
      "Epoch 1693. Learning Rate: 0.000125 Total Training Loss:  1.9289162954082713\n",
      "Epoch 1694. Learning Rate: 0.000125 Total Training Loss:  1.9285062709532212\n",
      "Epoch 1695. Learning Rate: 0.000125 Total Training Loss:  1.929883276403416\n",
      "Epoch 1696. Learning Rate: 0.000125 Total Training Loss:  1.9283056164567824\n",
      "Epoch 1697. Learning Rate: 0.000125 Total Training Loss:  1.929795836971607\n",
      "Epoch 1698. Learning Rate: 0.000125 Total Training Loss:  1.9288599067658652\n",
      "Epoch 1699. Learning Rate: 0.000125 Total Training Loss:  1.9276701386552304\n",
      "Epoch 1700. Learning Rate: 0.000125 Total Training Loss:  1.930178068956593\n",
      "Epoch 1701. Learning Rate: 0.000125 Total Training Loss:  1.9269407037645578\n",
      "Epoch 1702. Learning Rate: 0.000125 Total Training Loss:  1.9290735640388448\n",
      "Epoch 1703. Learning Rate: 0.000125 Total Training Loss:  1.927890743361786\n",
      "Epoch 1704. Learning Rate: 0.000125 Total Training Loss:  1.9270126529154368\n",
      "Epoch 1705. Learning Rate: 0.000125 Total Training Loss:  1.9288748277758714\n",
      "Epoch 1706. Learning Rate: 0.000125 Total Training Loss:  1.9279508139879908\n",
      "Epoch 1707. Learning Rate: 0.000125 Total Training Loss:  1.9271319246909115\n",
      "Epoch 1708. Learning Rate: 0.000125 Total Training Loss:  1.927791831229115\n",
      "Epoch 1709. Learning Rate: 0.000125 Total Training Loss:  1.9260931737953797\n",
      "Epoch 1710. Learning Rate: 0.000125 Total Training Loss:  1.9290164581034333\n",
      "Epoch 1711. Learning Rate: 0.000125 Total Training Loss:  1.9265774667437654\n",
      "Epoch 1712. Learning Rate: 0.000125 Total Training Loss:  1.9276490682386793\n",
      "Epoch 1713. Learning Rate: 0.000125 Total Training Loss:  1.925122168730013\n",
      "Epoch 1714. Learning Rate: 0.000125 Total Training Loss:  1.927652483980637\n",
      "Epoch 1715. Learning Rate: 0.000125 Total Training Loss:  1.925640548375668\n",
      "Epoch 1716. Learning Rate: 0.000125 Total Training Loss:  1.928962640260579\n",
      "Epoch 1717. Learning Rate: 0.000125 Total Training Loss:  1.9251829638669733\n",
      "Epoch 1718. Learning Rate: 0.000125 Total Training Loss:  1.9268487297231331\n",
      "Epoch 1719. Learning Rate: 0.000125 Total Training Loss:  1.9243048685893882\n",
      "Epoch 1720. Learning Rate: 0.000125 Total Training Loss:  1.9266730455565266\n",
      "Epoch 1721. Learning Rate: 0.000125 Total Training Loss:  1.92498386092484\n",
      "Epoch 1722. Learning Rate: 0.000125 Total Training Loss:  1.9264188335218932\n",
      "Epoch 1723. Learning Rate: 0.000125 Total Training Loss:  1.924302441708278\n",
      "Epoch 1724. Learning Rate: 0.000125 Total Training Loss:  1.9258803603006527\n",
      "Epoch 1725. Learning Rate: 0.000125 Total Training Loss:  1.924496583553264\n",
      "Epoch 1726. Learning Rate: 0.000125 Total Training Loss:  1.9251487599103712\n",
      "Epoch 1727. Learning Rate: 0.000125 Total Training Loss:  1.924984185170615\n",
      "Epoch 1728. Learning Rate: 0.000125 Total Training Loss:  1.9240416068641935\n",
      "Epoch 1729. Learning Rate: 0.000125 Total Training Loss:  1.9247488606488332\n",
      "Epoch 1730. Learning Rate: 0.000125 Total Training Loss:  1.924071741726948\n",
      "Epoch 1731. Learning Rate: 0.000125 Total Training Loss:  1.923734880896518\n",
      "Epoch 1732. Learning Rate: 0.000125 Total Training Loss:  1.9240781642438378\n",
      "Epoch 1733. Learning Rate: 0.000125 Total Training Loss:  1.9240569897228852\n",
      "Epoch 1734. Learning Rate: 0.000125 Total Training Loss:  1.9237244338728487\n",
      "Epoch 1735. Learning Rate: 0.000125 Total Training Loss:  1.923257993592415\n",
      "Epoch 1736. Learning Rate: 0.000125 Total Training Loss:  1.9219609140127432\n",
      "Epoch 1737. Learning Rate: 0.000125 Total Training Loss:  1.9239652053220198\n",
      "Epoch 1738. Learning Rate: 0.000125 Total Training Loss:  1.9231563593493775\n",
      "Epoch 1739. Learning Rate: 0.000125 Total Training Loss:  1.9232085891999304\n",
      "Epoch 1740. Learning Rate: 0.000125 Total Training Loss:  1.9228123488719575\n",
      "Epoch 1741. Learning Rate: 0.000125 Total Training Loss:  1.9210281630221289\n",
      "Epoch 1742. Learning Rate: 0.000125 Total Training Loss:  1.9233670681423973\n",
      "Epoch 1743. Learning Rate: 0.000125 Total Training Loss:  1.9213219741359353\n",
      "Epoch 1744. Learning Rate: 0.000125 Total Training Loss:  1.9249227879045065\n",
      "Epoch 1745. Learning Rate: 0.000125 Total Training Loss:  1.9219336487585679\n",
      "Epoch 1746. Learning Rate: 0.000125 Total Training Loss:  1.9229651800123975\n",
      "Epoch 1747. Learning Rate: 0.000125 Total Training Loss:  1.9228197687189095\n",
      "Epoch 1748. Learning Rate: 0.000125 Total Training Loss:  1.922110368963331\n",
      "Epoch 1749. Learning Rate: 0.000125 Total Training Loss:  1.9206599027384073\n",
      "Epoch 1750. Learning Rate: 0.000125 Total Training Loss:  1.9216381218284369\n",
      "Epoch 1751. Learning Rate: 0.000125 Total Training Loss:  1.9206181722984184\n",
      "Epoch 1752. Learning Rate: 0.000125 Total Training Loss:  1.921453674294753\n",
      "Epoch 1753. Learning Rate: 0.000125 Total Training Loss:  1.9203588181117084\n",
      "Epoch 1754. Learning Rate: 0.000125 Total Training Loss:  1.9228754623618443\n",
      "Epoch 1755. Learning Rate: 0.000125 Total Training Loss:  1.9197871498763561\n",
      "Epoch 1756. Learning Rate: 0.000125 Total Training Loss:  1.9219183468085248\n",
      "Epoch 1757. Learning Rate: 0.000125 Total Training Loss:  1.9202882295066956\n",
      "Epoch 1758. Learning Rate: 0.000125 Total Training Loss:  1.9215563875914086\n",
      "Epoch 1759. Learning Rate: 0.000125 Total Training Loss:  1.9197461085568648\n",
      "Epoch 1760. Learning Rate: 0.000125 Total Training Loss:  1.921795474132523\n",
      "Epoch 1761. Learning Rate: 0.000125 Total Training Loss:  1.9183904486999381\n",
      "Epoch 1762. Learning Rate: 0.000125 Total Training Loss:  1.9214197408873588\n",
      "Epoch 1763. Learning Rate: 0.000125 Total Training Loss:  1.9193093763897195\n",
      "Epoch 1764. Learning Rate: 0.000125 Total Training Loss:  1.9198371802922338\n",
      "Epoch 1765. Learning Rate: 0.000125 Total Training Loss:  1.9196511662448756\n",
      "Epoch 1766. Learning Rate: 0.000125 Total Training Loss:  1.9197699096403085\n",
      "Epoch 1767. Learning Rate: 0.000125 Total Training Loss:  1.9187859781086445\n",
      "Epoch 1768. Learning Rate: 0.000125 Total Training Loss:  1.9205009330471512\n",
      "Epoch 1769. Learning Rate: 0.000125 Total Training Loss:  1.918034273054218\n",
      "Epoch 1770. Learning Rate: 0.000125 Total Training Loss:  1.920467921620002\n",
      "Epoch 1771. Learning Rate: 0.000125 Total Training Loss:  1.9184918294486124\n",
      "Epoch 1772. Learning Rate: 0.000125 Total Training Loss:  1.920417255780194\n",
      "Epoch 1773. Learning Rate: 0.000125 Total Training Loss:  1.917985644977307\n",
      "Epoch 1774. Learning Rate: 0.000125 Total Training Loss:  1.9196374740276951\n",
      "Epoch 1775. Learning Rate: 0.000125 Total Training Loss:  1.9180138265655842\n",
      "Epoch 1776. Learning Rate: 0.000125 Total Training Loss:  1.9195985347614624\n",
      "Epoch 1777. Learning Rate: 0.000125 Total Training Loss:  1.9181201098836027\n",
      "Epoch 1778. Learning Rate: 0.000125 Total Training Loss:  1.9179896396235563\n",
      "Epoch 1779. Learning Rate: 0.000125 Total Training Loss:  1.917037659441121\n",
      "Epoch 1780. Learning Rate: 0.000125 Total Training Loss:  1.917409021552885\n",
      "Epoch 1781. Learning Rate: 0.000125 Total Training Loss:  1.9167730619665235\n",
      "Epoch 1782. Learning Rate: 0.000125 Total Training Loss:  1.9173353042569943\n",
      "Epoch 1783. Learning Rate: 0.000125 Total Training Loss:  1.91759183947579\n",
      "Epoch 1784. Learning Rate: 0.000125 Total Training Loss:  1.916764061403228\n",
      "Epoch 1785. Learning Rate: 0.000125 Total Training Loss:  1.9172662437777035\n",
      "Epoch 1786. Learning Rate: 0.000125 Total Training Loss:  1.91575214479235\n",
      "Epoch 1787. Learning Rate: 0.000125 Total Training Loss:  1.9183946627890691\n",
      "Epoch 1788. Learning Rate: 0.000125 Total Training Loss:  1.9158452298725024\n",
      "Epoch 1789. Learning Rate: 0.000125 Total Training Loss:  1.9166611819237005\n",
      "Epoch 1790. Learning Rate: 0.000125 Total Training Loss:  1.917822968098335\n",
      "Epoch 1791. Learning Rate: 0.000125 Total Training Loss:  1.9166592414549086\n",
      "Epoch 1792. Learning Rate: 0.000125 Total Training Loss:  1.9168607396422885\n",
      "Epoch 1793. Learning Rate: 0.000125 Total Training Loss:  1.9160450906201731\n",
      "Epoch 1794. Learning Rate: 0.000125 Total Training Loss:  1.9170004636107478\n",
      "Epoch 1795. Learning Rate: 0.000125 Total Training Loss:  1.9156635534600355\n",
      "Epoch 1796. Learning Rate: 0.000125 Total Training Loss:  1.9167827078490518\n",
      "Epoch 1797. Learning Rate: 0.000125 Total Training Loss:  1.9161359236459248\n",
      "Epoch 1798. Learning Rate: 0.000125 Total Training Loss:  1.9166216975427233\n",
      "Epoch 1799. Learning Rate: 0.000125 Total Training Loss:  1.914234871568624\n",
      "Epoch 1800. Learning Rate: 0.000125 Total Training Loss:  1.9172757396590896\n",
      "Epoch 1801. Learning Rate: 0.000125 Total Training Loss:  1.9153344563674182\n",
      "Epoch 1802. Learning Rate: 0.000125 Total Training Loss:  1.9147931514598895\n",
      "Epoch 1803. Learning Rate: 0.000125 Total Training Loss:  1.9155206336581614\n",
      "Epoch 1804. Learning Rate: 0.000125 Total Training Loss:  1.9150263346964493\n",
      "Epoch 1805. Learning Rate: 0.000125 Total Training Loss:  1.9144119613338262\n",
      "Epoch 1806. Learning Rate: 0.000125 Total Training Loss:  1.9155853903503157\n",
      "Epoch 1807. Learning Rate: 0.000125 Total Training Loss:  1.9145547334919684\n",
      "Epoch 1808. Learning Rate: 0.000125 Total Training Loss:  1.9141982031869702\n",
      "Epoch 1809. Learning Rate: 0.000125 Total Training Loss:  1.9136008212517481\n",
      "Epoch 1810. Learning Rate: 0.000125 Total Training Loss:  1.9140938402852044\n",
      "Epoch 1811. Learning Rate: 0.000125 Total Training Loss:  1.9142281771055423\n",
      "Epoch 1812. Learning Rate: 0.000125 Total Training Loss:  1.913478730712086\n",
      "Epoch 1813. Learning Rate: 0.000125 Total Training Loss:  1.9153924545098562\n",
      "Epoch 1814. Learning Rate: 0.000125 Total Training Loss:  1.912910621438641\n",
      "Epoch 1815. Learning Rate: 0.000125 Total Training Loss:  1.9140131626045331\n",
      "Epoch 1816. Learning Rate: 0.000125 Total Training Loss:  1.9131037561164703\n",
      "Epoch 1817. Learning Rate: 0.000125 Total Training Loss:  1.9128896793117747\n",
      "Epoch 1818. Learning Rate: 0.000125 Total Training Loss:  1.9126545643084683\n",
      "Epoch 1819. Learning Rate: 0.000125 Total Training Loss:  1.9138240581960417\n",
      "Epoch 1820. Learning Rate: 0.000125 Total Training Loss:  1.912522518367041\n",
      "Epoch 1821. Learning Rate: 0.000125 Total Training Loss:  1.911914269614499\n",
      "Epoch 1822. Learning Rate: 0.000125 Total Training Loss:  1.911507619312033\n",
      "Epoch 1823. Learning Rate: 0.000125 Total Training Loss:  1.9131169542088173\n",
      "Epoch 1824. Learning Rate: 0.000125 Total Training Loss:  1.9113666628254578\n",
      "Epoch 1825. Learning Rate: 0.000125 Total Training Loss:  1.9122674435784575\n",
      "Epoch 1826. Learning Rate: 0.000125 Total Training Loss:  1.9104783206421416\n",
      "Epoch 1827. Learning Rate: 0.000125 Total Training Loss:  1.9135480393888429\n",
      "Epoch 1828. Learning Rate: 0.000125 Total Training Loss:  1.9102243964443915\n",
      "Epoch 1829. Learning Rate: 0.000125 Total Training Loss:  1.9110479429655243\n",
      "Epoch 1830. Learning Rate: 0.000125 Total Training Loss:  1.9111671251885127\n",
      "Epoch 1831. Learning Rate: 0.000125 Total Training Loss:  1.910952184087364\n",
      "Epoch 1832. Learning Rate: 0.000125 Total Training Loss:  1.911441869015107\n",
      "Epoch 1833. Learning Rate: 0.000125 Total Training Loss:  1.910928476776462\n",
      "Epoch 1834. Learning Rate: 0.000125 Total Training Loss:  1.9105669001874048\n",
      "Epoch 1835. Learning Rate: 0.000125 Total Training Loss:  1.9096084607299417\n",
      "Epoch 1836. Learning Rate: 0.000125 Total Training Loss:  1.911568594747223\n",
      "Epoch 1837. Learning Rate: 0.000125 Total Training Loss:  1.909021318599116\n",
      "Epoch 1838. Learning Rate: 0.000125 Total Training Loss:  1.9114066410111263\n",
      "Epoch 1839. Learning Rate: 0.000125 Total Training Loss:  1.908573761495063\n",
      "Epoch 1840. Learning Rate: 0.000125 Total Training Loss:  1.910591909661889\n",
      "Epoch 1841. Learning Rate: 0.000125 Total Training Loss:  1.9088541636592709\n",
      "Epoch 1842. Learning Rate: 0.000125 Total Training Loss:  1.9110493143380154\n",
      "Epoch 1843. Learning Rate: 0.000125 Total Training Loss:  1.9079265468753874\n",
      "Epoch 1844. Learning Rate: 0.000125 Total Training Loss:  1.9101559927512426\n",
      "Epoch 1845. Learning Rate: 0.000125 Total Training Loss:  1.9084916186111514\n",
      "Epoch 1846. Learning Rate: 0.000125 Total Training Loss:  1.9088075351610314\n",
      "Epoch 1847. Learning Rate: 0.000125 Total Training Loss:  1.9082298279972747\n",
      "Epoch 1848. Learning Rate: 0.000125 Total Training Loss:  1.9089212099206634\n",
      "Epoch 1849. Learning Rate: 0.000125 Total Training Loss:  1.9075653835898265\n",
      "Epoch 1850. Learning Rate: 0.000125 Total Training Loss:  1.9084363150468562\n",
      "Epoch 1851. Learning Rate: 0.000125 Total Training Loss:  1.908974095073063\n",
      "Epoch 1852. Learning Rate: 0.000125 Total Training Loss:  1.9075051450054161\n",
      "Epoch 1853. Learning Rate: 0.000125 Total Training Loss:  1.9096171291312203\n",
      "Epoch 1854. Learning Rate: 0.000125 Total Training Loss:  1.9082540162780788\n",
      "Epoch 1855. Learning Rate: 0.000125 Total Training Loss:  1.907178432069486\n",
      "Epoch 1856. Learning Rate: 0.000125 Total Training Loss:  1.9077506099129096\n",
      "Epoch 1857. Learning Rate: 0.000125 Total Training Loss:  1.9080753417511005\n",
      "Epoch 1858. Learning Rate: 0.000125 Total Training Loss:  1.9077618699811865\n",
      "Epoch 1859. Learning Rate: 0.000125 Total Training Loss:  1.907310673239408\n",
      "Epoch 1860. Learning Rate: 0.000125 Total Training Loss:  1.906409851362696\n",
      "Epoch 1861. Learning Rate: 0.000125 Total Training Loss:  1.9073902989621274\n",
      "Epoch 1862. Learning Rate: 0.000125 Total Training Loss:  1.906249143066816\n",
      "Epoch 1863. Learning Rate: 0.000125 Total Training Loss:  1.907123202632647\n",
      "Epoch 1864. Learning Rate: 0.000125 Total Training Loss:  1.9056111911486369\n",
      "Epoch 1865. Learning Rate: 0.000125 Total Training Loss:  1.9070651222427841\n",
      "Epoch 1866. Learning Rate: 0.000125 Total Training Loss:  1.906251223932486\n",
      "Epoch 1867. Learning Rate: 0.000125 Total Training Loss:  1.907372060639318\n",
      "Epoch 1868. Learning Rate: 0.000125 Total Training Loss:  1.906721643084893\n",
      "Epoch 1869. Learning Rate: 0.000125 Total Training Loss:  1.9051023984211497\n",
      "Epoch 1870. Learning Rate: 0.000125 Total Training Loss:  1.9066747070173733\n",
      "Epoch 1871. Learning Rate: 0.000125 Total Training Loss:  1.9046797740156762\n",
      "Epoch 1872. Learning Rate: 0.000125 Total Training Loss:  1.9061493094777688\n",
      "Epoch 1873. Learning Rate: 0.000125 Total Training Loss:  1.9053352661721874\n",
      "Epoch 1874. Learning Rate: 0.000125 Total Training Loss:  1.905359839060111\n",
      "Epoch 1875. Learning Rate: 0.000125 Total Training Loss:  1.9054676721862052\n",
      "Epoch 1876. Learning Rate: 0.000125 Total Training Loss:  1.9051364579645451\n",
      "Epoch 1877. Learning Rate: 0.000125 Total Training Loss:  1.9049359352211468\n",
      "Epoch 1878. Learning Rate: 0.000125 Total Training Loss:  1.9048842180636711\n",
      "Epoch 1879. Learning Rate: 0.000125 Total Training Loss:  1.9046271525439806\n",
      "Epoch 1880. Learning Rate: 0.000125 Total Training Loss:  1.9053903187159449\n",
      "Epoch 1881. Learning Rate: 0.000125 Total Training Loss:  1.9033347306831274\n",
      "Epoch 1882. Learning Rate: 0.000125 Total Training Loss:  1.9058886829880066\n",
      "Epoch 1883. Learning Rate: 0.000125 Total Training Loss:  1.9051273394434247\n",
      "Epoch 1884. Learning Rate: 0.000125 Total Training Loss:  1.9048047978139948\n",
      "Epoch 1885. Learning Rate: 0.000125 Total Training Loss:  1.9030273836688139\n",
      "Epoch 1886. Learning Rate: 0.000125 Total Training Loss:  1.9047261509113014\n",
      "Epoch 1887. Learning Rate: 0.000125 Total Training Loss:  1.9032890377566218\n",
      "Epoch 1888. Learning Rate: 0.000125 Total Training Loss:  1.9056089696241543\n",
      "Epoch 1889. Learning Rate: 0.000125 Total Training Loss:  1.9014982353546657\n",
      "Epoch 1890. Learning Rate: 0.000125 Total Training Loss:  1.9054228661989328\n",
      "Epoch 1891. Learning Rate: 0.000125 Total Training Loss:  1.9027672454540152\n",
      "Epoch 1892. Learning Rate: 0.000125 Total Training Loss:  1.905101799289696\n",
      "Epoch 1893. Learning Rate: 0.000125 Total Training Loss:  1.902369294577511\n",
      "Epoch 1894. Learning Rate: 0.000125 Total Training Loss:  1.9031072770885658\n",
      "Epoch 1895. Learning Rate: 0.000125 Total Training Loss:  1.90253457397921\n",
      "Epoch 1896. Learning Rate: 0.000125 Total Training Loss:  1.9038260653906036\n",
      "Epoch 1897. Learning Rate: 0.000125 Total Training Loss:  1.9007889901986346\n",
      "Epoch 1898. Learning Rate: 0.000125 Total Training Loss:  1.903497946972493\n",
      "Epoch 1899. Learning Rate: 0.000125 Total Training Loss:  1.9015621927101165\n",
      "Epoch 1900. Learning Rate: 0.000125 Total Training Loss:  1.904448246321408\n",
      "Epoch 1901. Learning Rate: 0.000125 Total Training Loss:  1.9018034127657302\n",
      "Epoch 1902. Learning Rate: 0.000125 Total Training Loss:  1.9022260389174335\n",
      "Epoch 1903. Learning Rate: 0.000125 Total Training Loss:  1.901728690660093\n",
      "Epoch 1904. Learning Rate: 0.000125 Total Training Loss:  1.9014563093951438\n",
      "Epoch 1905. Learning Rate: 0.000125 Total Training Loss:  1.9015887072018813\n",
      "Epoch 1906. Learning Rate: 0.000125 Total Training Loss:  1.9032557578466367\n",
      "Epoch 1907. Learning Rate: 0.000125 Total Training Loss:  1.9014346603944432\n",
      "Epoch 1908. Learning Rate: 0.000125 Total Training Loss:  1.9009019761288073\n",
      "Epoch 1909. Learning Rate: 0.000125 Total Training Loss:  1.90144540273468\n",
      "Epoch 1910. Learning Rate: 0.000125 Total Training Loss:  1.9015284932975192\n",
      "Epoch 1911. Learning Rate: 0.000125 Total Training Loss:  1.9002335160621442\n",
      "Epoch 1912. Learning Rate: 0.000125 Total Training Loss:  1.9026039391756058\n",
      "Epoch 1913. Learning Rate: 0.000125 Total Training Loss:  1.8995500066084787\n",
      "Epoch 1914. Learning Rate: 0.000125 Total Training Loss:  1.9011957232141867\n",
      "Epoch 1915. Learning Rate: 0.000125 Total Training Loss:  1.9006403340608813\n",
      "Epoch 1916. Learning Rate: 0.000125 Total Training Loss:  1.9010032405203674\n",
      "Epoch 1917. Learning Rate: 0.000125 Total Training Loss:  1.898965971544385\n",
      "Epoch 1918. Learning Rate: 0.000125 Total Training Loss:  1.901456766150659\n",
      "Epoch 1919. Learning Rate: 0.000125 Total Training Loss:  1.8999112326127943\n",
      "Epoch 1920. Learning Rate: 0.000125 Total Training Loss:  1.8998340615362395\n",
      "Epoch 1921. Learning Rate: 0.000125 Total Training Loss:  1.8991587798518594\n",
      "Epoch 1922. Learning Rate: 0.000125 Total Training Loss:  1.901394179178169\n",
      "Epoch 1923. Learning Rate: 0.000125 Total Training Loss:  1.8985642752086278\n",
      "Epoch 1924. Learning Rate: 0.000125 Total Training Loss:  1.9000471675826702\n",
      "Epoch 1925. Learning Rate: 0.000125 Total Training Loss:  1.8988429109740537\n",
      "Epoch 1926. Learning Rate: 0.000125 Total Training Loss:  1.8993184851715341\n",
      "Epoch 1927. Learning Rate: 0.000125 Total Training Loss:  1.8998912386014126\n",
      "Epoch 1928. Learning Rate: 0.000125 Total Training Loss:  1.899731446930673\n",
      "Epoch 1929. Learning Rate: 0.000125 Total Training Loss:  1.8983433307439554\n",
      "Epoch 1930. Learning Rate: 0.000125 Total Training Loss:  1.899186397989979\n",
      "Epoch 1931. Learning Rate: 0.000125 Total Training Loss:  1.8991352833108976\n",
      "Epoch 1932. Learning Rate: 0.000125 Total Training Loss:  1.8982657461601775\n",
      "Epoch 1933. Learning Rate: 0.000125 Total Training Loss:  1.8993242265714798\n",
      "Epoch 1934. Learning Rate: 0.000125 Total Training Loss:  1.898805080621969\n",
      "Epoch 1935. Learning Rate: 0.000125 Total Training Loss:  1.8979837793740444\n",
      "Epoch 1936. Learning Rate: 0.000125 Total Training Loss:  1.9002689768676646\n",
      "Epoch 1937. Learning Rate: 0.000125 Total Training Loss:  1.8970865717856213\n",
      "Epoch 1938. Learning Rate: 0.000125 Total Training Loss:  1.8988050225307234\n",
      "Epoch 1939. Learning Rate: 0.000125 Total Training Loss:  1.8977016345015727\n",
      "Epoch 1940. Learning Rate: 0.000125 Total Training Loss:  1.8985820843372494\n",
      "Epoch 1941. Learning Rate: 0.000125 Total Training Loss:  1.8975005157408305\n",
      "Epoch 1942. Learning Rate: 0.000125 Total Training Loss:  1.8979535542603116\n",
      "Epoch 1943. Learning Rate: 0.000125 Total Training Loss:  1.8965934317093343\n",
      "Epoch 1944. Learning Rate: 0.000125 Total Training Loss:  1.8981281155138277\n",
      "Epoch 1945. Learning Rate: 0.000125 Total Training Loss:  1.8971589345892426\n",
      "Epoch 1946. Learning Rate: 0.000125 Total Training Loss:  1.898448133142665\n",
      "Epoch 1947. Learning Rate: 0.000125 Total Training Loss:  1.8963545764563605\n",
      "Epoch 1948. Learning Rate: 0.000125 Total Training Loss:  1.8966798244509846\n",
      "Epoch 1949. Learning Rate: 0.000125 Total Training Loss:  1.8965346292825416\n",
      "Epoch 1950. Learning Rate: 0.000125 Total Training Loss:  1.8975915708579123\n",
      "Epoch 1951. Learning Rate: 0.000125 Total Training Loss:  1.8964216274616774\n",
      "Epoch 1952. Learning Rate: 0.000125 Total Training Loss:  1.8974484672071412\n",
      "Epoch 1953. Learning Rate: 0.000125 Total Training Loss:  1.8956965953111649\n",
      "Epoch 1954. Learning Rate: 0.000125 Total Training Loss:  1.896431577304611\n",
      "Epoch 1955. Learning Rate: 0.000125 Total Training Loss:  1.8965938013861887\n",
      "Epoch 1956. Learning Rate: 0.000125 Total Training Loss:  1.8969931828905828\n",
      "Epoch 1957. Learning Rate: 0.000125 Total Training Loss:  1.896108778368216\n",
      "Epoch 1958. Learning Rate: 0.000125 Total Training Loss:  1.8955775295908097\n",
      "Epoch 1959. Learning Rate: 0.000125 Total Training Loss:  1.8957520623225719\n",
      "Epoch 1960. Learning Rate: 0.000125 Total Training Loss:  1.8961656637839042\n",
      "Epoch 1961. Learning Rate: 0.000125 Total Training Loss:  1.8961564226192422\n",
      "Epoch 1962. Learning Rate: 0.000125 Total Training Loss:  1.895482062769588\n",
      "Epoch 1963. Learning Rate: 0.000125 Total Training Loss:  1.8948498971294612\n",
      "Epoch 1964. Learning Rate: 0.000125 Total Training Loss:  1.8955439963610843\n",
      "Epoch 1965. Learning Rate: 0.000125 Total Training Loss:  1.8958017002732959\n",
      "Epoch 1966. Learning Rate: 0.000125 Total Training Loss:  1.8959821517055389\n",
      "Epoch 1967. Learning Rate: 0.000125 Total Training Loss:  1.8953473923902493\n",
      "Epoch 1968. Learning Rate: 0.000125 Total Training Loss:  1.8937726309522986\n",
      "Epoch 1969. Learning Rate: 0.000125 Total Training Loss:  1.8954944729339331\n",
      "Epoch 1970. Learning Rate: 0.000125 Total Training Loss:  1.8944841250195168\n",
      "Epoch 1971. Learning Rate: 0.000125 Total Training Loss:  1.8940411761286668\n",
      "Epoch 1972. Learning Rate: 0.000125 Total Training Loss:  1.8948692061239854\n",
      "Epoch 1973. Learning Rate: 0.000125 Total Training Loss:  1.8952582312049344\n",
      "Epoch 1974. Learning Rate: 0.000125 Total Training Loss:  1.8945612098323181\n",
      "Epoch 1975. Learning Rate: 0.000125 Total Training Loss:  1.8947195736982394\n",
      "Epoch 1976. Learning Rate: 0.000125 Total Training Loss:  1.8944417884922586\n",
      "Epoch 1977. Learning Rate: 0.000125 Total Training Loss:  1.8945602224848699\n",
      "Epoch 1978. Learning Rate: 0.000125 Total Training Loss:  1.8936611509416252\n",
      "Epoch 1979. Learning Rate: 0.000125 Total Training Loss:  1.895016174588818\n",
      "Epoch 1980. Learning Rate: 0.000125 Total Training Loss:  1.8932502102979925\n",
      "Epoch 1981. Learning Rate: 0.000125 Total Training Loss:  1.893864784768084\n",
      "Epoch 1982. Learning Rate: 0.000125 Total Training Loss:  1.8936763143865392\n",
      "Epoch 1983. Learning Rate: 0.000125 Total Training Loss:  1.8926122779375874\n",
      "Epoch 1984. Learning Rate: 0.000125 Total Training Loss:  1.8931985925883055\n",
      "Epoch 1985. Learning Rate: 0.000125 Total Training Loss:  1.893768058536807\n",
      "Epoch 1986. Learning Rate: 0.000125 Total Training Loss:  1.8924322202219628\n",
      "Epoch 1987. Learning Rate: 0.000125 Total Training Loss:  1.8932532630860806\n",
      "Epoch 1988. Learning Rate: 0.000125 Total Training Loss:  1.8929956197389401\n",
      "Epoch 1989. Learning Rate: 0.000125 Total Training Loss:  1.8927092268131673\n",
      "Epoch 1990. Learning Rate: 0.000125 Total Training Loss:  1.8932049332070164\n",
      "Epoch 1991. Learning Rate: 0.000125 Total Training Loss:  1.8912691363075282\n",
      "Epoch 1992. Learning Rate: 0.000125 Total Training Loss:  1.8924167232471518\n",
      "Epoch 1993. Learning Rate: 0.000125 Total Training Loss:  1.8927571525564417\n",
      "Epoch 1994. Learning Rate: 0.000125 Total Training Loss:  1.8935212943761144\n",
      "Epoch 1995. Learning Rate: 0.000125 Total Training Loss:  1.8913989158754703\n",
      "Epoch 1996. Learning Rate: 0.000125 Total Training Loss:  1.8928143269731663\n",
      "Epoch 1997. Learning Rate: 0.000125 Total Training Loss:  1.8903190483397339\n",
      "Epoch 1998. Learning Rate: 0.000125 Total Training Loss:  1.8927824903512374\n",
      "Epoch 1999. Learning Rate: 0.000125 Total Training Loss:  1.8917390228016302\n",
      "Epoch 2000. Learning Rate: 6.25e-05 Total Training Loss:  1.891233915637713\n",
      "Epoch 2001. Learning Rate: 6.25e-05 Total Training Loss:  1.8782738725130912\n",
      "Epoch 2002. Learning Rate: 6.25e-05 Total Training Loss:  1.8695701501856092\n",
      "Epoch 2003. Learning Rate: 6.25e-05 Total Training Loss:  1.869328313798178\n",
      "Epoch 2004. Learning Rate: 6.25e-05 Total Training Loss:  1.8680699050892144\n",
      "Epoch 2005. Learning Rate: 6.25e-05 Total Training Loss:  1.8688476640090812\n",
      "Epoch 2006. Learning Rate: 6.25e-05 Total Training Loss:  1.868355216778582\n",
      "Epoch 2007. Learning Rate: 6.25e-05 Total Training Loss:  1.868362034088932\n",
      "Epoch 2008. Learning Rate: 6.25e-05 Total Training Loss:  1.868221092474414\n",
      "Epoch 2009. Learning Rate: 6.25e-05 Total Training Loss:  1.8676715991750825\n",
      "Epoch 2010. Learning Rate: 6.25e-05 Total Training Loss:  1.8684797130117659\n",
      "Epoch 2011. Learning Rate: 6.25e-05 Total Training Loss:  1.8675147299072705\n",
      "Epoch 2012. Learning Rate: 6.25e-05 Total Training Loss:  1.8681721816537902\n",
      "Epoch 2013. Learning Rate: 6.25e-05 Total Training Loss:  1.8673010102356784\n",
      "Epoch 2014. Learning Rate: 6.25e-05 Total Training Loss:  1.8683913476706948\n",
      "Epoch 2015. Learning Rate: 6.25e-05 Total Training Loss:  1.8674709675542545\n",
      "Epoch 2016. Learning Rate: 6.25e-05 Total Training Loss:  1.867873676790623\n",
      "Epoch 2017. Learning Rate: 6.25e-05 Total Training Loss:  1.8676564483030234\n",
      "Epoch 2018. Learning Rate: 6.25e-05 Total Training Loss:  1.867780857544858\n",
      "Epoch 2019. Learning Rate: 6.25e-05 Total Training Loss:  1.867384078213945\n",
      "Epoch 2020. Learning Rate: 6.25e-05 Total Training Loss:  1.8675935052742716\n",
      "Epoch 2021. Learning Rate: 6.25e-05 Total Training Loss:  1.867515184887452\n",
      "Epoch 2022. Learning Rate: 6.25e-05 Total Training Loss:  1.867381917545572\n",
      "Epoch 2023. Learning Rate: 6.25e-05 Total Training Loss:  1.8671825593919493\n",
      "Epoch 2024. Learning Rate: 6.25e-05 Total Training Loss:  1.8674592448514886\n",
      "Epoch 2025. Learning Rate: 6.25e-05 Total Training Loss:  1.867037184885703\n",
      "Epoch 2026. Learning Rate: 6.25e-05 Total Training Loss:  1.867390159779461\n",
      "Epoch 2027. Learning Rate: 6.25e-05 Total Training Loss:  1.8668550608854275\n",
      "Epoch 2028. Learning Rate: 6.25e-05 Total Training Loss:  1.8667899651045445\n",
      "Epoch 2029. Learning Rate: 6.25e-05 Total Training Loss:  1.8671499670599587\n",
      "Epoch 2030. Learning Rate: 6.25e-05 Total Training Loss:  1.8667264073446859\n",
      "Epoch 2031. Learning Rate: 6.25e-05 Total Training Loss:  1.866786364786094\n",
      "Epoch 2032. Learning Rate: 6.25e-05 Total Training Loss:  1.8667816658562515\n",
      "Epoch 2033. Learning Rate: 6.25e-05 Total Training Loss:  1.8666815083124675\n",
      "Epoch 2034. Learning Rate: 6.25e-05 Total Training Loss:  1.8666572546062525\n",
      "Epoch 2035. Learning Rate: 6.25e-05 Total Training Loss:  1.8662228753673844\n",
      "Epoch 2036. Learning Rate: 6.25e-05 Total Training Loss:  1.8664504636544734\n",
      "Epoch 2037. Learning Rate: 6.25e-05 Total Training Loss:  1.8663536136446055\n",
      "Epoch 2038. Learning Rate: 6.25e-05 Total Training Loss:  1.8671204860438593\n",
      "Epoch 2039. Learning Rate: 6.25e-05 Total Training Loss:  1.8659432951244526\n",
      "Epoch 2040. Learning Rate: 6.25e-05 Total Training Loss:  1.866322216752451\n",
      "Epoch 2041. Learning Rate: 6.25e-05 Total Training Loss:  1.865724349830998\n",
      "Epoch 2042. Learning Rate: 6.25e-05 Total Training Loss:  1.8663235606218223\n",
      "Epoch 2043. Learning Rate: 6.25e-05 Total Training Loss:  1.8661285462148953\n",
      "Epoch 2044. Learning Rate: 6.25e-05 Total Training Loss:  1.8657942891295534\n",
      "Epoch 2045. Learning Rate: 6.25e-05 Total Training Loss:  1.866053639590973\n",
      "Epoch 2046. Learning Rate: 6.25e-05 Total Training Loss:  1.8658262926910538\n",
      "Epoch 2047. Learning Rate: 6.25e-05 Total Training Loss:  1.8660664286871906\n",
      "Epoch 2048. Learning Rate: 6.25e-05 Total Training Loss:  1.8657337813347112\n",
      "Epoch 2049. Learning Rate: 6.25e-05 Total Training Loss:  1.8657678768213373\n",
      "Epoch 2050. Learning Rate: 6.25e-05 Total Training Loss:  1.8653873912408017\n",
      "Epoch 2051. Learning Rate: 6.25e-05 Total Training Loss:  1.8663797508634161\n",
      "Epoch 2052. Learning Rate: 6.25e-05 Total Training Loss:  1.8650863515795209\n",
      "Epoch 2053. Learning Rate: 6.25e-05 Total Training Loss:  1.8656170089379884\n",
      "Epoch 2054. Learning Rate: 6.25e-05 Total Training Loss:  1.8651223491760902\n",
      "Epoch 2055. Learning Rate: 6.25e-05 Total Training Loss:  1.865586549276486\n",
      "Epoch 2056. Learning Rate: 6.25e-05 Total Training Loss:  1.8649360933050048\n",
      "Epoch 2057. Learning Rate: 6.25e-05 Total Training Loss:  1.8654099958366714\n",
      "Epoch 2058. Learning Rate: 6.25e-05 Total Training Loss:  1.8650452264409978\n",
      "Epoch 2059. Learning Rate: 6.25e-05 Total Training Loss:  1.8650755462585948\n",
      "Epoch 2060. Learning Rate: 6.25e-05 Total Training Loss:  1.8653894427698106\n",
      "Epoch 2061. Learning Rate: 6.25e-05 Total Training Loss:  1.8647438733896706\n",
      "Epoch 2062. Learning Rate: 6.25e-05 Total Training Loss:  1.8652115853619762\n",
      "Epoch 2063. Learning Rate: 6.25e-05 Total Training Loss:  1.8646054478886072\n",
      "Epoch 2064. Learning Rate: 6.25e-05 Total Training Loss:  1.8649202469387092\n",
      "Epoch 2065. Learning Rate: 6.25e-05 Total Training Loss:  1.8646362201834563\n",
      "Epoch 2066. Learning Rate: 6.25e-05 Total Training Loss:  1.8650151614565402\n",
      "Epoch 2067. Learning Rate: 6.25e-05 Total Training Loss:  1.8644661830330733\n",
      "Epoch 2068. Learning Rate: 6.25e-05 Total Training Loss:  1.8645457911479753\n",
      "Epoch 2069. Learning Rate: 6.25e-05 Total Training Loss:  1.8644258006825112\n",
      "Epoch 2070. Learning Rate: 6.25e-05 Total Training Loss:  1.8646951246482786\n",
      "Epoch 2071. Learning Rate: 6.25e-05 Total Training Loss:  1.8644385647494346\n",
      "Epoch 2072. Learning Rate: 6.25e-05 Total Training Loss:  1.864269479032373\n",
      "Epoch 2073. Learning Rate: 6.25e-05 Total Training Loss:  1.864219032402616\n",
      "Epoch 2074. Learning Rate: 6.25e-05 Total Training Loss:  1.864052226766944\n",
      "Epoch 2075. Learning Rate: 6.25e-05 Total Training Loss:  1.864035508420784\n",
      "Epoch 2076. Learning Rate: 6.25e-05 Total Training Loss:  1.8639065974857658\n",
      "Epoch 2077. Learning Rate: 6.25e-05 Total Training Loss:  1.864104530250188\n",
      "Epoch 2078. Learning Rate: 6.25e-05 Total Training Loss:  1.8640995570167433\n",
      "Epoch 2079. Learning Rate: 6.25e-05 Total Training Loss:  1.864165712322574\n",
      "Epoch 2080. Learning Rate: 6.25e-05 Total Training Loss:  1.8637612904713023\n",
      "Epoch 2081. Learning Rate: 6.25e-05 Total Training Loss:  1.863314181973692\n",
      "Epoch 2082. Learning Rate: 6.25e-05 Total Training Loss:  1.863969861442456\n",
      "Epoch 2083. Learning Rate: 6.25e-05 Total Training Loss:  1.8636359465890564\n",
      "Epoch 2084. Learning Rate: 6.25e-05 Total Training Loss:  1.8637209649023134\n",
      "Epoch 2085. Learning Rate: 6.25e-05 Total Training Loss:  1.8635929177398793\n",
      "Epoch 2086. Learning Rate: 6.25e-05 Total Training Loss:  1.8635693511168938\n",
      "Epoch 2087. Learning Rate: 6.25e-05 Total Training Loss:  1.8635378838516772\n",
      "Epoch 2088. Learning Rate: 6.25e-05 Total Training Loss:  1.863929113256745\n",
      "Epoch 2089. Learning Rate: 6.25e-05 Total Training Loss:  1.8629720221215393\n",
      "Epoch 2090. Learning Rate: 6.25e-05 Total Training Loss:  1.8631974148447625\n",
      "Epoch 2091. Learning Rate: 6.25e-05 Total Training Loss:  1.8633727085543796\n",
      "Epoch 2092. Learning Rate: 6.25e-05 Total Training Loss:  1.862910311028827\n",
      "Epoch 2093. Learning Rate: 6.25e-05 Total Training Loss:  1.8632855813775677\n",
      "Epoch 2094. Learning Rate: 6.25e-05 Total Training Loss:  1.862665094435215\n",
      "Epoch 2095. Learning Rate: 6.25e-05 Total Training Loss:  1.8633018309774343\n",
      "Epoch 2096. Learning Rate: 6.25e-05 Total Training Loss:  1.8627433971560095\n",
      "Epoch 2097. Learning Rate: 6.25e-05 Total Training Loss:  1.862916626414517\n",
      "Epoch 2098. Learning Rate: 6.25e-05 Total Training Loss:  1.8627538260188885\n",
      "Epoch 2099. Learning Rate: 6.25e-05 Total Training Loss:  1.8626308907405473\n",
      "Epoch 2100. Learning Rate: 6.25e-05 Total Training Loss:  1.8627175152942073\n",
      "Epoch 2101. Learning Rate: 6.25e-05 Total Training Loss:  1.862388191599166\n",
      "Epoch 2102. Learning Rate: 6.25e-05 Total Training Loss:  1.862909398128977\n",
      "Epoch 2103. Learning Rate: 6.25e-05 Total Training Loss:  1.8622549117426388\n",
      "Epoch 2104. Learning Rate: 6.25e-05 Total Training Loss:  1.862571272533387\n",
      "Epoch 2105. Learning Rate: 6.25e-05 Total Training Loss:  1.8620937627274543\n",
      "Epoch 2106. Learning Rate: 6.25e-05 Total Training Loss:  1.8630509096255992\n",
      "Epoch 2107. Learning Rate: 6.25e-05 Total Training Loss:  1.8620671777171083\n",
      "Epoch 2108. Learning Rate: 6.25e-05 Total Training Loss:  1.862541078851791\n",
      "Epoch 2109. Learning Rate: 6.25e-05 Total Training Loss:  1.8618232521694154\n",
      "Epoch 2110. Learning Rate: 6.25e-05 Total Training Loss:  1.8622771221853327\n",
      "Epoch 2111. Learning Rate: 6.25e-05 Total Training Loss:  1.8619096731126774\n",
      "Epoch 2112. Learning Rate: 6.25e-05 Total Training Loss:  1.862078660808038\n",
      "Epoch 2113. Learning Rate: 6.25e-05 Total Training Loss:  1.8621527887007687\n",
      "Epoch 2114. Learning Rate: 6.25e-05 Total Training Loss:  1.8615892532397993\n",
      "Epoch 2115. Learning Rate: 6.25e-05 Total Training Loss:  1.862280274683144\n",
      "Epoch 2116. Learning Rate: 6.25e-05 Total Training Loss:  1.8613272965012584\n",
      "Epoch 2117. Learning Rate: 6.25e-05 Total Training Loss:  1.861849280889146\n",
      "Epoch 2118. Learning Rate: 6.25e-05 Total Training Loss:  1.8615017457632348\n",
      "Epoch 2119. Learning Rate: 6.25e-05 Total Training Loss:  1.8622290867206175\n",
      "Epoch 2120. Learning Rate: 6.25e-05 Total Training Loss:  1.8614118744444568\n",
      "Epoch 2121. Learning Rate: 6.25e-05 Total Training Loss:  1.8618756186915562\n",
      "Epoch 2122. Learning Rate: 6.25e-05 Total Training Loss:  1.861518641380826\n",
      "Epoch 2123. Learning Rate: 6.25e-05 Total Training Loss:  1.8610884883673862\n",
      "Epoch 2124. Learning Rate: 6.25e-05 Total Training Loss:  1.8615786142181605\n",
      "Epoch 2125. Learning Rate: 6.25e-05 Total Training Loss:  1.8611314603476785\n",
      "Epoch 2126. Learning Rate: 6.25e-05 Total Training Loss:  1.861529238987714\n",
      "Epoch 2127. Learning Rate: 6.25e-05 Total Training Loss:  1.8609267847496085\n",
      "Epoch 2128. Learning Rate: 6.25e-05 Total Training Loss:  1.8612523434858304\n",
      "Epoch 2129. Learning Rate: 6.25e-05 Total Training Loss:  1.8613590218592435\n",
      "Epoch 2130. Learning Rate: 6.25e-05 Total Training Loss:  1.8608045486034825\n",
      "Epoch 2131. Learning Rate: 6.25e-05 Total Training Loss:  1.8611483664135449\n",
      "Epoch 2132. Learning Rate: 6.25e-05 Total Training Loss:  1.8607780140591785\n",
      "Epoch 2133. Learning Rate: 6.25e-05 Total Training Loss:  1.8610572731122375\n",
      "Epoch 2134. Learning Rate: 6.25e-05 Total Training Loss:  1.8605133319215383\n",
      "Epoch 2135. Learning Rate: 6.25e-05 Total Training Loss:  1.8608863181434572\n",
      "Epoch 2136. Learning Rate: 6.25e-05 Total Training Loss:  1.8606176793109626\n",
      "Epoch 2137. Learning Rate: 6.25e-05 Total Training Loss:  1.8607329289079644\n",
      "Epoch 2138. Learning Rate: 6.25e-05 Total Training Loss:  1.860246557800565\n",
      "Epoch 2139. Learning Rate: 6.25e-05 Total Training Loss:  1.8608267386152875\n",
      "Epoch 2140. Learning Rate: 6.25e-05 Total Training Loss:  1.8606275324418675\n",
      "Epoch 2141. Learning Rate: 6.25e-05 Total Training Loss:  1.8604206064192113\n",
      "Epoch 2142. Learning Rate: 6.25e-05 Total Training Loss:  1.8602944431477226\n",
      "Epoch 2143. Learning Rate: 6.25e-05 Total Training Loss:  1.860114740789868\n",
      "Epoch 2144. Learning Rate: 6.25e-05 Total Training Loss:  1.860370683745714\n",
      "Epoch 2145. Learning Rate: 6.25e-05 Total Training Loss:  1.8603125969530083\n",
      "Epoch 2146. Learning Rate: 6.25e-05 Total Training Loss:  1.8600835068209562\n",
      "Epoch 2147. Learning Rate: 6.25e-05 Total Training Loss:  1.8600697582005523\n",
      "Epoch 2148. Learning Rate: 6.25e-05 Total Training Loss:  1.8601006860262714\n",
      "Epoch 2149. Learning Rate: 6.25e-05 Total Training Loss:  1.860056995676132\n",
      "Epoch 2150. Learning Rate: 6.25e-05 Total Training Loss:  1.8603109194082208\n",
      "Epoch 2151. Learning Rate: 6.25e-05 Total Training Loss:  1.8591402914607897\n",
      "Epoch 2152. Learning Rate: 6.25e-05 Total Training Loss:  1.8605060781992506\n",
      "Epoch 2153. Learning Rate: 6.25e-05 Total Training Loss:  1.8596448335738387\n",
      "Epoch 2154. Learning Rate: 6.25e-05 Total Training Loss:  1.8597763115540147\n",
      "Epoch 2155. Learning Rate: 6.25e-05 Total Training Loss:  1.8592171574709937\n",
      "Epoch 2156. Learning Rate: 6.25e-05 Total Training Loss:  1.8602625454077497\n",
      "Epoch 2157. Learning Rate: 6.25e-05 Total Training Loss:  1.859478389902506\n",
      "Epoch 2158. Learning Rate: 6.25e-05 Total Training Loss:  1.8594640164810698\n",
      "Epoch 2159. Learning Rate: 6.25e-05 Total Training Loss:  1.8601040790672414\n",
      "Epoch 2160. Learning Rate: 6.25e-05 Total Training Loss:  1.8591141447832342\n",
      "Epoch 2161. Learning Rate: 6.25e-05 Total Training Loss:  1.859296084032394\n",
      "Epoch 2162. Learning Rate: 6.25e-05 Total Training Loss:  1.8592929480946623\n",
      "Epoch 2163. Learning Rate: 6.25e-05 Total Training Loss:  1.8594893697591033\n",
      "Epoch 2164. Learning Rate: 6.25e-05 Total Training Loss:  1.8589320183964446\n",
      "Epoch 2165. Learning Rate: 6.25e-05 Total Training Loss:  1.8593936961551663\n",
      "Epoch 2166. Learning Rate: 6.25e-05 Total Training Loss:  1.8586075513449032\n",
      "Epoch 2167. Learning Rate: 6.25e-05 Total Training Loss:  1.8590168526570778\n",
      "Epoch 2168. Learning Rate: 6.25e-05 Total Training Loss:  1.8591275509388652\n",
      "Epoch 2169. Learning Rate: 6.25e-05 Total Training Loss:  1.8588236671930645\n",
      "Epoch 2170. Learning Rate: 6.25e-05 Total Training Loss:  1.8584439365076832\n",
      "Epoch 2171. Learning Rate: 6.25e-05 Total Training Loss:  1.8591573168232571\n",
      "Epoch 2172. Learning Rate: 6.25e-05 Total Training Loss:  1.858638452948071\n",
      "Epoch 2173. Learning Rate: 6.25e-05 Total Training Loss:  1.8592955315543804\n",
      "Epoch 2174. Learning Rate: 6.25e-05 Total Training Loss:  1.8586192714283243\n",
      "Epoch 2175. Learning Rate: 6.25e-05 Total Training Loss:  1.8586099662352353\n",
      "Epoch 2176. Learning Rate: 6.25e-05 Total Training Loss:  1.8581899121927563\n",
      "Epoch 2177. Learning Rate: 6.25e-05 Total Training Loss:  1.8586813230067492\n",
      "Epoch 2178. Learning Rate: 6.25e-05 Total Training Loss:  1.8581488630443346\n",
      "Epoch 2179. Learning Rate: 6.25e-05 Total Training Loss:  1.8590056358661968\n",
      "Epoch 2180. Learning Rate: 6.25e-05 Total Training Loss:  1.858296416583471\n",
      "Epoch 2181. Learning Rate: 6.25e-05 Total Training Loss:  1.85843353823293\n",
      "Epoch 2182. Learning Rate: 6.25e-05 Total Training Loss:  1.8585391946544405\n",
      "Epoch 2183. Learning Rate: 6.25e-05 Total Training Loss:  1.8578737045172602\n",
      "Epoch 2184. Learning Rate: 6.25e-05 Total Training Loss:  1.8581587891967501\n",
      "Epoch 2185. Learning Rate: 6.25e-05 Total Training Loss:  1.8577540408005007\n",
      "Epoch 2186. Learning Rate: 6.25e-05 Total Training Loss:  1.8583601034770254\n",
      "Epoch 2187. Learning Rate: 6.25e-05 Total Training Loss:  1.8572433368535712\n",
      "Epoch 2188. Learning Rate: 6.25e-05 Total Training Loss:  1.8585920884506777\n",
      "Epoch 2189. Learning Rate: 6.25e-05 Total Training Loss:  1.8576293144142255\n",
      "Epoch 2190. Learning Rate: 6.25e-05 Total Training Loss:  1.858088225737447\n",
      "Epoch 2191. Learning Rate: 6.25e-05 Total Training Loss:  1.8578212861320935\n",
      "Epoch 2192. Learning Rate: 6.25e-05 Total Training Loss:  1.8579405950731598\n",
      "Epoch 2193. Learning Rate: 6.25e-05 Total Training Loss:  1.8574560936831404\n",
      "Epoch 2194. Learning Rate: 6.25e-05 Total Training Loss:  1.8576776729605626\n",
      "Epoch 2195. Learning Rate: 6.25e-05 Total Training Loss:  1.8574589258932974\n",
      "Epoch 2196. Learning Rate: 6.25e-05 Total Training Loss:  1.8579822606407106\n",
      "Epoch 2197. Learning Rate: 6.25e-05 Total Training Loss:  1.8567648456664756\n",
      "Epoch 2198. Learning Rate: 6.25e-05 Total Training Loss:  1.8579092402360402\n",
      "Epoch 2199. Learning Rate: 6.25e-05 Total Training Loss:  1.857255988579709\n",
      "Epoch 2200. Learning Rate: 6.25e-05 Total Training Loss:  1.8571246677311137\n",
      "Epoch 2201. Learning Rate: 6.25e-05 Total Training Loss:  1.8578290971345268\n",
      "Epoch 2202. Learning Rate: 6.25e-05 Total Training Loss:  1.856748785910895\n",
      "Epoch 2203. Learning Rate: 6.25e-05 Total Training Loss:  1.8576388667279389\n",
      "Epoch 2204. Learning Rate: 6.25e-05 Total Training Loss:  1.8571624077449087\n",
      "Epoch 2205. Learning Rate: 6.25e-05 Total Training Loss:  1.8568346456449945\n",
      "Epoch 2206. Learning Rate: 6.25e-05 Total Training Loss:  1.8561930788564496\n",
      "Epoch 2207. Learning Rate: 6.25e-05 Total Training Loss:  1.8579980196373072\n",
      "Epoch 2208. Learning Rate: 6.25e-05 Total Training Loss:  1.8565596292901319\n",
      "Epoch 2209. Learning Rate: 6.25e-05 Total Training Loss:  1.8570791275124066\n",
      "Epoch 2210. Learning Rate: 6.25e-05 Total Training Loss:  1.8565968766633887\n",
      "Epoch 2211. Learning Rate: 6.25e-05 Total Training Loss:  1.8569221748621203\n",
      "Epoch 2212. Learning Rate: 6.25e-05 Total Training Loss:  1.85669479414355\n",
      "Epoch 2213. Learning Rate: 6.25e-05 Total Training Loss:  1.8565629897639155\n",
      "Epoch 2214. Learning Rate: 6.25e-05 Total Training Loss:  1.8569389144540764\n",
      "Epoch 2215. Learning Rate: 6.25e-05 Total Training Loss:  1.8566951965913177\n",
      "Epoch 2216. Learning Rate: 6.25e-05 Total Training Loss:  1.8563206921389792\n",
      "Epoch 2217. Learning Rate: 6.25e-05 Total Training Loss:  1.8561394891585223\n",
      "Epoch 2218. Learning Rate: 6.25e-05 Total Training Loss:  1.8561896395985968\n",
      "Epoch 2219. Learning Rate: 6.25e-05 Total Training Loss:  1.8564219899708405\n",
      "Epoch 2220. Learning Rate: 6.25e-05 Total Training Loss:  1.8559322944201995\n",
      "Epoch 2221. Learning Rate: 6.25e-05 Total Training Loss:  1.8562530361814424\n",
      "Epoch 2222. Learning Rate: 6.25e-05 Total Training Loss:  1.8561430336849298\n",
      "Epoch 2223. Learning Rate: 6.25e-05 Total Training Loss:  1.8563203070661984\n",
      "Epoch 2224. Learning Rate: 6.25e-05 Total Training Loss:  1.856062499107793\n",
      "Epoch 2225. Learning Rate: 6.25e-05 Total Training Loss:  1.855737608653726\n",
      "Epoch 2226. Learning Rate: 6.25e-05 Total Training Loss:  1.8558116964122746\n",
      "Epoch 2227. Learning Rate: 6.25e-05 Total Training Loss:  1.8561550106387585\n",
      "Epoch 2228. Learning Rate: 6.25e-05 Total Training Loss:  1.8556882115080953\n",
      "Epoch 2229. Learning Rate: 6.25e-05 Total Training Loss:  1.8555737818824127\n",
      "Epoch 2230. Learning Rate: 6.25e-05 Total Training Loss:  1.8554422819288447\n",
      "Epoch 2231. Learning Rate: 6.25e-05 Total Training Loss:  1.856050232076086\n",
      "Epoch 2232. Learning Rate: 6.25e-05 Total Training Loss:  1.855069505632855\n",
      "Epoch 2233. Learning Rate: 6.25e-05 Total Training Loss:  1.8556181483145338\n",
      "Epoch 2234. Learning Rate: 6.25e-05 Total Training Loss:  1.8552171694173012\n",
      "Epoch 2235. Learning Rate: 6.25e-05 Total Training Loss:  1.8557504015043378\n",
      "Epoch 2236. Learning Rate: 6.25e-05 Total Training Loss:  1.8551942395279184\n",
      "Epoch 2237. Learning Rate: 6.25e-05 Total Training Loss:  1.8549174103245605\n",
      "Epoch 2238. Learning Rate: 6.25e-05 Total Training Loss:  1.8551496046420652\n",
      "Epoch 2239. Learning Rate: 6.25e-05 Total Training Loss:  1.855345915275393\n",
      "Epoch 2240. Learning Rate: 6.25e-05 Total Training Loss:  1.8548615563486237\n",
      "Epoch 2241. Learning Rate: 6.25e-05 Total Training Loss:  1.8549328068038449\n",
      "Epoch 2242. Learning Rate: 6.25e-05 Total Training Loss:  1.8549801759654656\n",
      "Epoch 2243. Learning Rate: 6.25e-05 Total Training Loss:  1.8551362431899179\n",
      "Epoch 2244. Learning Rate: 6.25e-05 Total Training Loss:  1.8547980534785893\n",
      "Epoch 2245. Learning Rate: 6.25e-05 Total Training Loss:  1.8549641725549009\n",
      "Epoch 2246. Learning Rate: 6.25e-05 Total Training Loss:  1.8548605519463308\n",
      "Epoch 2247. Learning Rate: 6.25e-05 Total Training Loss:  1.8547605799685698\n",
      "Epoch 2248. Learning Rate: 6.25e-05 Total Training Loss:  1.854530925367726\n",
      "Epoch 2249. Learning Rate: 6.25e-05 Total Training Loss:  1.8548151713621337\n",
      "Epoch 2250. Learning Rate: 6.25e-05 Total Training Loss:  1.8546946901769843\n",
      "Epoch 2251. Learning Rate: 6.25e-05 Total Training Loss:  1.8547935712849721\n",
      "Epoch 2252. Learning Rate: 6.25e-05 Total Training Loss:  1.8538618517050054\n",
      "Epoch 2253. Learning Rate: 6.25e-05 Total Training Loss:  1.8546942786779255\n",
      "Epoch 2254. Learning Rate: 6.25e-05 Total Training Loss:  1.853840970172314\n",
      "Epoch 2255. Learning Rate: 6.25e-05 Total Training Loss:  1.8548504018981475\n",
      "Epoch 2256. Learning Rate: 6.25e-05 Total Training Loss:  1.8539846472267527\n",
      "Epoch 2257. Learning Rate: 6.25e-05 Total Training Loss:  1.8542850342637394\n",
      "Epoch 2258. Learning Rate: 6.25e-05 Total Training Loss:  1.8539982658112422\n",
      "Epoch 2259. Learning Rate: 6.25e-05 Total Training Loss:  1.8541928180784453\n",
      "Epoch 2260. Learning Rate: 6.25e-05 Total Training Loss:  1.8542004818737041\n",
      "Epoch 2261. Learning Rate: 6.25e-05 Total Training Loss:  1.8535919384739827\n",
      "Epoch 2262. Learning Rate: 6.25e-05 Total Training Loss:  1.853633569960948\n",
      "Epoch 2263. Learning Rate: 6.25e-05 Total Training Loss:  1.8544001754780766\n",
      "Epoch 2264. Learning Rate: 6.25e-05 Total Training Loss:  1.853625135117909\n",
      "Epoch 2265. Learning Rate: 6.25e-05 Total Training Loss:  1.8538801278918982\n",
      "Epoch 2266. Learning Rate: 6.25e-05 Total Training Loss:  1.8535386105941143\n",
      "Epoch 2267. Learning Rate: 6.25e-05 Total Training Loss:  1.8537737209699117\n",
      "Epoch 2268. Learning Rate: 6.25e-05 Total Training Loss:  1.8536505862721242\n",
      "Epoch 2269. Learning Rate: 6.25e-05 Total Training Loss:  1.8536656398209743\n",
      "Epoch 2270. Learning Rate: 6.25e-05 Total Training Loss:  1.8530901763297152\n",
      "Epoch 2271. Learning Rate: 6.25e-05 Total Training Loss:  1.8535777092911303\n",
      "Epoch 2272. Learning Rate: 6.25e-05 Total Training Loss:  1.8535246830433607\n",
      "Epoch 2273. Learning Rate: 6.25e-05 Total Training Loss:  1.8530984450189862\n",
      "Epoch 2274. Learning Rate: 6.25e-05 Total Training Loss:  1.8530027430970222\n",
      "Epoch 2275. Learning Rate: 6.25e-05 Total Training Loss:  1.8532618901517708\n",
      "Epoch 2276. Learning Rate: 6.25e-05 Total Training Loss:  1.853361394489184\n",
      "Epoch 2277. Learning Rate: 6.25e-05 Total Training Loss:  1.8532859812839888\n",
      "Epoch 2278. Learning Rate: 6.25e-05 Total Training Loss:  1.8532461124123074\n",
      "Epoch 2279. Learning Rate: 6.25e-05 Total Training Loss:  1.8530593406176195\n",
      "Epoch 2280. Learning Rate: 6.25e-05 Total Training Loss:  1.8529931461962406\n",
      "Epoch 2281. Learning Rate: 6.25e-05 Total Training Loss:  1.8530848323425744\n",
      "Epoch 2282. Learning Rate: 6.25e-05 Total Training Loss:  1.8526315379422158\n",
      "Epoch 2283. Learning Rate: 6.25e-05 Total Training Loss:  1.8527999842190184\n",
      "Epoch 2284. Learning Rate: 6.25e-05 Total Training Loss:  1.8526544493215624\n",
      "Epoch 2285. Learning Rate: 6.25e-05 Total Training Loss:  1.8529773051268421\n",
      "Epoch 2286. Learning Rate: 6.25e-05 Total Training Loss:  1.8523642242653295\n",
      "Epoch 2287. Learning Rate: 6.25e-05 Total Training Loss:  1.8523706898849923\n",
      "Epoch 2288. Learning Rate: 6.25e-05 Total Training Loss:  1.8522713704151101\n",
      "Epoch 2289. Learning Rate: 6.25e-05 Total Training Loss:  1.852709474915173\n",
      "Epoch 2290. Learning Rate: 6.25e-05 Total Training Loss:  1.8517792430357076\n",
      "Epoch 2291. Learning Rate: 6.25e-05 Total Training Loss:  1.8527763879974373\n",
      "Epoch 2292. Learning Rate: 6.25e-05 Total Training Loss:  1.8523027884948533\n",
      "Epoch 2293. Learning Rate: 6.25e-05 Total Training Loss:  1.852105382597074\n",
      "Epoch 2294. Learning Rate: 6.25e-05 Total Training Loss:  1.8517341196420603\n",
      "Epoch 2295. Learning Rate: 6.25e-05 Total Training Loss:  1.8528687576472294\n",
      "Epoch 2296. Learning Rate: 6.25e-05 Total Training Loss:  1.8518726298352703\n",
      "Epoch 2297. Learning Rate: 6.25e-05 Total Training Loss:  1.8516321771894582\n",
      "Epoch 2298. Learning Rate: 6.25e-05 Total Training Loss:  1.8524894595029764\n",
      "Epoch 2299. Learning Rate: 6.25e-05 Total Training Loss:  1.8517549169482663\n",
      "Epoch 2300. Learning Rate: 6.25e-05 Total Training Loss:  1.8517407947510947\n",
      "Epoch 2301. Learning Rate: 6.25e-05 Total Training Loss:  1.8514505561906844\n",
      "Epoch 2302. Learning Rate: 6.25e-05 Total Training Loss:  1.8517161249183118\n",
      "Epoch 2303. Learning Rate: 6.25e-05 Total Training Loss:  1.8516413346223999\n",
      "Epoch 2304. Learning Rate: 6.25e-05 Total Training Loss:  1.8517588378454093\n",
      "Epoch 2305. Learning Rate: 6.25e-05 Total Training Loss:  1.8516936673258897\n",
      "Epoch 2306. Learning Rate: 6.25e-05 Total Training Loss:  1.8514814208610915\n",
      "Epoch 2307. Learning Rate: 6.25e-05 Total Training Loss:  1.8512314908148255\n",
      "Epoch 2308. Learning Rate: 6.25e-05 Total Training Loss:  1.8515281035506632\n",
      "Epoch 2309. Learning Rate: 6.25e-05 Total Training Loss:  1.8512804258498363\n",
      "Epoch 2310. Learning Rate: 6.25e-05 Total Training Loss:  1.8516612999083009\n",
      "Epoch 2311. Learning Rate: 6.25e-05 Total Training Loss:  1.851038220222108\n",
      "Epoch 2312. Learning Rate: 6.25e-05 Total Training Loss:  1.8509903649101034\n",
      "Epoch 2313. Learning Rate: 6.25e-05 Total Training Loss:  1.8509763658803422\n",
      "Epoch 2314. Learning Rate: 6.25e-05 Total Training Loss:  1.851146535947919\n",
      "Epoch 2315. Learning Rate: 6.25e-05 Total Training Loss:  1.8512573010229971\n",
      "Epoch 2316. Learning Rate: 6.25e-05 Total Training Loss:  1.8511025643965695\n",
      "Epoch 2317. Learning Rate: 6.25e-05 Total Training Loss:  1.8507091239152942\n",
      "Epoch 2318. Learning Rate: 6.25e-05 Total Training Loss:  1.8506097871577367\n",
      "Epoch 2319. Learning Rate: 6.25e-05 Total Training Loss:  1.8506714295072015\n",
      "Epoch 2320. Learning Rate: 6.25e-05 Total Training Loss:  1.8509949379076716\n",
      "Epoch 2321. Learning Rate: 6.25e-05 Total Training Loss:  1.8502327240421437\n",
      "Epoch 2322. Learning Rate: 6.25e-05 Total Training Loss:  1.8506652807118371\n",
      "Epoch 2323. Learning Rate: 6.25e-05 Total Training Loss:  1.8500784835778177\n",
      "Epoch 2324. Learning Rate: 6.25e-05 Total Training Loss:  1.850661732867593\n",
      "Epoch 2325. Learning Rate: 6.25e-05 Total Training Loss:  1.85054744509398\n",
      "Epoch 2326. Learning Rate: 6.25e-05 Total Training Loss:  1.8505509313254151\n",
      "Epoch 2327. Learning Rate: 6.25e-05 Total Training Loss:  1.850437269109534\n",
      "Epoch 2328. Learning Rate: 6.25e-05 Total Training Loss:  1.8504739502677694\n",
      "Epoch 2329. Learning Rate: 6.25e-05 Total Training Loss:  1.850085961370496\n",
      "Epoch 2330. Learning Rate: 6.25e-05 Total Training Loss:  1.8502916120924056\n",
      "Epoch 2331. Learning Rate: 6.25e-05 Total Training Loss:  1.8495200288307387\n",
      "Epoch 2332. Learning Rate: 6.25e-05 Total Training Loss:  1.8502680734090973\n",
      "Epoch 2333. Learning Rate: 6.25e-05 Total Training Loss:  1.8500442763324827\n",
      "Epoch 2334. Learning Rate: 6.25e-05 Total Training Loss:  1.8496560794883408\n",
      "Epoch 2335. Learning Rate: 6.25e-05 Total Training Loss:  1.8497211242211051\n",
      "Epoch 2336. Learning Rate: 6.25e-05 Total Training Loss:  1.8500649924390018\n",
      "Epoch 2337. Learning Rate: 6.25e-05 Total Training Loss:  1.849434473842848\n",
      "Epoch 2338. Learning Rate: 6.25e-05 Total Training Loss:  1.84979345620377\n",
      "Epoch 2339. Learning Rate: 6.25e-05 Total Training Loss:  1.849251967127202\n",
      "Epoch 2340. Learning Rate: 6.25e-05 Total Training Loss:  1.8499937298765872\n",
      "Epoch 2341. Learning Rate: 6.25e-05 Total Training Loss:  1.8489615384314675\n",
      "Epoch 2342. Learning Rate: 6.25e-05 Total Training Loss:  1.8505669509468134\n",
      "Epoch 2343. Learning Rate: 6.25e-05 Total Training Loss:  1.8489190268446691\n",
      "Epoch 2344. Learning Rate: 6.25e-05 Total Training Loss:  1.8497990510077216\n",
      "Epoch 2345. Learning Rate: 6.25e-05 Total Training Loss:  1.84920261069783\n",
      "Epoch 2346. Learning Rate: 6.25e-05 Total Training Loss:  1.8490303695434704\n",
      "Epoch 2347. Learning Rate: 6.25e-05 Total Training Loss:  1.8489620308682788\n",
      "Epoch 2348. Learning Rate: 6.25e-05 Total Training Loss:  1.848964842618443\n",
      "Epoch 2349. Learning Rate: 6.25e-05 Total Training Loss:  1.848720618581865\n",
      "Epoch 2350. Learning Rate: 6.25e-05 Total Training Loss:  1.8493898850283585\n",
      "Epoch 2351. Learning Rate: 6.25e-05 Total Training Loss:  1.8490130069840234\n",
      "Epoch 2352. Learning Rate: 6.25e-05 Total Training Loss:  1.8486708676209673\n",
      "Epoch 2353. Learning Rate: 6.25e-05 Total Training Loss:  1.8488758357416373\n",
      "Epoch 2354. Learning Rate: 6.25e-05 Total Training Loss:  1.8482034134212881\n",
      "Epoch 2355. Learning Rate: 6.25e-05 Total Training Loss:  1.8494518490624614\n",
      "Epoch 2356. Learning Rate: 6.25e-05 Total Training Loss:  1.8483736464113463\n",
      "Epoch 2357. Learning Rate: 6.25e-05 Total Training Loss:  1.848488189134514\n",
      "Epoch 2358. Learning Rate: 6.25e-05 Total Training Loss:  1.8481414648413192\n",
      "Epoch 2359. Learning Rate: 6.25e-05 Total Training Loss:  1.849413337185979\n",
      "Epoch 2360. Learning Rate: 6.25e-05 Total Training Loss:  1.8480329818848986\n",
      "Epoch 2361. Learning Rate: 6.25e-05 Total Training Loss:  1.8480843280558474\n",
      "Epoch 2362. Learning Rate: 6.25e-05 Total Training Loss:  1.8484181413950864\n",
      "Epoch 2363. Learning Rate: 6.25e-05 Total Training Loss:  1.847822591138538\n",
      "Epoch 2364. Learning Rate: 6.25e-05 Total Training Loss:  1.8483346771099605\n",
      "Epoch 2365. Learning Rate: 6.25e-05 Total Training Loss:  1.8486120047746226\n",
      "Epoch 2366. Learning Rate: 6.25e-05 Total Training Loss:  1.8475805998314172\n",
      "Epoch 2367. Learning Rate: 6.25e-05 Total Training Loss:  1.8484769355272874\n",
      "Epoch 2368. Learning Rate: 6.25e-05 Total Training Loss:  1.8481076494499575\n",
      "Epoch 2369. Learning Rate: 6.25e-05 Total Training Loss:  1.848474580066977\n",
      "Epoch 2370. Learning Rate: 6.25e-05 Total Training Loss:  1.8472606736468151\n",
      "Epoch 2371. Learning Rate: 6.25e-05 Total Training Loss:  1.8479690180975012\n",
      "Epoch 2372. Learning Rate: 6.25e-05 Total Training Loss:  1.8478893562569283\n",
      "Epoch 2373. Learning Rate: 6.25e-05 Total Training Loss:  1.847173094196478\n",
      "Epoch 2374. Learning Rate: 6.25e-05 Total Training Loss:  1.8477099528245162\n",
      "Epoch 2375. Learning Rate: 6.25e-05 Total Training Loss:  1.8475013157876674\n",
      "Epoch 2376. Learning Rate: 6.25e-05 Total Training Loss:  1.8482526531151962\n",
      "Epoch 2377. Learning Rate: 6.25e-05 Total Training Loss:  1.8474193958682008\n",
      "Epoch 2378. Learning Rate: 6.25e-05 Total Training Loss:  1.847334843419958\n",
      "Epoch 2379. Learning Rate: 6.25e-05 Total Training Loss:  1.847747727908427\n",
      "Epoch 2380. Learning Rate: 6.25e-05 Total Training Loss:  1.84715882508317\n",
      "Epoch 2381. Learning Rate: 6.25e-05 Total Training Loss:  1.8472774018591736\n",
      "Epoch 2382. Learning Rate: 6.25e-05 Total Training Loss:  1.8468942864856217\n",
      "Epoch 2383. Learning Rate: 6.25e-05 Total Training Loss:  1.8476954358629882\n",
      "Epoch 2384. Learning Rate: 6.25e-05 Total Training Loss:  1.8469505201792344\n",
      "Epoch 2385. Learning Rate: 6.25e-05 Total Training Loss:  1.846862206410151\n",
      "Epoch 2386. Learning Rate: 6.25e-05 Total Training Loss:  1.8470141530560795\n",
      "Epoch 2387. Learning Rate: 6.25e-05 Total Training Loss:  1.8469775961420964\n",
      "Epoch 2388. Learning Rate: 6.25e-05 Total Training Loss:  1.8469966141274199\n",
      "Epoch 2389. Learning Rate: 6.25e-05 Total Training Loss:  1.8466374323470518\n",
      "Epoch 2390. Learning Rate: 6.25e-05 Total Training Loss:  1.8468621533247642\n",
      "Epoch 2391. Learning Rate: 6.25e-05 Total Training Loss:  1.846911231026752\n",
      "Epoch 2392. Learning Rate: 6.25e-05 Total Training Loss:  1.846736764156958\n",
      "Epoch 2393. Learning Rate: 6.25e-05 Total Training Loss:  1.846807685593376\n",
      "Epoch 2394. Learning Rate: 6.25e-05 Total Training Loss:  1.8469416489824653\n",
      "Epoch 2395. Learning Rate: 6.25e-05 Total Training Loss:  1.8461778487253468\n",
      "Epoch 2396. Learning Rate: 6.25e-05 Total Training Loss:  1.846475196449319\n",
      "Epoch 2397. Learning Rate: 6.25e-05 Total Training Loss:  1.8466678590048105\n",
      "Epoch 2398. Learning Rate: 6.25e-05 Total Training Loss:  1.8460720560105983\n",
      "Epoch 2399. Learning Rate: 6.25e-05 Total Training Loss:  1.8464220437163021\n",
      "Epoch 2400. Learning Rate: 6.25e-05 Total Training Loss:  1.846075977286091\n",
      "Epoch 2401. Learning Rate: 6.25e-05 Total Training Loss:  1.8460628390021157\n",
      "Epoch 2402. Learning Rate: 6.25e-05 Total Training Loss:  1.8465732390177436\n",
      "Epoch 2403. Learning Rate: 6.25e-05 Total Training Loss:  1.8462528773816302\n",
      "Epoch 2404. Learning Rate: 6.25e-05 Total Training Loss:  1.846090296516195\n",
      "Epoch 2405. Learning Rate: 6.25e-05 Total Training Loss:  1.845969244808657\n",
      "Epoch 2406. Learning Rate: 6.25e-05 Total Training Loss:  1.8459460833691992\n",
      "Epoch 2407. Learning Rate: 6.25e-05 Total Training Loss:  1.8456142498471308\n",
      "Epoch 2408. Learning Rate: 6.25e-05 Total Training Loss:  1.846173104509944\n",
      "Epoch 2409. Learning Rate: 6.25e-05 Total Training Loss:  1.845682692277478\n",
      "Epoch 2410. Learning Rate: 6.25e-05 Total Training Loss:  1.846097495261347\n",
      "Epoch 2411. Learning Rate: 6.25e-05 Total Training Loss:  1.8460165935393889\n",
      "Epoch 2412. Learning Rate: 6.25e-05 Total Training Loss:  1.8454726547061\n",
      "Epoch 2413. Learning Rate: 6.25e-05 Total Training Loss:  1.845434103073785\n",
      "Epoch 2414. Learning Rate: 6.25e-05 Total Training Loss:  1.845122255559545\n",
      "Epoch 2415. Learning Rate: 6.25e-05 Total Training Loss:  1.8460314279072918\n",
      "Epoch 2416. Learning Rate: 6.25e-05 Total Training Loss:  1.845146732754074\n",
      "Epoch 2417. Learning Rate: 6.25e-05 Total Training Loss:  1.845399209909374\n",
      "Epoch 2418. Learning Rate: 6.25e-05 Total Training Loss:  1.845085426029982\n",
      "Epoch 2419. Learning Rate: 6.25e-05 Total Training Loss:  1.8456285128195304\n",
      "Epoch 2420. Learning Rate: 6.25e-05 Total Training Loss:  1.8447142242221162\n",
      "Epoch 2421. Learning Rate: 6.25e-05 Total Training Loss:  1.845577857166063\n",
      "Epoch 2422. Learning Rate: 6.25e-05 Total Training Loss:  1.8449766272096895\n",
      "Epoch 2423. Learning Rate: 6.25e-05 Total Training Loss:  1.8452303386293352\n",
      "Epoch 2424. Learning Rate: 6.25e-05 Total Training Loss:  1.8449302129156422\n",
      "Epoch 2425. Learning Rate: 6.25e-05 Total Training Loss:  1.8446435852965806\n",
      "Epoch 2426. Learning Rate: 6.25e-05 Total Training Loss:  1.844846324500395\n",
      "Epoch 2427. Learning Rate: 6.25e-05 Total Training Loss:  1.845156311057508\n",
      "Epoch 2428. Learning Rate: 6.25e-05 Total Training Loss:  1.8451047687849496\n",
      "Epoch 2429. Learning Rate: 6.25e-05 Total Training Loss:  1.8446318820060696\n",
      "Epoch 2430. Learning Rate: 6.25e-05 Total Training Loss:  1.8446009614272043\n",
      "Epoch 2431. Learning Rate: 6.25e-05 Total Training Loss:  1.8446889568003826\n",
      "Epoch 2432. Learning Rate: 6.25e-05 Total Training Loss:  1.8447216151689645\n",
      "Epoch 2433. Learning Rate: 6.25e-05 Total Training Loss:  1.8443186584336217\n",
      "Epoch 2434. Learning Rate: 6.25e-05 Total Training Loss:  1.8445427254482638\n",
      "Epoch 2435. Learning Rate: 6.25e-05 Total Training Loss:  1.8441313144867308\n",
      "Epoch 2436. Learning Rate: 6.25e-05 Total Training Loss:  1.8442821739590727\n",
      "Epoch 2437. Learning Rate: 6.25e-05 Total Training Loss:  1.8440868871111888\n",
      "Epoch 2438. Learning Rate: 6.25e-05 Total Training Loss:  1.8441947125538718\n",
      "Epoch 2439. Learning Rate: 6.25e-05 Total Training Loss:  1.844361092604231\n",
      "Epoch 2440. Learning Rate: 6.25e-05 Total Training Loss:  1.8442511392349843\n",
      "Epoch 2441. Learning Rate: 6.25e-05 Total Training Loss:  1.8437813637428917\n",
      "Epoch 2442. Learning Rate: 6.25e-05 Total Training Loss:  1.844498864171328\n",
      "Epoch 2443. Learning Rate: 6.25e-05 Total Training Loss:  1.8438542479707394\n",
      "Epoch 2444. Learning Rate: 6.25e-05 Total Training Loss:  1.8443891131610144\n",
      "Epoch 2445. Learning Rate: 6.25e-05 Total Training Loss:  1.8439065227285028\n",
      "Epoch 2446. Learning Rate: 6.25e-05 Total Training Loss:  1.8439289007801563\n",
      "Epoch 2447. Learning Rate: 6.25e-05 Total Training Loss:  1.8435367018682882\n",
      "Epoch 2448. Learning Rate: 6.25e-05 Total Training Loss:  1.8438429321686272\n",
      "Epoch 2449. Learning Rate: 6.25e-05 Total Training Loss:  1.843333041499136\n",
      "Epoch 2450. Learning Rate: 6.25e-05 Total Training Loss:  1.8438407463836484\n",
      "Epoch 2451. Learning Rate: 6.25e-05 Total Training Loss:  1.843224898650078\n",
      "Epoch 2452. Learning Rate: 6.25e-05 Total Training Loss:  1.843833551480202\n",
      "Epoch 2453. Learning Rate: 6.25e-05 Total Training Loss:  1.843639377708314\n",
      "Epoch 2454. Learning Rate: 6.25e-05 Total Training Loss:  1.8428516021522228\n",
      "Epoch 2455. Learning Rate: 6.25e-05 Total Training Loss:  1.84421535328147\n",
      "Epoch 2456. Learning Rate: 6.25e-05 Total Training Loss:  1.8427288054372184\n",
      "Epoch 2457. Learning Rate: 6.25e-05 Total Training Loss:  1.843339357554214\n",
      "Epoch 2458. Learning Rate: 6.25e-05 Total Training Loss:  1.8427897970250342\n",
      "Epoch 2459. Learning Rate: 6.25e-05 Total Training Loss:  1.843710994173307\n",
      "Epoch 2460. Learning Rate: 6.25e-05 Total Training Loss:  1.8428725739941\n",
      "Epoch 2461. Learning Rate: 6.25e-05 Total Training Loss:  1.843098046228988\n",
      "Epoch 2462. Learning Rate: 6.25e-05 Total Training Loss:  1.8425042417948134\n",
      "Epoch 2463. Learning Rate: 6.25e-05 Total Training Loss:  1.8432923670334276\n",
      "Epoch 2464. Learning Rate: 6.25e-05 Total Training Loss:  1.843282158835791\n",
      "Epoch 2465. Learning Rate: 6.25e-05 Total Training Loss:  1.8424693238048349\n",
      "Epoch 2466. Learning Rate: 6.25e-05 Total Training Loss:  1.8429115324106533\n",
      "Epoch 2467. Learning Rate: 6.25e-05 Total Training Loss:  1.8432914229342714\n",
      "Epoch 2468. Learning Rate: 6.25e-05 Total Training Loss:  1.8430178221606184\n",
      "Epoch 2469. Learning Rate: 6.25e-05 Total Training Loss:  1.842470205272548\n",
      "Epoch 2470. Learning Rate: 6.25e-05 Total Training Loss:  1.8423796460265294\n",
      "Epoch 2471. Learning Rate: 6.25e-05 Total Training Loss:  1.8426247421011794\n",
      "Epoch 2472. Learning Rate: 6.25e-05 Total Training Loss:  1.8422061888850294\n",
      "Epoch 2473. Learning Rate: 6.25e-05 Total Training Loss:  1.8421850475715473\n",
      "Epoch 2474. Learning Rate: 6.25e-05 Total Training Loss:  1.8428637550678104\n",
      "Epoch 2475. Learning Rate: 6.25e-05 Total Training Loss:  1.8419632553705014\n",
      "Epoch 2476. Learning Rate: 6.25e-05 Total Training Loss:  1.8424158869602252\n",
      "Epoch 2477. Learning Rate: 6.25e-05 Total Training Loss:  1.8425288356957026\n",
      "Epoch 2478. Learning Rate: 6.25e-05 Total Training Loss:  1.8420530364091974\n",
      "Epoch 2479. Learning Rate: 6.25e-05 Total Training Loss:  1.841970401670551\n",
      "Epoch 2480. Learning Rate: 6.25e-05 Total Training Loss:  1.8426187134173233\n",
      "Epoch 2481. Learning Rate: 6.25e-05 Total Training Loss:  1.8415636954305228\n",
      "Epoch 2482. Learning Rate: 6.25e-05 Total Training Loss:  1.8424109853513073\n",
      "Epoch 2483. Learning Rate: 6.25e-05 Total Training Loss:  1.8413189374259673\n",
      "Epoch 2484. Learning Rate: 6.25e-05 Total Training Loss:  1.8420792962133419\n",
      "Epoch 2485. Learning Rate: 6.25e-05 Total Training Loss:  1.841493923071539\n",
      "Epoch 2486. Learning Rate: 6.25e-05 Total Training Loss:  1.8417044005182106\n",
      "Epoch 2487. Learning Rate: 6.25e-05 Total Training Loss:  1.8419890831864905\n",
      "Epoch 2488. Learning Rate: 6.25e-05 Total Training Loss:  1.8412569140200503\n",
      "Epoch 2489. Learning Rate: 6.25e-05 Total Training Loss:  1.8416597142640967\n",
      "Epoch 2490. Learning Rate: 6.25e-05 Total Training Loss:  1.8414521341619547\n",
      "Epoch 2491. Learning Rate: 6.25e-05 Total Training Loss:  1.8413709152373485\n",
      "Epoch 2492. Learning Rate: 6.25e-05 Total Training Loss:  1.8419629580166657\n",
      "Epoch 2493. Learning Rate: 6.25e-05 Total Training Loss:  1.8409474361978937\n",
      "Epoch 2494. Learning Rate: 6.25e-05 Total Training Loss:  1.8414389599929564\n",
      "Epoch 2495. Learning Rate: 6.25e-05 Total Training Loss:  1.8414230215130374\n",
      "Epoch 2496. Learning Rate: 6.25e-05 Total Training Loss:  1.8412437575461809\n",
      "Epoch 2497. Learning Rate: 6.25e-05 Total Training Loss:  1.8410672643803991\n",
      "Epoch 2498. Learning Rate: 6.25e-05 Total Training Loss:  1.8409911587368697\n",
      "Epoch 2499. Learning Rate: 6.25e-05 Total Training Loss:  1.8415486814046744\n",
      "Epoch 2500. Learning Rate: 3.125e-05 Total Training Loss:  1.8411504965624772\n",
      "Epoch 2501. Learning Rate: 3.125e-05 Total Training Loss:  1.8330721640959382\n",
      "Epoch 2502. Learning Rate: 3.125e-05 Total Training Loss:  1.8306271134351846\n",
      "Epoch 2503. Learning Rate: 3.125e-05 Total Training Loss:  1.8298713475523982\n",
      "Epoch 2504. Learning Rate: 3.125e-05 Total Training Loss:  1.8296733405150007\n",
      "Epoch 2505. Learning Rate: 3.125e-05 Total Training Loss:  1.8296745205589104\n",
      "Epoch 2506. Learning Rate: 3.125e-05 Total Training Loss:  1.829578905046219\n",
      "Epoch 2507. Learning Rate: 3.125e-05 Total Training Loss:  1.8294835872657131\n",
      "Epoch 2508. Learning Rate: 3.125e-05 Total Training Loss:  1.829372687410796\n",
      "Epoch 2509. Learning Rate: 3.125e-05 Total Training Loss:  1.829470742319245\n",
      "Epoch 2510. Learning Rate: 3.125e-05 Total Training Loss:  1.8293232672731392\n",
      "Epoch 2511. Learning Rate: 3.125e-05 Total Training Loss:  1.8293835455551744\n",
      "Epoch 2512. Learning Rate: 3.125e-05 Total Training Loss:  1.8292717995645944\n",
      "Epoch 2513. Learning Rate: 3.125e-05 Total Training Loss:  1.8294432988914195\n",
      "Epoch 2514. Learning Rate: 3.125e-05 Total Training Loss:  1.8294330155476928\n",
      "Epoch 2515. Learning Rate: 3.125e-05 Total Training Loss:  1.8290067311609164\n",
      "Epoch 2516. Learning Rate: 3.125e-05 Total Training Loss:  1.8294465486251283\n",
      "Epoch 2517. Learning Rate: 3.125e-05 Total Training Loss:  1.8290007147879805\n",
      "Epoch 2518. Learning Rate: 3.125e-05 Total Training Loss:  1.8295216991682537\n",
      "Epoch 2519. Learning Rate: 3.125e-05 Total Training Loss:  1.8289244994230103\n",
      "Epoch 2520. Learning Rate: 3.125e-05 Total Training Loss:  1.8290189372200985\n",
      "Epoch 2521. Learning Rate: 3.125e-05 Total Training Loss:  1.8292220346338581\n",
      "Epoch 2522. Learning Rate: 3.125e-05 Total Training Loss:  1.8289729495882057\n",
      "Epoch 2523. Learning Rate: 3.125e-05 Total Training Loss:  1.829221937659895\n",
      "Epoch 2524. Learning Rate: 3.125e-05 Total Training Loss:  1.8289864228281658\n",
      "Epoch 2525. Learning Rate: 3.125e-05 Total Training Loss:  1.8288885327347089\n",
      "Epoch 2526. Learning Rate: 3.125e-05 Total Training Loss:  1.8289754700963385\n",
      "Epoch 2527. Learning Rate: 3.125e-05 Total Training Loss:  1.8290911433578003\n",
      "Epoch 2528. Learning Rate: 3.125e-05 Total Training Loss:  1.8287638932524715\n",
      "Epoch 2529. Learning Rate: 3.125e-05 Total Training Loss:  1.8287567797815427\n",
      "Epoch 2530. Learning Rate: 3.125e-05 Total Training Loss:  1.8287457767874002\n",
      "Epoch 2531. Learning Rate: 3.125e-05 Total Training Loss:  1.8288994413160253\n",
      "Epoch 2532. Learning Rate: 3.125e-05 Total Training Loss:  1.8287459945422597\n",
      "Epoch 2533. Learning Rate: 3.125e-05 Total Training Loss:  1.8288475342851598\n",
      "Epoch 2534. Learning Rate: 3.125e-05 Total Training Loss:  1.8288010257820133\n",
      "Epoch 2535. Learning Rate: 3.125e-05 Total Training Loss:  1.8285357439599466\n",
      "Epoch 2536. Learning Rate: 3.125e-05 Total Training Loss:  1.8288083926890977\n",
      "Epoch 2537. Learning Rate: 3.125e-05 Total Training Loss:  1.828665213775821\n",
      "Epoch 2538. Learning Rate: 3.125e-05 Total Training Loss:  1.8286002993118018\n",
      "Epoch 2539. Learning Rate: 3.125e-05 Total Training Loss:  1.8285052186110988\n",
      "Epoch 2540. Learning Rate: 3.125e-05 Total Training Loss:  1.8286306468653493\n",
      "Epoch 2541. Learning Rate: 3.125e-05 Total Training Loss:  1.8283740068727639\n",
      "Epoch 2542. Learning Rate: 3.125e-05 Total Training Loss:  1.8287892725784332\n",
      "Epoch 2543. Learning Rate: 3.125e-05 Total Training Loss:  1.828383056417806\n",
      "Epoch 2544. Learning Rate: 3.125e-05 Total Training Loss:  1.828557863511378\n",
      "Epoch 2545. Learning Rate: 3.125e-05 Total Training Loss:  1.8284182107890956\n",
      "Epoch 2546. Learning Rate: 3.125e-05 Total Training Loss:  1.82839040827821\n",
      "Epoch 2547. Learning Rate: 3.125e-05 Total Training Loss:  1.8283477084187325\n",
      "Epoch 2548. Learning Rate: 3.125e-05 Total Training Loss:  1.8282793871767353\n",
      "Epoch 2549. Learning Rate: 3.125e-05 Total Training Loss:  1.8282741453149356\n",
      "Epoch 2550. Learning Rate: 3.125e-05 Total Training Loss:  1.8282111291773617\n",
      "Epoch 2551. Learning Rate: 3.125e-05 Total Training Loss:  1.828348427428864\n",
      "Epoch 2552. Learning Rate: 3.125e-05 Total Training Loss:  1.828195028792834\n",
      "Epoch 2553. Learning Rate: 3.125e-05 Total Training Loss:  1.8284045534092002\n",
      "Epoch 2554. Learning Rate: 3.125e-05 Total Training Loss:  1.8281421173887793\n",
      "Epoch 2555. Learning Rate: 3.125e-05 Total Training Loss:  1.828097093733959\n",
      "Epoch 2556. Learning Rate: 3.125e-05 Total Training Loss:  1.8281414376106113\n",
      "Epoch 2557. Learning Rate: 3.125e-05 Total Training Loss:  1.8279446725791786\n",
      "Epoch 2558. Learning Rate: 3.125e-05 Total Training Loss:  1.8283876783389132\n",
      "Epoch 2559. Learning Rate: 3.125e-05 Total Training Loss:  1.8279161643295083\n",
      "Epoch 2560. Learning Rate: 3.125e-05 Total Training Loss:  1.828042058419669\n",
      "Epoch 2561. Learning Rate: 3.125e-05 Total Training Loss:  1.8279628003074322\n",
      "Epoch 2562. Learning Rate: 3.125e-05 Total Training Loss:  1.8280610352521762\n",
      "Epoch 2563. Learning Rate: 3.125e-05 Total Training Loss:  1.8278439569694456\n",
      "Epoch 2564. Learning Rate: 3.125e-05 Total Training Loss:  1.828020718676271\n",
      "Epoch 2565. Learning Rate: 3.125e-05 Total Training Loss:  1.82786527913413\n",
      "Epoch 2566. Learning Rate: 3.125e-05 Total Training Loss:  1.8276890700799413\n",
      "Epoch 2567. Learning Rate: 3.125e-05 Total Training Loss:  1.827849223074736\n",
      "Epoch 2568. Learning Rate: 3.125e-05 Total Training Loss:  1.8278805825975724\n",
      "Epoch 2569. Learning Rate: 3.125e-05 Total Training Loss:  1.8278102400363423\n",
      "Epoch 2570. Learning Rate: 3.125e-05 Total Training Loss:  1.8278505413327366\n",
      "Epoch 2571. Learning Rate: 3.125e-05 Total Training Loss:  1.8274430616293103\n",
      "Epoch 2572. Learning Rate: 3.125e-05 Total Training Loss:  1.8278909574146383\n",
      "Epoch 2573. Learning Rate: 3.125e-05 Total Training Loss:  1.827652307023527\n",
      "Epoch 2574. Learning Rate: 3.125e-05 Total Training Loss:  1.8276996007480193\n",
      "Epoch 2575. Learning Rate: 3.125e-05 Total Training Loss:  1.8274350865685847\n",
      "Epoch 2576. Learning Rate: 3.125e-05 Total Training Loss:  1.8277460313402116\n",
      "Epoch 2577. Learning Rate: 3.125e-05 Total Training Loss:  1.82733123775688\n",
      "Epoch 2578. Learning Rate: 3.125e-05 Total Training Loss:  1.8277954219665844\n",
      "Epoch 2579. Learning Rate: 3.125e-05 Total Training Loss:  1.8274743205111008\n",
      "Epoch 2580. Learning Rate: 3.125e-05 Total Training Loss:  1.8273429028049577\n",
      "Epoch 2581. Learning Rate: 3.125e-05 Total Training Loss:  1.827636346744839\n",
      "Epoch 2582. Learning Rate: 3.125e-05 Total Training Loss:  1.8273330615193117\n",
      "Epoch 2583. Learning Rate: 3.125e-05 Total Training Loss:  1.8276217839156743\n",
      "Epoch 2584. Learning Rate: 3.125e-05 Total Training Loss:  1.8272026398626622\n",
      "Epoch 2585. Learning Rate: 3.125e-05 Total Training Loss:  1.8275741102406755\n",
      "Epoch 2586. Learning Rate: 3.125e-05 Total Training Loss:  1.8273637348320335\n",
      "Epoch 2587. Learning Rate: 3.125e-05 Total Training Loss:  1.8271828522265423\n",
      "Epoch 2588. Learning Rate: 3.125e-05 Total Training Loss:  1.8273275662213564\n",
      "Epoch 2589. Learning Rate: 3.125e-05 Total Training Loss:  1.82730203459505\n",
      "Epoch 2590. Learning Rate: 3.125e-05 Total Training Loss:  1.8271695628354792\n",
      "Epoch 2591. Learning Rate: 3.125e-05 Total Training Loss:  1.8271673540293705\n",
      "Epoch 2592. Learning Rate: 3.125e-05 Total Training Loss:  1.8271980764111504\n",
      "Epoch 2593. Learning Rate: 3.125e-05 Total Training Loss:  1.8268415884231217\n",
      "Epoch 2594. Learning Rate: 3.125e-05 Total Training Loss:  1.8271552725927904\n",
      "Epoch 2595. Learning Rate: 3.125e-05 Total Training Loss:  1.82698096925742\n",
      "Epoch 2596. Learning Rate: 3.125e-05 Total Training Loss:  1.827320055279415\n",
      "Epoch 2597. Learning Rate: 3.125e-05 Total Training Loss:  1.826868258183822\n",
      "Epoch 2598. Learning Rate: 3.125e-05 Total Training Loss:  1.827197957871249\n",
      "Epoch 2599. Learning Rate: 3.125e-05 Total Training Loss:  1.8270709262578748\n",
      "Epoch 2600. Learning Rate: 3.125e-05 Total Training Loss:  1.827140403271187\n",
      "Epoch 2601. Learning Rate: 3.125e-05 Total Training Loss:  1.8269753158674575\n",
      "Epoch 2602. Learning Rate: 3.125e-05 Total Training Loss:  1.8270555804483593\n",
      "Epoch 2603. Learning Rate: 3.125e-05 Total Training Loss:  1.826586976822\n",
      "Epoch 2604. Learning Rate: 3.125e-05 Total Training Loss:  1.8270889756968245\n",
      "Epoch 2605. Learning Rate: 3.125e-05 Total Training Loss:  1.8268474009819329\n",
      "Epoch 2606. Learning Rate: 3.125e-05 Total Training Loss:  1.8266915999702178\n",
      "Epoch 2607. Learning Rate: 3.125e-05 Total Training Loss:  1.8267306890920736\n",
      "Epoch 2608. Learning Rate: 3.125e-05 Total Training Loss:  1.8267825621296652\n",
      "Epoch 2609. Learning Rate: 3.125e-05 Total Training Loss:  1.8266677632345818\n",
      "Epoch 2610. Learning Rate: 3.125e-05 Total Training Loss:  1.8267255908285733\n",
      "Epoch 2611. Learning Rate: 3.125e-05 Total Training Loss:  1.8268062831484713\n",
      "Epoch 2612. Learning Rate: 3.125e-05 Total Training Loss:  1.8266058313311078\n",
      "Epoch 2613. Learning Rate: 3.125e-05 Total Training Loss:  1.8265209144446999\n",
      "Epoch 2614. Learning Rate: 3.125e-05 Total Training Loss:  1.8269560401968192\n",
      "Epoch 2615. Learning Rate: 3.125e-05 Total Training Loss:  1.8263806780450977\n",
      "Epoch 2616. Learning Rate: 3.125e-05 Total Training Loss:  1.8266654593753628\n",
      "Epoch 2617. Learning Rate: 3.125e-05 Total Training Loss:  1.8264692864613608\n",
      "Epoch 2618. Learning Rate: 3.125e-05 Total Training Loss:  1.8266503066406585\n",
      "Epoch 2619. Learning Rate: 3.125e-05 Total Training Loss:  1.8264585650758818\n",
      "Epoch 2620. Learning Rate: 3.125e-05 Total Training Loss:  1.8263981228228658\n",
      "Epoch 2621. Learning Rate: 3.125e-05 Total Training Loss:  1.8265429498860613\n",
      "Epoch 2622. Learning Rate: 3.125e-05 Total Training Loss:  1.8263912083930336\n",
      "Epoch 2623. Learning Rate: 3.125e-05 Total Training Loss:  1.8262750992143992\n",
      "Epoch 2624. Learning Rate: 3.125e-05 Total Training Loss:  1.8262883051647805\n",
      "Epoch 2625. Learning Rate: 3.125e-05 Total Training Loss:  1.826377017365303\n",
      "Epoch 2626. Learning Rate: 3.125e-05 Total Training Loss:  1.826348454138497\n",
      "Epoch 2627. Learning Rate: 3.125e-05 Total Training Loss:  1.8262946156901307\n",
      "Epoch 2628. Learning Rate: 3.125e-05 Total Training Loss:  1.8261393909633625\n",
      "Epoch 2629. Learning Rate: 3.125e-05 Total Training Loss:  1.8262737418117467\n",
      "Epoch 2630. Learning Rate: 3.125e-05 Total Training Loss:  1.8261295236588921\n",
      "Epoch 2631. Learning Rate: 3.125e-05 Total Training Loss:  1.8261114482593257\n",
      "Epoch 2632. Learning Rate: 3.125e-05 Total Training Loss:  1.8261316262651235\n",
      "Epoch 2633. Learning Rate: 3.125e-05 Total Training Loss:  1.8261086606944446\n",
      "Epoch 2634. Learning Rate: 3.125e-05 Total Training Loss:  1.825953437917633\n",
      "Epoch 2635. Learning Rate: 3.125e-05 Total Training Loss:  1.8258165712468326\n",
      "Epoch 2636. Learning Rate: 3.125e-05 Total Training Loss:  1.8261769729433581\n",
      "Epoch 2637. Learning Rate: 3.125e-05 Total Training Loss:  1.8257351895736065\n",
      "Epoch 2638. Learning Rate: 3.125e-05 Total Training Loss:  1.825910827203188\n",
      "Epoch 2639. Learning Rate: 3.125e-05 Total Training Loss:  1.8257217126956675\n",
      "Epoch 2640. Learning Rate: 3.125e-05 Total Training Loss:  1.8258810034603812\n",
      "Epoch 2641. Learning Rate: 3.125e-05 Total Training Loss:  1.825738621817436\n",
      "Epoch 2642. Learning Rate: 3.125e-05 Total Training Loss:  1.8256615154386964\n",
      "Epoch 2643. Learning Rate: 3.125e-05 Total Training Loss:  1.825644721655408\n",
      "Epoch 2644. Learning Rate: 3.125e-05 Total Training Loss:  1.825598594470648\n",
      "Epoch 2645. Learning Rate: 3.125e-05 Total Training Loss:  1.8258013052982278\n",
      "Epoch 2646. Learning Rate: 3.125e-05 Total Training Loss:  1.8253947288321797\n",
      "Epoch 2647. Learning Rate: 3.125e-05 Total Training Loss:  1.8257839630241506\n",
      "Epoch 2648. Learning Rate: 3.125e-05 Total Training Loss:  1.8253597117436584\n",
      "Epoch 2649. Learning Rate: 3.125e-05 Total Training Loss:  1.8254245841235388\n",
      "Epoch 2650. Learning Rate: 3.125e-05 Total Training Loss:  1.8253692632133607\n",
      "Epoch 2651. Learning Rate: 3.125e-05 Total Training Loss:  1.8253888939507306\n",
      "Epoch 2652. Learning Rate: 3.125e-05 Total Training Loss:  1.825294342415873\n",
      "Epoch 2653. Learning Rate: 3.125e-05 Total Training Loss:  1.825286638486432\n",
      "Epoch 2654. Learning Rate: 3.125e-05 Total Training Loss:  1.825383124203654\n",
      "Epoch 2655. Learning Rate: 3.125e-05 Total Training Loss:  1.8251383664028253\n",
      "Epoch 2656. Learning Rate: 3.125e-05 Total Training Loss:  1.8252620666171424\n",
      "Epoch 2657. Learning Rate: 3.125e-05 Total Training Loss:  1.825309038220439\n",
      "Epoch 2658. Learning Rate: 3.125e-05 Total Training Loss:  1.8249082249240018\n",
      "Epoch 2659. Learning Rate: 3.125e-05 Total Training Loss:  1.825125719391508\n",
      "Epoch 2660. Learning Rate: 3.125e-05 Total Training Loss:  1.8250548430660274\n",
      "Epoch 2661. Learning Rate: 3.125e-05 Total Training Loss:  1.8249837708135601\n",
      "Epoch 2662. Learning Rate: 3.125e-05 Total Training Loss:  1.82503566678497\n",
      "Epoch 2663. Learning Rate: 3.125e-05 Total Training Loss:  1.824956693337299\n",
      "Epoch 2664. Learning Rate: 3.125e-05 Total Training Loss:  1.8249806033272762\n",
      "Epoch 2665. Learning Rate: 3.125e-05 Total Training Loss:  1.8248664749262389\n",
      "Epoch 2666. Learning Rate: 3.125e-05 Total Training Loss:  1.824801081820624\n",
      "Epoch 2667. Learning Rate: 3.125e-05 Total Training Loss:  1.8248801580630243\n",
      "Epoch 2668. Learning Rate: 3.125e-05 Total Training Loss:  1.8246624598104972\n",
      "Epoch 2669. Learning Rate: 3.125e-05 Total Training Loss:  1.8244831636257004\n",
      "Epoch 2670. Learning Rate: 3.125e-05 Total Training Loss:  1.8247135003621224\n",
      "Epoch 2671. Learning Rate: 3.125e-05 Total Training Loss:  1.824587821844034\n",
      "Epoch 2672. Learning Rate: 3.125e-05 Total Training Loss:  1.8245727156754583\n",
      "Epoch 2673. Learning Rate: 3.125e-05 Total Training Loss:  1.8241640513588209\n",
      "Epoch 2674. Learning Rate: 3.125e-05 Total Training Loss:  1.8248242237896193\n",
      "Epoch 2675. Learning Rate: 3.125e-05 Total Training Loss:  1.8243810891581234\n",
      "Epoch 2676. Learning Rate: 3.125e-05 Total Training Loss:  1.8246801281056833\n",
      "Epoch 2677. Learning Rate: 3.125e-05 Total Training Loss:  1.8243856457411312\n",
      "Epoch 2678. Learning Rate: 3.125e-05 Total Training Loss:  1.8243071727629285\n",
      "Epoch 2679. Learning Rate: 3.125e-05 Total Training Loss:  1.824160508433124\n",
      "Epoch 2680. Learning Rate: 3.125e-05 Total Training Loss:  1.8242816178535577\n",
      "Epoch 2681. Learning Rate: 3.125e-05 Total Training Loss:  1.8243922723922879\n",
      "Epoch 2682. Learning Rate: 3.125e-05 Total Training Loss:  1.8240791718126275\n",
      "Epoch 2683. Learning Rate: 3.125e-05 Total Training Loss:  1.8243760514887981\n",
      "Epoch 2684. Learning Rate: 3.125e-05 Total Training Loss:  1.8240069332823623\n",
      "Epoch 2685. Learning Rate: 3.125e-05 Total Training Loss:  1.8241736713680439\n",
      "Epoch 2686. Learning Rate: 3.125e-05 Total Training Loss:  1.8239770657964982\n",
      "Epoch 2687. Learning Rate: 3.125e-05 Total Training Loss:  1.823893508350011\n",
      "Epoch 2688. Learning Rate: 3.125e-05 Total Training Loss:  1.823969775257865\n",
      "Epoch 2689. Learning Rate: 3.125e-05 Total Training Loss:  1.8241992444673087\n",
      "Epoch 2690. Learning Rate: 3.125e-05 Total Training Loss:  1.8237791511055548\n",
      "Epoch 2691. Learning Rate: 3.125e-05 Total Training Loss:  1.8240668298094533\n",
      "Epoch 2692. Learning Rate: 3.125e-05 Total Training Loss:  1.823881151038222\n",
      "Epoch 2693. Learning Rate: 3.125e-05 Total Training Loss:  1.8238374174688943\n",
      "Epoch 2694. Learning Rate: 3.125e-05 Total Training Loss:  1.8237362022628076\n",
      "Epoch 2695. Learning Rate: 3.125e-05 Total Training Loss:  1.8237973150971811\n",
      "Epoch 2696. Learning Rate: 3.125e-05 Total Training Loss:  1.8239403955230955\n",
      "Epoch 2697. Learning Rate: 3.125e-05 Total Training Loss:  1.8234753789147362\n",
      "Epoch 2698. Learning Rate: 3.125e-05 Total Training Loss:  1.8236967281263787\n",
      "Epoch 2699. Learning Rate: 3.125e-05 Total Training Loss:  1.8235399821714964\n",
      "Epoch 2700. Learning Rate: 3.125e-05 Total Training Loss:  1.8235668525157962\n",
      "Epoch 2701. Learning Rate: 3.125e-05 Total Training Loss:  1.8234525442530867\n",
      "Epoch 2702. Learning Rate: 3.125e-05 Total Training Loss:  1.8237306484370492\n",
      "Epoch 2703. Learning Rate: 3.125e-05 Total Training Loss:  1.823493836185662\n",
      "Epoch 2704. Learning Rate: 3.125e-05 Total Training Loss:  1.8234822801605333\n",
      "Epoch 2705. Learning Rate: 3.125e-05 Total Training Loss:  1.8233899724727962\n",
      "Epoch 2706. Learning Rate: 3.125e-05 Total Training Loss:  1.823502535233274\n",
      "Epoch 2707. Learning Rate: 3.125e-05 Total Training Loss:  1.8234202025923878\n",
      "Epoch 2708. Learning Rate: 3.125e-05 Total Training Loss:  1.8232884176832158\n",
      "Epoch 2709. Learning Rate: 3.125e-05 Total Training Loss:  1.8233547648706008\n",
      "Epoch 2710. Learning Rate: 3.125e-05 Total Training Loss:  1.8233095656032674\n",
      "Epoch 2711. Learning Rate: 3.125e-05 Total Training Loss:  1.8232068338256795\n",
      "Epoch 2712. Learning Rate: 3.125e-05 Total Training Loss:  1.8231164652388543\n",
      "Epoch 2713. Learning Rate: 3.125e-05 Total Training Loss:  1.8233659154793713\n",
      "Epoch 2714. Learning Rate: 3.125e-05 Total Training Loss:  1.822926743188873\n",
      "Epoch 2715. Learning Rate: 3.125e-05 Total Training Loss:  1.823234303650679\n",
      "Epoch 2716. Learning Rate: 3.125e-05 Total Training Loss:  1.8229840130370576\n",
      "Epoch 2717. Learning Rate: 3.125e-05 Total Training Loss:  1.8232583496719599\n",
      "Epoch 2718. Learning Rate: 3.125e-05 Total Training Loss:  1.822972244612174\n",
      "Epoch 2719. Learning Rate: 3.125e-05 Total Training Loss:  1.8229069849767257\n",
      "Epoch 2720. Learning Rate: 3.125e-05 Total Training Loss:  1.8228848920261953\n",
      "Epoch 2721. Learning Rate: 3.125e-05 Total Training Loss:  1.8229794581711758\n",
      "Epoch 2722. Learning Rate: 3.125e-05 Total Training Loss:  1.822819049411919\n",
      "Epoch 2723. Learning Rate: 3.125e-05 Total Training Loss:  1.8230232748901471\n",
      "Epoch 2724. Learning Rate: 3.125e-05 Total Training Loss:  1.8227423906209879\n",
      "Epoch 2725. Learning Rate: 3.125e-05 Total Training Loss:  1.8227297215198632\n",
      "Epoch 2726. Learning Rate: 3.125e-05 Total Training Loss:  1.8228311556449626\n",
      "Epoch 2727. Learning Rate: 3.125e-05 Total Training Loss:  1.8226792600180488\n",
      "Epoch 2728. Learning Rate: 3.125e-05 Total Training Loss:  1.8225655273417942\n",
      "Epoch 2729. Learning Rate: 3.125e-05 Total Training Loss:  1.822657674580114\n",
      "Epoch 2730. Learning Rate: 3.125e-05 Total Training Loss:  1.8224890010314994\n",
      "Epoch 2731. Learning Rate: 3.125e-05 Total Training Loss:  1.8229318587691523\n",
      "Epoch 2732. Learning Rate: 3.125e-05 Total Training Loss:  1.8226583281357307\n",
      "Epoch 2733. Learning Rate: 3.125e-05 Total Training Loss:  1.8225498687825166\n",
      "Epoch 2734. Learning Rate: 3.125e-05 Total Training Loss:  1.8224118637444917\n",
      "Epoch 2735. Learning Rate: 3.125e-05 Total Training Loss:  1.8226098688901402\n",
      "Epoch 2736. Learning Rate: 3.125e-05 Total Training Loss:  1.822558591782581\n",
      "Epoch 2737. Learning Rate: 3.125e-05 Total Training Loss:  1.8221819351019803\n",
      "Epoch 2738. Learning Rate: 3.125e-05 Total Training Loss:  1.8224621876433957\n",
      "Epoch 2739. Learning Rate: 3.125e-05 Total Training Loss:  1.8222272715356667\n",
      "Epoch 2740. Learning Rate: 3.125e-05 Total Training Loss:  1.822432858432876\n",
      "Epoch 2741. Learning Rate: 3.125e-05 Total Training Loss:  1.8222860237001441\n",
      "Epoch 2742. Learning Rate: 3.125e-05 Total Training Loss:  1.822264580114279\n",
      "Epoch 2743. Learning Rate: 3.125e-05 Total Training Loss:  1.822184274555184\n",
      "Epoch 2744. Learning Rate: 3.125e-05 Total Training Loss:  1.8224150502646808\n",
      "Epoch 2745. Learning Rate: 3.125e-05 Total Training Loss:  1.8220925141358748\n",
      "Epoch 2746. Learning Rate: 3.125e-05 Total Training Loss:  1.8223022062738892\n",
      "Epoch 2747. Learning Rate: 3.125e-05 Total Training Loss:  1.8220245034026448\n",
      "Epoch 2748. Learning Rate: 3.125e-05 Total Training Loss:  1.8222356206388213\n",
      "Epoch 2749. Learning Rate: 3.125e-05 Total Training Loss:  1.8220420824363828\n",
      "Epoch 2750. Learning Rate: 3.125e-05 Total Training Loss:  1.8218650954659097\n",
      "Epoch 2751. Learning Rate: 3.125e-05 Total Training Loss:  1.822088948538294\n",
      "Epoch 2752. Learning Rate: 3.125e-05 Total Training Loss:  1.8219616687274538\n",
      "Epoch 2753. Learning Rate: 3.125e-05 Total Training Loss:  1.8219266476517078\n",
      "Epoch 2754. Learning Rate: 3.125e-05 Total Training Loss:  1.8220241163508035\n",
      "Epoch 2755. Learning Rate: 3.125e-05 Total Training Loss:  1.8219607191567775\n",
      "Epoch 2756. Learning Rate: 3.125e-05 Total Training Loss:  1.8217631150619127\n",
      "Epoch 2757. Learning Rate: 3.125e-05 Total Training Loss:  1.821756223362172\n",
      "Epoch 2758. Learning Rate: 3.125e-05 Total Training Loss:  1.8220143042562995\n",
      "Epoch 2759. Learning Rate: 3.125e-05 Total Training Loss:  1.8217219652142376\n",
      "Epoch 2760. Learning Rate: 3.125e-05 Total Training Loss:  1.821660686226096\n",
      "Epoch 2761. Learning Rate: 3.125e-05 Total Training Loss:  1.8217511069087777\n",
      "Epoch 2762. Learning Rate: 3.125e-05 Total Training Loss:  1.821682459762087\n",
      "Epoch 2763. Learning Rate: 3.125e-05 Total Training Loss:  1.821559778851224\n",
      "Epoch 2764. Learning Rate: 3.125e-05 Total Training Loss:  1.8216001902765129\n",
      "Epoch 2765. Learning Rate: 3.125e-05 Total Training Loss:  1.8214928041852545\n",
      "Epoch 2766. Learning Rate: 3.125e-05 Total Training Loss:  1.8215553422342055\n",
      "Epoch 2767. Learning Rate: 3.125e-05 Total Training Loss:  1.8216641145409085\n",
      "Epoch 2768. Learning Rate: 3.125e-05 Total Training Loss:  1.8216846431023441\n",
      "Epoch 2769. Learning Rate: 3.125e-05 Total Training Loss:  1.8213604119955562\n",
      "Epoch 2770. Learning Rate: 3.125e-05 Total Training Loss:  1.821524839499034\n",
      "Epoch 2771. Learning Rate: 3.125e-05 Total Training Loss:  1.8215548279986251\n",
      "Epoch 2772. Learning Rate: 3.125e-05 Total Training Loss:  1.8213542893354315\n",
      "Epoch 2773. Learning Rate: 3.125e-05 Total Training Loss:  1.8214213834726252\n",
      "Epoch 2774. Learning Rate: 3.125e-05 Total Training Loss:  1.821309611084871\n",
      "Epoch 2775. Learning Rate: 3.125e-05 Total Training Loss:  1.8213555371330585\n",
      "Epoch 2776. Learning Rate: 3.125e-05 Total Training Loss:  1.8212028872512747\n",
      "Epoch 2777. Learning Rate: 3.125e-05 Total Training Loss:  1.821298816561466\n",
      "Epoch 2778. Learning Rate: 3.125e-05 Total Training Loss:  1.8209798454481643\n",
      "Epoch 2779. Learning Rate: 3.125e-05 Total Training Loss:  1.8213326486002188\n",
      "Epoch 2780. Learning Rate: 3.125e-05 Total Training Loss:  1.8212468858109787\n",
      "Epoch 2781. Learning Rate: 3.125e-05 Total Training Loss:  1.821008284430718\n",
      "Epoch 2782. Learning Rate: 3.125e-05 Total Training Loss:  1.8211244255653583\n",
      "Epoch 2783. Learning Rate: 3.125e-05 Total Training Loss:  1.8211041715112515\n",
      "Epoch 2784. Learning Rate: 3.125e-05 Total Training Loss:  1.820974483678583\n",
      "Epoch 2785. Learning Rate: 3.125e-05 Total Training Loss:  1.8207659978070296\n",
      "Epoch 2786. Learning Rate: 3.125e-05 Total Training Loss:  1.8212018496124074\n",
      "Epoch 2787. Learning Rate: 3.125e-05 Total Training Loss:  1.8208333964284975\n",
      "Epoch 2788. Learning Rate: 3.125e-05 Total Training Loss:  1.8208450973324943\n",
      "Epoch 2789. Learning Rate: 3.125e-05 Total Training Loss:  1.8207506951293908\n",
      "Epoch 2790. Learning Rate: 3.125e-05 Total Training Loss:  1.8205897251609713\n",
      "Epoch 2791. Learning Rate: 3.125e-05 Total Training Loss:  1.8211386375478469\n",
      "Epoch 2792. Learning Rate: 3.125e-05 Total Training Loss:  1.8207964104076382\n",
      "Epoch 2793. Learning Rate: 3.125e-05 Total Training Loss:  1.8206471894227434\n",
      "Epoch 2794. Learning Rate: 3.125e-05 Total Training Loss:  1.8206605424929876\n",
      "Epoch 2795. Learning Rate: 3.125e-05 Total Training Loss:  1.820609642949421\n",
      "Epoch 2796. Learning Rate: 3.125e-05 Total Training Loss:  1.8206720717425924\n",
      "Epoch 2797. Learning Rate: 3.125e-05 Total Training Loss:  1.8204405230062548\n",
      "Epoch 2798. Learning Rate: 3.125e-05 Total Training Loss:  1.8206718961300794\n",
      "Epoch 2799. Learning Rate: 3.125e-05 Total Training Loss:  1.8205097071477212\n",
      "Epoch 2800. Learning Rate: 3.125e-05 Total Training Loss:  1.8205114101001527\n",
      "Epoch 2801. Learning Rate: 3.125e-05 Total Training Loss:  1.8206044828984886\n",
      "Epoch 2802. Learning Rate: 3.125e-05 Total Training Loss:  1.8204447345342487\n",
      "Epoch 2803. Learning Rate: 3.125e-05 Total Training Loss:  1.8203089776507113\n",
      "Epoch 2804. Learning Rate: 3.125e-05 Total Training Loss:  1.8205337704275735\n",
      "Epoch 2805. Learning Rate: 3.125e-05 Total Training Loss:  1.8203102305124048\n",
      "Epoch 2806. Learning Rate: 3.125e-05 Total Training Loss:  1.820363451144658\n",
      "Epoch 2807. Learning Rate: 3.125e-05 Total Training Loss:  1.8202924304350745\n",
      "Epoch 2808. Learning Rate: 3.125e-05 Total Training Loss:  1.8201797522197012\n",
      "Epoch 2809. Learning Rate: 3.125e-05 Total Training Loss:  1.8204442789428867\n",
      "Epoch 2810. Learning Rate: 3.125e-05 Total Training Loss:  1.8202142968657427\n",
      "Epoch 2811. Learning Rate: 3.125e-05 Total Training Loss:  1.8200748531962745\n",
      "Epoch 2812. Learning Rate: 3.125e-05 Total Training Loss:  1.820209295139648\n",
      "Epoch 2813. Learning Rate: 3.125e-05 Total Training Loss:  1.8199532523867674\n",
      "Epoch 2814. Learning Rate: 3.125e-05 Total Training Loss:  1.82014929613797\n",
      "Epoch 2815. Learning Rate: 3.125e-05 Total Training Loss:  1.8199768827180378\n",
      "Epoch 2816. Learning Rate: 3.125e-05 Total Training Loss:  1.8202802106097806\n",
      "Epoch 2817. Learning Rate: 3.125e-05 Total Training Loss:  1.819904432806652\n",
      "Epoch 2818. Learning Rate: 3.125e-05 Total Training Loss:  1.819868801656412\n",
      "Epoch 2819. Learning Rate: 3.125e-05 Total Training Loss:  1.8197343315987382\n",
      "Epoch 2820. Learning Rate: 3.125e-05 Total Training Loss:  1.8202068780956324\n",
      "Epoch 2821. Learning Rate: 3.125e-05 Total Training Loss:  1.8196731535135768\n",
      "Epoch 2822. Learning Rate: 3.125e-05 Total Training Loss:  1.81981271202676\n",
      "Epoch 2823. Learning Rate: 3.125e-05 Total Training Loss:  1.8197410857246723\n",
      "Epoch 2824. Learning Rate: 3.125e-05 Total Training Loss:  1.8199985802930314\n",
      "Epoch 2825. Learning Rate: 3.125e-05 Total Training Loss:  1.8198754511540756\n",
      "Epoch 2826. Learning Rate: 3.125e-05 Total Training Loss:  1.8196745029999875\n",
      "Epoch 2827. Learning Rate: 3.125e-05 Total Training Loss:  1.8198537131538615\n",
      "Epoch 2828. Learning Rate: 3.125e-05 Total Training Loss:  1.8197371210262645\n",
      "Epoch 2829. Learning Rate: 3.125e-05 Total Training Loss:  1.8197203526215162\n",
      "Epoch 2830. Learning Rate: 3.125e-05 Total Training Loss:  1.8195579155581072\n",
      "Epoch 2831. Learning Rate: 3.125e-05 Total Training Loss:  1.8194081469264347\n",
      "Epoch 2832. Learning Rate: 3.125e-05 Total Training Loss:  1.8196049675752874\n",
      "Epoch 2833. Learning Rate: 3.125e-05 Total Training Loss:  1.8194869554718025\n",
      "Epoch 2834. Learning Rate: 3.125e-05 Total Training Loss:  1.8196693652425893\n",
      "Epoch 2835. Learning Rate: 3.125e-05 Total Training Loss:  1.8192854192457162\n",
      "Epoch 2836. Learning Rate: 3.125e-05 Total Training Loss:  1.8194416703190655\n",
      "Epoch 2837. Learning Rate: 3.125e-05 Total Training Loss:  1.8193024593929294\n",
      "Epoch 2838. Learning Rate: 3.125e-05 Total Training Loss:  1.8194923772534821\n",
      "Epoch 2839. Learning Rate: 3.125e-05 Total Training Loss:  1.8192476388067007\n",
      "Epoch 2840. Learning Rate: 3.125e-05 Total Training Loss:  1.819581834890414\n",
      "Epoch 2841. Learning Rate: 3.125e-05 Total Training Loss:  1.819431128562428\n",
      "Epoch 2842. Learning Rate: 3.125e-05 Total Training Loss:  1.8191467896394897\n",
      "Epoch 2843. Learning Rate: 3.125e-05 Total Training Loss:  1.8191492028126959\n",
      "Epoch 2844. Learning Rate: 3.125e-05 Total Training Loss:  1.819223334430717\n",
      "Epoch 2845. Learning Rate: 3.125e-05 Total Training Loss:  1.818926705251215\n",
      "Epoch 2846. Learning Rate: 3.125e-05 Total Training Loss:  1.8191216454724781\n",
      "Epoch 2847. Learning Rate: 3.125e-05 Total Training Loss:  1.8187137476052158\n",
      "Epoch 2848. Learning Rate: 3.125e-05 Total Training Loss:  1.8194301091425586\n",
      "Epoch 2849. Learning Rate: 3.125e-05 Total Training Loss:  1.8187563817191403\n",
      "Epoch 2850. Learning Rate: 3.125e-05 Total Training Loss:  1.818978597089881\n",
      "Epoch 2851. Learning Rate: 3.125e-05 Total Training Loss:  1.8188136027310975\n",
      "Epoch 2852. Learning Rate: 3.125e-05 Total Training Loss:  1.8190391712123528\n",
      "Epoch 2853. Learning Rate: 3.125e-05 Total Training Loss:  1.8187705937889405\n",
      "Epoch 2854. Learning Rate: 3.125e-05 Total Training Loss:  1.8188460007368121\n",
      "Epoch 2855. Learning Rate: 3.125e-05 Total Training Loss:  1.8186194050940685\n",
      "Epoch 2856. Learning Rate: 3.125e-05 Total Training Loss:  1.8189185471564997\n",
      "Epoch 2857. Learning Rate: 3.125e-05 Total Training Loss:  1.8184328661882319\n",
      "Epoch 2858. Learning Rate: 3.125e-05 Total Training Loss:  1.8188641231972724\n",
      "Epoch 2859. Learning Rate: 3.125e-05 Total Training Loss:  1.8185733072459698\n",
      "Epoch 2860. Learning Rate: 3.125e-05 Total Training Loss:  1.8186355204379652\n",
      "Epoch 2861. Learning Rate: 3.125e-05 Total Training Loss:  1.8184476588794496\n",
      "Epoch 2862. Learning Rate: 3.125e-05 Total Training Loss:  1.8185683070623782\n",
      "Epoch 2863. Learning Rate: 3.125e-05 Total Training Loss:  1.8183946356002707\n",
      "Epoch 2864. Learning Rate: 3.125e-05 Total Training Loss:  1.8182220146118198\n",
      "Epoch 2865. Learning Rate: 3.125e-05 Total Training Loss:  1.8186010114732198\n",
      "Epoch 2866. Learning Rate: 3.125e-05 Total Training Loss:  1.8182606195041444\n",
      "Epoch 2867. Learning Rate: 3.125e-05 Total Training Loss:  1.8182979369012173\n",
      "Epoch 2868. Learning Rate: 3.125e-05 Total Training Loss:  1.8183128954842687\n",
      "Epoch 2869. Learning Rate: 3.125e-05 Total Training Loss:  1.818343934690347\n",
      "Epoch 2870. Learning Rate: 3.125e-05 Total Training Loss:  1.8182278313324787\n",
      "Epoch 2871. Learning Rate: 3.125e-05 Total Training Loss:  1.8184352862881497\n",
      "Epoch 2872. Learning Rate: 3.125e-05 Total Training Loss:  1.8180766802979633\n",
      "Epoch 2873. Learning Rate: 3.125e-05 Total Training Loss:  1.8185422963870224\n",
      "Epoch 2874. Learning Rate: 3.125e-05 Total Training Loss:  1.817918974760687\n",
      "Epoch 2875. Learning Rate: 3.125e-05 Total Training Loss:  1.8182666490611155\n",
      "Epoch 2876. Learning Rate: 3.125e-05 Total Training Loss:  1.8179110105265863\n",
      "Epoch 2877. Learning Rate: 3.125e-05 Total Training Loss:  1.8181623503623996\n",
      "Epoch 2878. Learning Rate: 3.125e-05 Total Training Loss:  1.817905641568359\n",
      "Epoch 2879. Learning Rate: 3.125e-05 Total Training Loss:  1.8181314272223972\n",
      "Epoch 2880. Learning Rate: 3.125e-05 Total Training Loss:  1.8177865135367028\n",
      "Epoch 2881. Learning Rate: 3.125e-05 Total Training Loss:  1.8179580440046266\n",
      "Epoch 2882. Learning Rate: 3.125e-05 Total Training Loss:  1.817823217017576\n",
      "Epoch 2883. Learning Rate: 3.125e-05 Total Training Loss:  1.8179152679513209\n",
      "Epoch 2884. Learning Rate: 3.125e-05 Total Training Loss:  1.8177745703724213\n",
      "Epoch 2885. Learning Rate: 3.125e-05 Total Training Loss:  1.8178997016511858\n",
      "Epoch 2886. Learning Rate: 3.125e-05 Total Training Loss:  1.8178576632053591\n",
      "Epoch 2887. Learning Rate: 3.125e-05 Total Training Loss:  1.817599224013975\n",
      "Epoch 2888. Learning Rate: 3.125e-05 Total Training Loss:  1.8175120778323617\n",
      "Epoch 2889. Learning Rate: 3.125e-05 Total Training Loss:  1.8179163051827345\n",
      "Epoch 2890. Learning Rate: 3.125e-05 Total Training Loss:  1.8176797077176161\n",
      "Epoch 2891. Learning Rate: 3.125e-05 Total Training Loss:  1.8176509464392439\n",
      "Epoch 2892. Learning Rate: 3.125e-05 Total Training Loss:  1.8177359616965987\n",
      "Epoch 2893. Learning Rate: 3.125e-05 Total Training Loss:  1.8175359398883302\n",
      "Epoch 2894. Learning Rate: 3.125e-05 Total Training Loss:  1.817560452822363\n",
      "Epoch 2895. Learning Rate: 3.125e-05 Total Training Loss:  1.8174293029587716\n",
      "Epoch 2896. Learning Rate: 3.125e-05 Total Training Loss:  1.8173690605326556\n",
      "Epoch 2897. Learning Rate: 3.125e-05 Total Training Loss:  1.8176132830849383\n",
      "Epoch 2898. Learning Rate: 3.125e-05 Total Training Loss:  1.8172852932475507\n",
      "Epoch 2899. Learning Rate: 3.125e-05 Total Training Loss:  1.8176901208935305\n",
      "Epoch 2900. Learning Rate: 3.125e-05 Total Training Loss:  1.817279847280588\n",
      "Epoch 2901. Learning Rate: 3.125e-05 Total Training Loss:  1.8173849908926059\n",
      "Epoch 2902. Learning Rate: 3.125e-05 Total Training Loss:  1.817267317819642\n",
      "Epoch 2903. Learning Rate: 3.125e-05 Total Training Loss:  1.8171204982791096\n",
      "Epoch 2904. Learning Rate: 3.125e-05 Total Training Loss:  1.8175119103107136\n",
      "Epoch 2905. Learning Rate: 3.125e-05 Total Training Loss:  1.8171110690454952\n",
      "Epoch 2906. Learning Rate: 3.125e-05 Total Training Loss:  1.8169369685347192\n",
      "Epoch 2907. Learning Rate: 3.125e-05 Total Training Loss:  1.81757733199629\n",
      "Epoch 2908. Learning Rate: 3.125e-05 Total Training Loss:  1.817115931014996\n",
      "Epoch 2909. Learning Rate: 3.125e-05 Total Training Loss:  1.817066112475004\n",
      "Epoch 2910. Learning Rate: 3.125e-05 Total Training Loss:  1.81706930193468\n",
      "Epoch 2911. Learning Rate: 3.125e-05 Total Training Loss:  1.816985386685701\n",
      "Epoch 2912. Learning Rate: 3.125e-05 Total Training Loss:  1.8171238752547652\n",
      "Epoch 2913. Learning Rate: 3.125e-05 Total Training Loss:  1.8170061616692692\n",
      "Epoch 2914. Learning Rate: 3.125e-05 Total Training Loss:  1.8170926851162221\n",
      "Epoch 2915. Learning Rate: 3.125e-05 Total Training Loss:  1.8167894397047348\n",
      "Epoch 2916. Learning Rate: 3.125e-05 Total Training Loss:  1.8169734708208125\n",
      "Epoch 2917. Learning Rate: 3.125e-05 Total Training Loss:  1.816739141504513\n",
      "Epoch 2918. Learning Rate: 3.125e-05 Total Training Loss:  1.8171233460889198\n",
      "Epoch 2919. Learning Rate: 3.125e-05 Total Training Loss:  1.8166253313829657\n",
      "Epoch 2920. Learning Rate: 3.125e-05 Total Training Loss:  1.816958145238459\n",
      "Epoch 2921. Learning Rate: 3.125e-05 Total Training Loss:  1.8168391813524067\n",
      "Epoch 2922. Learning Rate: 3.125e-05 Total Training Loss:  1.8166946778947022\n",
      "Epoch 2923. Learning Rate: 3.125e-05 Total Training Loss:  1.8168625855760183\n",
      "Epoch 2924. Learning Rate: 3.125e-05 Total Training Loss:  1.8167859973036684\n",
      "Epoch 2925. Learning Rate: 3.125e-05 Total Training Loss:  1.8164946789329406\n",
      "Epoch 2926. Learning Rate: 3.125e-05 Total Training Loss:  1.816525062720757\n",
      "Epoch 2927. Learning Rate: 3.125e-05 Total Training Loss:  1.8167359860381112\n",
      "Epoch 2928. Learning Rate: 3.125e-05 Total Training Loss:  1.8166556153737474\n",
      "Epoch 2929. Learning Rate: 3.125e-05 Total Training Loss:  1.8163645016320515\n",
      "Epoch 2930. Learning Rate: 3.125e-05 Total Training Loss:  1.816570930503076\n",
      "Epoch 2931. Learning Rate: 3.125e-05 Total Training Loss:  1.816739146830514\n",
      "Epoch 2932. Learning Rate: 3.125e-05 Total Training Loss:  1.816197775217006\n",
      "Epoch 2933. Learning Rate: 3.125e-05 Total Training Loss:  1.8166044082317967\n",
      "Epoch 2934. Learning Rate: 3.125e-05 Total Training Loss:  1.816470524558099\n",
      "Epoch 2935. Learning Rate: 3.125e-05 Total Training Loss:  1.8163519215304404\n",
      "Epoch 2936. Learning Rate: 3.125e-05 Total Training Loss:  1.8165424982144032\n",
      "Epoch 2937. Learning Rate: 3.125e-05 Total Training Loss:  1.8162168173876125\n",
      "Epoch 2938. Learning Rate: 3.125e-05 Total Training Loss:  1.8163508909929078\n",
      "Epoch 2939. Learning Rate: 3.125e-05 Total Training Loss:  1.8164533694507554\n",
      "Epoch 2940. Learning Rate: 3.125e-05 Total Training Loss:  1.8161025746376254\n",
      "Epoch 2941. Learning Rate: 3.125e-05 Total Training Loss:  1.8161171650572214\n",
      "Epoch 2942. Learning Rate: 3.125e-05 Total Training Loss:  1.8162244659324642\n",
      "Epoch 2943. Learning Rate: 3.125e-05 Total Training Loss:  1.8165188914863393\n",
      "Epoch 2944. Learning Rate: 3.125e-05 Total Training Loss:  1.816006715700496\n",
      "Epoch 2945. Learning Rate: 3.125e-05 Total Training Loss:  1.8162334752560128\n",
      "Epoch 2946. Learning Rate: 3.125e-05 Total Training Loss:  1.8161319470964372\n",
      "Epoch 2947. Learning Rate: 3.125e-05 Total Training Loss:  1.8160293431719765\n",
      "Epoch 2948. Learning Rate: 3.125e-05 Total Training Loss:  1.8160487439890858\n",
      "Epoch 2949. Learning Rate: 3.125e-05 Total Training Loss:  1.8159332785871811\n",
      "Epoch 2950. Learning Rate: 3.125e-05 Total Training Loss:  1.816209085751325\n",
      "Epoch 2951. Learning Rate: 3.125e-05 Total Training Loss:  1.8158659740001895\n",
      "Epoch 2952. Learning Rate: 3.125e-05 Total Training Loss:  1.8158719336497597\n",
      "Epoch 2953. Learning Rate: 3.125e-05 Total Training Loss:  1.815968937240541\n",
      "Epoch 2954. Learning Rate: 3.125e-05 Total Training Loss:  1.815787407627795\n",
      "Epoch 2955. Learning Rate: 3.125e-05 Total Training Loss:  1.8157665920734871\n",
      "Epoch 2956. Learning Rate: 3.125e-05 Total Training Loss:  1.8160112429177389\n",
      "Epoch 2957. Learning Rate: 3.125e-05 Total Training Loss:  1.8157703957695048\n",
      "Epoch 2958. Learning Rate: 3.125e-05 Total Training Loss:  1.815805180260213\n",
      "Epoch 2959. Learning Rate: 3.125e-05 Total Training Loss:  1.8158885282755364\n",
      "Epoch 2960. Learning Rate: 3.125e-05 Total Training Loss:  1.815772180998465\n",
      "Epoch 2961. Learning Rate: 3.125e-05 Total Training Loss:  1.8155263780790847\n",
      "Epoch 2962. Learning Rate: 3.125e-05 Total Training Loss:  1.81586834488553\n",
      "Epoch 2963. Learning Rate: 3.125e-05 Total Training Loss:  1.815597087261267\n",
      "Epoch 2964. Learning Rate: 3.125e-05 Total Training Loss:  1.8154371421260294\n",
      "Epoch 2965. Learning Rate: 3.125e-05 Total Training Loss:  1.8157855977187864\n",
      "Epoch 2966. Learning Rate: 3.125e-05 Total Training Loss:  1.815844740747707\n",
      "Epoch 2967. Learning Rate: 3.125e-05 Total Training Loss:  1.8152244551456533\n",
      "Epoch 2968. Learning Rate: 3.125e-05 Total Training Loss:  1.81584376655519\n",
      "Epoch 2969. Learning Rate: 3.125e-05 Total Training Loss:  1.8152989976806566\n",
      "Epoch 2970. Learning Rate: 3.125e-05 Total Training Loss:  1.8156435847340617\n",
      "Epoch 2971. Learning Rate: 3.125e-05 Total Training Loss:  1.815319530054694\n",
      "Epoch 2972. Learning Rate: 3.125e-05 Total Training Loss:  1.815490998356836\n",
      "Epoch 2973. Learning Rate: 3.125e-05 Total Training Loss:  1.815285173826851\n",
      "Epoch 2974. Learning Rate: 3.125e-05 Total Training Loss:  1.8156038363813423\n",
      "Epoch 2975. Learning Rate: 3.125e-05 Total Training Loss:  1.8151770164840855\n",
      "Epoch 2976. Learning Rate: 3.125e-05 Total Training Loss:  1.8153753695369232\n",
      "Epoch 2977. Learning Rate: 3.125e-05 Total Training Loss:  1.815229987463681\n",
      "Epoch 2978. Learning Rate: 3.125e-05 Total Training Loss:  1.8153995239699725\n",
      "Epoch 2979. Learning Rate: 3.125e-05 Total Training Loss:  1.8151448591961525\n",
      "Epoch 2980. Learning Rate: 3.125e-05 Total Training Loss:  1.8152033958176617\n",
      "Epoch 2981. Learning Rate: 3.125e-05 Total Training Loss:  1.815243875113083\n",
      "Epoch 2982. Learning Rate: 3.125e-05 Total Training Loss:  1.8149761093372945\n",
      "Epoch 2983. Learning Rate: 3.125e-05 Total Training Loss:  1.8152663668151945\n",
      "Epoch 2984. Learning Rate: 3.125e-05 Total Training Loss:  1.8150068042450584\n",
      "Epoch 2985. Learning Rate: 3.125e-05 Total Training Loss:  1.8151714487466961\n",
      "Epoch 2986. Learning Rate: 3.125e-05 Total Training Loss:  1.8149365098797716\n",
      "Epoch 2987. Learning Rate: 3.125e-05 Total Training Loss:  1.815261745650787\n",
      "Epoch 2988. Learning Rate: 3.125e-05 Total Training Loss:  1.8151664554025047\n",
      "Epoch 2989. Learning Rate: 3.125e-05 Total Training Loss:  1.8148867610725574\n",
      "Epoch 2990. Learning Rate: 3.125e-05 Total Training Loss:  1.814994750864571\n",
      "Epoch 2991. Learning Rate: 3.125e-05 Total Training Loss:  1.8149358500668313\n",
      "Epoch 2992. Learning Rate: 3.125e-05 Total Training Loss:  1.8146549338998739\n",
      "Epoch 2993. Learning Rate: 3.125e-05 Total Training Loss:  1.8148899402585812\n",
      "Epoch 2994. Learning Rate: 3.125e-05 Total Training Loss:  1.8146377545199357\n",
      "Epoch 2995. Learning Rate: 3.125e-05 Total Training Loss:  1.8151584718725644\n",
      "Epoch 2996. Learning Rate: 3.125e-05 Total Training Loss:  1.8145703084010165\n",
      "Epoch 2997. Learning Rate: 3.125e-05 Total Training Loss:  1.8148323361529037\n",
      "Epoch 2998. Learning Rate: 3.125e-05 Total Training Loss:  1.8147578409407288\n",
      "Epoch 2999. Learning Rate: 3.125e-05 Total Training Loss:  1.8148337752209045\n",
      "[[500  11]\n",
      " [  0 489]]\n",
      "{'Accuracy': 0.989, 'Precision': 0.9784735812133072, 'Sensitivity_recall': 1.0, 'Specificity': 0.978, 'F1_score': 0.9891196834817013}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DATA_PATH = './data.csv'\n",
    "\n",
    "# Step 1: preprocess the data\n",
    "df = load_data(DATA_PATH)\n",
    "df_cleaned = clean_data(df)\n",
    "df_encoded = encode_data(df_cleaned)\n",
    "\n",
    "# Step 2: Standarize and reduce dimensions\n",
    "df_scaled = scale_features(df_encoded)\n",
    "df_reduced = reduce_dimensions(df_scaled)\n",
    "\n",
    "# Step 3: Create Classification Model\n",
    "classifier = NeuralNetwork()\n",
    "\n",
    "# Step 4: Train\n",
    "train_loader, test_loader = split_data(df_reduced)\n",
    "train(classifier, train_loader)\n",
    "\n",
    "# Step 5: Test Model\n",
    "evaluate(classifier, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
